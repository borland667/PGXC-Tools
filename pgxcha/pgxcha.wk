#!/bin/bash
#
# Configuration file.   Configuration file can be specified as -c option of
# the command like, or PGXCCONFIG environment variable.  If both are
# not specified, the following configuration file will be used.
#
configFile=$HOME/pgxc/pgxcConfig
#
#==========================================================================================================================
#
# Configuration Section
#
#		This section should be in the file $configFile for
#		user's configuration.
#
# Several assumptons:
# 1) configuration file will be set to data directory.
#    configuration file name is fixed to postgresql.conf
# 2) pg_hba.conf will be set to data directory.  File name is
#    fixed to pg_hba.conf
#
#================================================================
# MEMO
#
# max_connections, min_pool_size, max_pool_size --> should be configurable!
# They're not cluster specific.  So we may give a chance to include
# these specific options to be included from external files.
# They should not change by failover so they just have to be
# configured at first time only.
#===============================================================
#
#---- OVERALL -----------------------------------------------------------------------------------------------------------
#
pgxcOwner=koichi		# owner of the Postgres-XC database cluster.  Here, we use this
						# both as linus user and database user.  This must be
						# the super user of each coordinator and datanode.

pgxcInstallDir=$HOME/pgxc

tmpDir=/tmp					# temporary dir used in XC servers
localTmpDir=$tmpDir			# temporary dir used here locally


#---- GTM --------------------------------------------------------------------------------------------------------------

# GTM is mandatory.  You must have at least (and only) one GTM master in your Postgres-XC cluster.
# If GTM crashes and you need to reconfigure it, you can do it by pgxc_update_gtm command to update
# GTM master with others.   Of course, we provide pgxc_remove_gtm command to remove it.  This command
# will not stop the current GTM.  It is up to the operator.

#---- Overall -------
gtmName=gtm

#---- GTM Master -----------------------------------------------

#---- Overall ----
gtmMasterServer=node13
gtmMasterPort=20001
gtmMasterDir=$HOME/pgxc/nodes/gtm

#---- Configuration ---
gtmExtraConfig=""			# Will be added gtm.conf for both Master and Slave (done at initilization only)
gtmMasterSpecificExtraConfig=""	# Will be added to Master's gtm.conf (done at initialization only)

#---- GTM Slave -----------------------------------------------

# Because GTM is a key component to maintain database consistency, you may want to configure GTM slave
# for backup.

#---- Overall ------
gtmSlave=y					# Specify y if you configure GTM Slave.   Otherwise, GTM slave will not be configured and
							# all the following variables will be reset.
gtmSlaveServer=node12		# value N/A means GTM slave is not available.  Give N/A if you don't configure GTM Slave.
gtmSlavePort=20001			# Not used if you don't configure GTM slave.
gtmSlaveDir=$HOME/pgxc/nodes/gtm	# Not used if you don't configure GTM slave.
# Please note that when you have GTM failover, then there will be no slave available until you configure the slave
# again. (pgxc_add_gtm_slave function will handle it)

#---- Configuration ----
gtmSlaveSpecificExtraConfig="" # Will be added to Slave's gtm.conf (done at initialization only)

#---- GTM Proxy -------------------------------------------------------------------------------------------------------
# GTM proxy will be selected based upon which server each component runs on.
# When fails over to the slave, the slave inherits its master's gtm proxy.  It should be
# reconfigured based upon the new location.
#
# To do so, slave should be restarted.   So pg_ctl promote -> (edit postgresql.conf and recovery.conf) -> pg_ctl restart
#
# You don't have to configure GTM Proxy if you dont' configure GTM slave or you are happy if every component connects
# to GTM Master directly.  If you configure GTL slave, you must configure GTM proxy too.

#---- Shortcuts ------
gtmProxyDir=$HOME/pgxc/nodes/gtm_pxy

#---- Overall -------
gtmProxy=y				# Specify y if you conifugre at least one GTM proxy.   You may not configure gtm proxies
						# only when you dont' configure GTM slaves.
						# If you specify this value not to y, the following parameters will be set to default empty values.
						# If we find there're no valid Proxy server names (means, every servers are specified
						# as N/A), then gtmProxy value will be set to "n" and all the entries will be set to
						# empty values.
gtmProxyNames=(gtm_pxy1 gtm_pxy2 gtm_pxy3 gtm_pxy4)	# No used if it is not configured
gtmProxyServers=(node01 node02 node03 node04)			# Specify N/A if you dont' configure it.
gtmProxyPorts=(20001 20001 20001 20001)				# Not used if it is not configured.
gtmProxyDirs=($gtmProxyDir $gtmProxyDir $gtmProxyDir $gtmProxyDir)	# Not used if it is not configured.

#---- Configuration ----
gtmPxyExtraConfig=""		# Extra configuration parameter for gtm_proxy
gtmPxySpecificExtraConfig=("" "" "" "")

#---- Coordinators ----------------------------------------------------------------------------------------------------

#---- shortcuts ----------
coordMasterDir=$HOME/pgxc/nodes/coord
coordSlaveDir=$HOME/pgxc/nodes/coord_slave
coordArchLogDir=$HOME/pgxc/nodes/coord_archlog

#---- Overall ------------
coordNames=(coord1 coord2 coord3 coord4)		# Master and slave use the same name
coordPorts=(20004 20005 20004 20005)			# Master and slave use the same port
poolerPorts=(20010 20011 20010 20011)			# Master and slave use the same pooler port
coordPgHbaEntries=(0.0.0.0/0)					# Assumes that all the coordinator (master/slave) accepts
												# the same connection
# Note: The above parameter is extracted as "host all all 0.0.0.0/0 trust".   If you don't want
# such setups, specify the value () to this variable and suplly what you want using coordExtraPgHba
# and/or coordSpecificExtraPgHba variables.

#---- Master -------------
coordMasterServers=(node01 node02 node03 node04)		# N/A means this master is not available
coordMasterDirs=($coordMasterDir $coordMasterDir $coordMasterDir $coordMasterDir)
coordMaxWALsernder=5	# max_wal_senders: needed to configure slave. If zero value is specified,
						# it is expected to supply this parameter explicitly by external files
						# specified in the following.	If you don't configure slaves, leave this value to zero.
coordMaxWalSenters=($coordMaxWalSenders $coordMaxWalSenders $coordMaxWalSenders $coordMaxWalSenders)
						# max_wal_senders configuration for each coordinator.

#---- Slave -------------
coordSlave=y			# Specify y if you configure at least one coordiantor slave.  Otherwise, the following
						# configuration parameters will be set to empty values.
						# If no effective server names are found (that is, every servers are specified as N/A),
						# then coordSlave value will be set to n and all the following values will be set to
						# empty values.
coordSlaveServers=(node02 node03 node04 node01)			# N/A means this slave is not available
coordSlaveDirs=($coordSlaveDir $coordSlaveDir $coordSlaveDir $coordSlaveDir)
coordArchLogDirs=($coordArchLogDir $coordArchLogDir $coordArchLogDir $coordArchLogDir)

#---- Configuration files---
# Need these when you'd like setup specific non-default configuration 
# These files will go to corresponding files for the master.
# You may supply your bash script to setup extra config lines and extra pg_hba.conf entries 
# Or you may supply these files manually.
coordExtraConfig=""			# Extra configuration file for coordinators.  This file will be added to all the coordinators' postgresql.conf
coordSpecificExraConfig=("" "" "" "")
coordExtraPgHba=""			# Extra entry for pg_hba.conf.  This file will be added to all the coordinators' pg_hba.conf
coordSpecificExtraPgHba=("" "" "" "")

#---- Datanodes -------------------------------------------------------------------------------------------------------

#---- Shortcuts --------------
datanodeMasterDir=$HOME/pgxc/nodes/dn_master
datanodeSlaveDir=$HOME/pgxc/nodes/dn_slave
datanodeArchLogDir=$HOME/pgxc/nodes/datanode_archlog

#---- Overall ---------------
datanodeNames=(datanode1 datanode2 datanode3 datanode4)
datanodePorts=(20008 20009 20008 20009)		# Master and slave use the same port!
datanodePgHbaEntries=(0.0.0.0/0)				# Assumes that all the coordinator (master/slave) accepts
												# the same connection
# Note: The above parameter is extracted as "host all all 0.0.0.0/0 trust".   If you don't want
# such setups, specify the value () to this variable and suplly what you want using datanodeExtraPgHba
# and/or datanodeSpecificExtraPgHba variables.

#---- Master ----------------
datanodeMasterServers=(node01 node02 node03 node04)	# N/A means this master is not available.
														# This means that there should be the master but is down.
														# The cluster is not operational until the master is
														# recovered and ready to run.	
datanodeMasterDirs=($datanodeMasterDir $datanodeMasterDir $datanodeMasterDir $datanodeMasterDir)
datanodeMaxWALsernder=5								# max_wal_senders: needed to configure slave. If zero value is specified,
														# it is expected to supply this parameter explicitly by external files
													    # specified in the following.	
														# If you don't configure slaves, leave this value to zero.
datanodeMaxWalSenders=($datanodeMaxWalSender $datanodeMaxWalSender $datanodeMaxWalSender $datanodeMaxWalSender)
						# max_wal_senders configuration for each datanode

#---- Slave -----------------
datanodeSlave=y			# Specify y if you configure at least one coordiantor slave.  Otherwise, the following
						# configuration parameters will be set to empty values.
						# If no effective server names are found (that is, every servers are specified as N/A),
						# then datanodeSlave value will be set to n and all the following values will be set to
						# empty values.
datanodeSlaveServers=(node02 ode03 node04 node01)	# value N/A means this slave is not available
datanodeSlaveDirs=($datanodeSlaveDir $datanodeSlaveDir $datanodeSlaveDir $datanodeSlaveDir)
datanodeArchLogDirs=( $datanodeArchLogDir $datanodeArchLogDir $datanodeArchLogDir $datanodeArchLogDir )

# ---- Configuration files ---
# You may supply your bash script to setup extra config lines and extra pg_hba.conf entries here.
# These files will go to corresponding files for the master.
# Or you may supply these files manually.
datanodeExtraConfig=""		# Extra configuration file for datanodes.  This file will be added to all the datanodes' postgresql.conf
datanodeSpecificExtraConfig=("" "" "" "")
datanodeExtraPgHba=""		# Extra entry for pg_hba.conf.  This file will be added to all the datanodes' postgresql.conf
datanodeSpecificExtraPgHba=("" "" "" "")

#
#		End of Configuration Section
#
#==========================================================================================================================

##############################################################################################################
#
#   FUNCTIONS 
#
##############################################################################################################

function create_config_file
{
	# The configuration file is just a copy of the above configuraiton section.   If you modify the above,
	# you should reflect it to the below too.
	cat > $configFile <<EOF
#!/bin/bash
# The context will be supplied finally...
EOF
}

# Common variables ######################################################################
verbose=n
datetime=`date +%y%m%d_%H%M`
immediate="-m fast"			# option for pg_ctl stop.

allServers=()		# List of all the servers which appear in this configuation.

# Extract the server list into ${allServers[@]}
# Check if there's no duplicate elements in ${allServers[@]}.   If not, then add the argument
# to ${allServers[@]}
function addServer
{
	local append

	append=y
	if [ $1 == N/A ]
	then
		return
	fi
	for((i=0; i< ${#allServers[@]}; i++))
	do
	  if [ ${allServers[$i]} == $1 ]
	  then
		  append=n
	  fi
	done
	if [ $append == y]
	then
		allServers[$i]= $1
	fi
}

# Build unique server list
#
function makeServerList
{
	local i

	# GTM Master
	if [ $gtmMasterServer != N/A ]
	then
		addServer $gtmMasterServer
	fi
	# GTM Slave
	if [ $gtmSlaveServer != N/A ]
	then
		addServer $gtmSlaveServer
	fi
	# GTM Proxy
	for ((i=0; i< ${#gtmProxyServers[@]}; i++))
	do
	  if [ ${gtmProxyServers[$i]} != N/A ]
	  then
		  addServer ${gtmProxyServers[$i]}
	  fi
	done
	# Coordinator Master
	for ((i=0; i< ${#coordMasterServers[@]}; i++))
	do
	  if [ ${coordMasterServers[$i]} != N/A ]
	  then
		  addServer ${coordMasterServers[$i]}
	  fi
	done
	# Coordinator Slave
	for ((i=0; i< ${#coordSlaveServers[@]}; i++))
	do
	  if [ ${coordSlaveServers[$i]} != N/A ]
	  then
		  addServer ${coordSlaveServers[$i]}
	  fi
	done
	# Datanode Master
	for ((i=0; i< ${#datanodeMasterServers[@]}; i++))
	do
	  if [ ${datanodeMasterServers[$i]} != N/A ]
	  then
		  addServer ${datanodeMasterServers[$i]}
	  fi
	done
	# Datanode Slave
	for ((i=0; i< ${#datanodeSlaveServers[@]}; i++))
	do
	  if [ ${datanodeSlaveServers[$i]} != N/A ]
	  then
		  addServer ${datanodeSlaveServers[$i]}
	  fi
	done
}

#### Handle Slave Configurations ###################################
# Set GTM Proxy info if it is not configured at all
function gtm_proxy_set_to_no
{
	local i
	gtmProxyNames=()
	gtmProxyServers=()
	gtmProxyPorts=()
	gtmProxyDirs=()
	gtmPxySpecificExtraConfig=()
	gtmPxyExtraConfig=""
	for ((i=0; i< ${#allServers[@]}; i++))
	do
	  gtmProxyNames[$i]=N/A
	  gtmProxyServers[$i]=N/A
	  gtmProxyPorts[$i]=0
	  gtmProxyDirs[$i]=N/A
	  gtmProxySpecificExtraConfig[$i]=""
	done
}

# Set Coordinator Slave info if it is not configured at all
function coord_slave_set_to_no
{
	local i
	coordSlaveServers=()
	coordSlaveDirs=()
	coordArchLogDirs=()
	for ((i=0; i< ${#coordMasterServers[@]}; i++))
	do
	  coordSlaveServers[$i]=N/A
	  coordSlaveDirs[$i]=N/A
	  coordArchLogDirs[$i]=N/A
	done
}

# Set Datanode slave info if it is not configured at all
function datanode_slave_set_to_no
{
	local i
	datanodeSlaveServers=()
	datanodeSlaveDirs=()
	datanodeSlaveArchLogDirs=()
	for ((i=0; i< ${#datanodeMasterServers[@]}; i++))
	do
	  datanodeSlaveServers[$i]=N/A
	  datanodeSlaveDirs[$i]=N/A
	  datanodeSlaveArchLogDirs[$i]=N/A
	done
}

# Handle the case where slaves are not configured. --> Construct empty configuration for them
# We assume that all the server list has been constructed.
function handle_no_slaves
{
	local i
	local isEmpty

	# GTM slave
	if [ $gtmSlave != y ]
	then
		gtmSlaveServer=N/A
		gtmSlavePort=0
		gtmSlaveDir=N/A
	else
		if [ $gtmSlaveServer == N/A ]
		then
			gtmSlave=n
			gtmSlavePort=0
			gtmSlaveDir=N/A
		fi
	fi

	# GTM Proxy
	if [ $gtmProxy != y ]
	then
		gtm_proxy_set_to_no
	else
		isEmpty=y
		for ((i=0; i<${#gtmProxyServers[@]}; i++))
		do
		  if [ ${gtmProxyServers[$i]} != N/A -a ${gtmProxyServers[$i]} != "" ]
		  then
			  isEmpty=n
			  break
		  fi
		done
		if [ $isEmpty == y ]
		then
			gtm_proxy_set_to_no
			gtmProxy=n
		fi
	fi

	# Coordinator
	if [ $coordSlave != y ]
	then
		coord_slave_set_to_no
	else
		isEmpty=y
		for ((i=0; i<${#coordSlaveServers[@]}; i++))
		do
		  if [ ${coordSlaveServers[$i]} != N/A -a ${coordSlaveServers[$i]} != "" ]
		  then
			  isEmpty=n
			  break
		  fi
		done
		if [ $isEmpty == y ]
		then
			coord_slave_set_to_no
			coordSlave=n
		fi
	fi

	# Datanode
	if [ $datanodeSlave != y ]
	then
		datanode_slave_set_to_no
	else
		isEmpty=y
		for ((i=0; i<${#datanodeSlaveServers[@]}; i++))
		do
		  if [ ${datanodeSlaveServers[$i]} != N/A -a ${coordSlaveServers[$I]} != "" ]
		  then
			  isEmpty=n
			  break
		  fi
		done
		if [ $isEmpty == y ]
		then
			datanode_slave_set_to_no
			datanodeSlave=n
		fi
	fi
}

# Check if there're no duplicates in port and working directory assigment
function verifyResource
{
	local i
	local j
	local Ports
	local Dirs
	local Servers

	Ports=()
	Dirs=()
	Names=()


	# Number of array entries
	# GTM proxies
	if [ $gtmProxy == y ]
	then
		i=${#gtmProxyNames[@]}
		if [ $i -ne ${#gtmProxyServers[@]} -o $i -ne ${#gtmProxyPorts[@]} -o $i -ne ${#gtmProxyDirs[@]} -o $i -ne ${#gtmProxySpecificExtraConfig[@]} ]
		then
			echo ERROR: Invalid entry numbers in gtm proxy configuration.
			exit 1
		fi
	fi
	# Coordinators
	i=${#coordNames[@]}
	if [ $i -ne ${#coordPorts[@]} -o $i -ne ${#poolerPorts[@]} -o $i -ne ${#coordSpecificExtraConfig[@]} -o $i -ne ${#coordSpecificExtraPgHba[@]} ]
	then
		echo ERROR: Invalid entry numbers in coordinator configuration.
		exit 1
	fi
	if [ $i -ne ${#coordMasterServers[@]} -o $i -ne ${#coordMaterDirs[@]} -o $i -ne ${#coordMaxWALSenders[@]} ]
	then
		echo ERROR: Invalid entry numbers in coordinator configuration.
		exit 1
	fi
	if [ $coordSlave == y ]
	then
		if [ $i -ne ${#coordSlaveServers[@]} -o $i -ne ${#coordSlaveDirs[@]} -o $i -ne ${#coordArchLogDirs[@]} ]
		then
			echo ERROR: Invalid entry numbers in coordinator configuration.
			exit 1
		fi
	fi
	# Datanodes
	i=${#datanodeNames[@]}
	if [ $i -ne ${#datanodePorts[@]} -o $i -ne ${#datanodeSpecificExtraConfig[@]} -o $i -ne ${#datanodeSpecificExtraPgHba[@]} ]
	then
		echo ERROR: Invalid entry numbers in datanode configuration.
		exit 1
	fi
	if [ $i -ne ${#datanodeMasterServers[@]} -o $i -ne ${#datanodeMasterDirs[@]} -o $i -ne ${#datanodeMaxWalSenders[@]} ]
	then
		echo ERROR: Invalid entry numbers in datanode configuration.
		exit 1
	fi
	if [ $datanodeSlave == y ]
	then
		if [ $i -ne ${#datanodeSlaveServers[@]} -o $i -ne ${#datanodeSlaveDirs[@]} -o $i -ne ${#datanodeArchLogDirs[@]} ]
		then
			echo ERROR: Invalid entry numbers in datanode configuration.
			exit 1
		fi
	fi

	# Check if node names don't duplicate
	for ((i=0; i<${#gtmProxyNames[@]};i++))
	do
	  if [ $gtmName == ${gtmProxyNames[$i]} ]
	  then
		  echo ERROR: GTM name duplicates one of the GTM Proxies, $gtmName
		  exit 1
	  fi
	done
	for ((i=0; i<${#coordNames[@]}; i++))
	do
	  if [ $gtmName == ${coordNames[$i]} ]
	  then
		  echo ERROR: GTM name duplicates one of the coordinators, $gtmName
		  exit 1
	  fi
	done
	for ((i=0; i<${#datanodeNames[@]}; i++))
	do
	  if [ $gtmName == ${datanodeNames[$i]} ]
	  then
		  echo ERROR: GTM name duplicates one of the datanodes, $gtmName
		  exit 1
	  fi
	done
	# GTM Proxy
	for ((i=0; i<${#gtmProxyNames[@]}; i++))
	do
	  for ((j=$i+1;j<${#gtmProxyNames[@]}; j++))
	  do
		if [ ${gtmProxyNames[$i]} == ${gtmProxyNames[$j]} ]
		then
			echo ERROR: GTM proxy name duplicates one of the other GTM proxies, ${gtmProxyNames[$i]}
			exit 1
		fi
	  done
	done
	# Cordinator
	for ((i=0; i<${#coordNames[@]}; i++))
	do
	  for ((j=$i+1; j<${#coordNames[@]}; j++))
	  do
		if [ ${coordNames[$i]} == ${coordNames[$j]} ]
		then
			echo ERROR: Coordinator name duplicates on of the other coordinators, ${coordNames[$i]}
			exit 1
		fi
	  done
	  for ((j=0; j<${#datanodeNames[@]}; j++))
	  do
		if [ ${coordNames[$i]} == ${datanodeNames[$j]} ]
		then
			echo ERROR: Coordinator name duplicates one of the datanodes, ${coordNames[$i]}
			exit 1
		fi
	  done
	done
	# Datanode
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  for ((j=$i+1; j<${#datanodeNames[@]}; j++))
	  do
		if [ ${datanodeNames[$i]} == ${datanodeNames[$j]} ]
		then
			echo ERROR: Datanode name duplicates one of the other datanodes, ${datanodeNames[$i]}
			exit 1
		fi
	  done
	done

	# Check if there's no duplicate in port/directory assignment
	if [ $1 == $gtmMasterServer ]
	then
		Ports=( ${Ports[@]} $gtmMasterPort )
		Dirs=( ${Dirs[@]} $gtmMasterDir )
	fi
	if [ $1 == $gtmSlaveServer ]
	then
		Ports=( ${Ports[@]} $gtmSlavePort )
		Dirs=( ${Dirs[@]} $gtmSlaveDir )
	fi
	for ((i=0; i< ${#gtmProxyServers}; i++))
	do
	  if [ $1 == ${gtmProxyServers[$i]} ]
	  then
		  Ports=( ${Ports[@]} ${gtmProxyPorts[$i]} )
		  Dirs=( ${Dirs[@]} ${gtmProxyDirs[$i]} )
	  fi
	done
	for ((i=0; i< ${#coordMasterServers[@]}; i++))
	do
	  if [ $1 == ${coordMasterServers[$i]} ]
	  then
		  Ports=( ${Ports[@]} ${coordPorts[$i]} )
		  Ports=( ${Ports[@]} ${poolerPorts[$i]} )
		  Dirs=( ${Dirs[@]} ${coordMasterDirs[$i]} )
	  fi
	  if [ $1 == ${coordSlaveServers[$i]} ]
	  then
		  Ports=( ${Ports[@]} ${coordPorts[$i]} )
		  Ports=( ${Ports[@]} ${poolerPorts[$i]} )
		  Dirs=( ${Dirs[@]} ${coordSlaveDirs[$i]} )
	  fi
	done
	for ((i=0; i<${#datanodeMasterServers[$i]}; i++))
	do
	  if [ $1 == ${datanodeMasterServers[$i]} ]
	  then
		  Ports=( ${Ports[@]} ${datanodePorts[$i]} )
		  Dirs=( ${Dirs[@]} ${datanodeMasterDirs[$i]} )
	  fi
	  if [ $1 == ${datanodeSlaveServers[$i]} ]
	  then
		  Ports=( ${Ports[@]} ${datanodePorts[$i]} )
		  Ports=( ${Ports[@]} ${datanodeSlaveDirs[$i]} )
	  fi
	done
	for ((i=0; i< ${#Ports[@]}; i++))
	do
	  for ((j=$i+1; j<${#Ports[@]}; j++))
	  do
		if [ ${Ports[$i]} -eq ${Ports[$j]} -a ${Ports[$i]} != N/A ]
		then
			echo ERROR: duplicate port assignment for the server $1
			exit 1
		fi
	  done
	done
	for ((i=0; i< ${#Dirs[@]}; i++))
	do
	  for ((j=$i+1; j< ${#Dirs[@]}; j++))
	  do
		if [ ${Dirs[$i]} == ${Dirs[$j]} -a ${Dirs[$i]} != N/A ]
		then
			echo ERROR: duplicate work directory assignment for the server $1
			exit 1
		fi
	  done
	done
	# We should check if GTM proxy is configured when GTM slave is configured.
	# We could do this here but it's better to do it when we configure
	# postgresql.conf of coordinator/datanode master.
}

#----------------------------------------------
# DEBUG Aid
#----------------------------------------------

DEBUG=n

function funcname
{
	if [ $DEBUG == y ]
	then
		echo '******** ' "$1() called" ' **********'
	fi
}

##=================================================
##
## Print configuration information
##
##================================================
#
#function printConfig
#{
#	echo "=== Postgres-XC configuration ==="
#	echo Posrgres-XC owner: $pgxcOwner
#	echo Postgres-XC install directory: $pgxcInstallDir
#	for ((i=0; i< ${#allServers[@]}; i++))
#	do
#	  echo --- ${allServers[$i]} -----------------
#	  if [ ${allServers[$i]} == $gtmMasterServer ]
#	  then
#		  echo GTM Master: name: $gtmName port: $$gtmMasterPort dir: $gtmMasterDir
#	  fi
#	  if [ ${allServers[$i]} == $gtmSlaverServer ]
#	  then
#		  echo GTM Slave: name: $gtmName port: $gtmSlavePort dir: $gtmSlaveDir
#	  fi
#	  for ((j=0; j< ${#coordMasterServer[@]}; j++))
#	  do
#		if [ ${allServers[$i]} == ${coordMasterServers[$j]} ]
#		then
#			echo Coordinator Master: name: ${coordNames[$j]} port: ${coordPorts[$j]} dir: ${coordMasterDirs[$j]}
#		fi
#		if [ ${allServers[$i]} == ${coordSlaveServers[$j]} ]
#		then
#			echo Coordinator Slave: name: ${coordNames[$j]} port: ${coordPorts[$j]} dir: ${coordSlaveDirs[$j]}
#		fi
#	  done
#	  for ((j=0; j< ${#datanodeMasterServers[$j]}; j++))
#	  do
#		if [ ${allServers[$i]} == ${datanodeMasterServers[$j]} ]
#		then
#			echo Datanode Master: name: ${datanodeNames[$j]} port: ${datanodePorts[$j]} dir: ${datanodeMasterDirs[$j]}
#		fi
#		if [ ${allServers[$i]} == ${datanodeSlaveServers[$j]} ]
#		then
#			echo Datanode Slave: name: ${datanodeNames[$j]} port: ${datanodePorts[$j]} dir: {datanodeSlaveDirs[$j]}
#		fi
#	  done
#}

#============================================================
#
# Common functions
#
#============================================================

function vecho
{
	if [ $verbose == y ]
	then
		echo $*
	fi
}

function doit
{
	vecho $*
	$*
}

function doall
{
    vecho doall target: "(" ${allServers[@]} ")"
    for (( i=0; i< ${#allServers[@]}; i++ ))
    do
      vecho "${allServer[$i]}: $* ..."
      ssh $pgxcOwner@${allServers[$i]} $*
    done
}

function Doall
{
    vecho Doall target: "(" ${DoallTarget[@]} ")"
    for (( i=0; i< ${#DoallTarget[@]}; i++ ))
    do
      vecho "${DoallTarget[$i]}: $* ..."
      ssh $pgxcOwner@${DoallTarget[$i]} $*
    done
}

function cpall
{
    vecho cpall target: "(" ${allServers[@]} ")"

    for (( i=0; i < ${#allServers[@]}; i++ ))
    do
      vecho scp -r $1 $pgxcOwner@${allServers[$i]}:$2
      scp -r $1 $pgxcOwner@${allServers[$i]}:$2
    done
}

function Cpall
{
    vecho Cpall target: "(" ${CpallTarget[@]} ")"
    for (( i=0; i< ${#CpallTarget[@]}; i++ ))
    do
      vecho scp -r $1 $pgxcOwner@${CpallTarget[$i]}:$2
      scp -r $1 $pgxcOwner@${CpallTarget[$i]}:$2
    done
}

#===============================================================                                             
# Tool function to check -m option to stop coordinator and                                                   
# datanode                                                                                                   
#===============================================================                                             
function check_immediate
{
    case $1 in
		immediate )
            immediate= "-m immediate" ;;
        fast )
            immediate= "-m fast" ;;
        normal )
            immediate= "" ;;
        * )
            echo "ERROR: Please specify immediate, fast or normal"
            exit 1;;
    esac;
}
#==================================================                                                          
#                                                                                                            
# Setup .bashrc file for PATH and LD_LIBRARY_PATH                                                            
#                                                                                                            
#==================================================                                                          

function setup_bashrc
{
    vecho ================================================================
    vecho Setting .bashrc files
    for ((i=0; i< ${#allServers[@]}; i++))
    do
      vecho ---- ${allServers[$i]} -------------------------
      ssh $pgxcOwner@${allServers[$i]} "cat >> .bashrc" <<EOF
# .bachrc addition for Postgres-XC PATH and LD_LIBRARY_PATH
# $datetime
export PATH_ORG=$PATH
export PATH=$pgxcInstallDir/bin:$PATH
export LD_LIBRARY_PATH_ORG=$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$pgxcInstallDir/lib:$LD_LIBRARY_PATH
export MANPATH_ORG=$MANPATH
export MANPATH=$pgxcInstallDir/share/man:$MANPATH
EOF
	  done
}

function setup_bashrc_individual
{
    vecho ================================================================
    vecho Setting .bashrc files for $pgxcOwner at $1
    ssh $pgxcOwner@$1 "cat >> .bashrc" <<EOF
# .bachrc addition for Postgres-XC PATH and LD_LIBRARY_PATH
# $datetime
export PATH_ORG=$PATH
export PATH=$pgxcInstallDir/bin:$PATH
export LD_LIBRARY_PATH_ORG=$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$pgxcInstallDir/lib:$LD_LIBRARY_PATH
export MANPATH_ORG=$MANPATH
export MANPATH=$pgxcInstallDir/share/man:$MANPATH
EOF
}

#==================================================                                                          
#                                                                                                            
# Deploy binaries and other common things to each server                                                     
#                                                                                                            
# All the build materials will be deployed to each                                                           
# servers.                                                                                                   
#                                                                                                            
#=================================================                                                           

function pgxc_deploy_all
{
    vecho ================================================================
    vecho pgxc_deploy_all: copy built materials to all the target servers.

    doall rm -rf $pgxcInstallDir/bin $pgxcInstallDir/include $pgxcInstallDir/lib $pgxcInstallDir/share
    doall mkdir -p $pgxcInstallDir
    vecho tar czCf $pgxcInstallDir $tmpDir/wk.tgz bin include lib share
    tar czCf $pgxcInstallDir $tmpDir/wk.tgz bin include lib share
    cpall $tmpDir/wk.tgz $pgxcInstallDir/wk.tgz
    doall tar xzCf  $pgxcInstallDir $pgxcInstallDir/wk.tgz
    doall rm $pgxcInstallDir/wk.tgz
    doit rm $tmpDir/wk.tgz
}

# First argument is the target node.                                                                         

function pgxc_deploy_individual
{
    vecho ================================================================
    vecho pgxc_deploy_individual: copy built materials to the server $1
    doit ssh $pgxcOwner@$1 rm -rf $pgxcInstallDir/bin $pgxcInstallDir/include $pgxcInstallDir/lib $pgxcInstallDir/share
    doit ssh $pgxcOwner@$1 mkdir -p $pgxcInstallDir
    doit tar czvCf $pgxcInstallDir $tmpDir/wk.tgz bin include lib share
    doit scp $tmpDir/wk.tgz $pgxcOwner@$1:$pgxcInstallDir/wk.tgz
    doit ssh $pgxcOwner@$1 tar xzCf $pgxcInstallDir $pgxcInstallDir/wk.tgz
    doit ssh $pgxcOwner@$1 rm $pgxcInstallDir/wk.tgz
    doit rm $tmpDir/wk.tgz
}

#==================================================
#
# Cleanup work directories
#
#==================================================

# First argument is the server name. second argument is the directory name
# Server name could be N/A, where the target does not exist.
function pgxc_clean_dir
{
	if [ $1 == N/A ]
	then
		return
	fi
	doit ssh $pgxcOwner@$1 rm -rf $2
	doit ssh $pgxcOwner@$1 mkdir -p $2
}

# First argument is the nodename.  The second is "master", "slave" or "all".
function pgxc_clean_node
{
	local i

	if [ $1 == $gtmName ]
	then
		shift;
		case $1 in
			master )
				pgxc_clean_dir $gtmMasterServer $gtmMasterDir
				return;;
			slave )
				pgxc_clean_dir $gtmSlaveServer $gtmSlaveDir
				return;;
			all )
				pgxc_clean_dir $gtmMasterServer $gtmMasterDir
				pgxc_clean_dir $gtmSlaveServer $gtmSlaveDir
				return;;
			* )
				echo ERROR: invalid argument for pgxc_clean_node, $1
				exit 1;;
		esac
	fi
	for ((i= 0; i< ${#gtmProxyNames[@]}; i++))
	do
	  if [ $1 == ${gtmProxyNames[$i]} ]
	  then
		  pgxc_clean_dir ${gtmProxyServers[$i]} ${gtmProxyDirs[$i]}
		  return;
	  fi
	done
	for ((i= 0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} ]
	  then
		  shift;
		  case $1 in
			  master )
				  pgxc_clean_dir ${coordMasterServers[$i]} ${coordMasterDirs[$i]}
				  return;;
			  slave )
				  pgxc_clean_dir ${coordSlaveServers[$i]} ${coordSlaveDirs[$i]}
				  return;;
			  all )
				  pgxc_clean_dir ${coordMasterServers[$i]} ${coordMasterDirs[$i]}
				  pgxc_clean_dir ${coordSlaveServers[$i]} ${coordSlaveDirs[$i]}
				  return;;
			  * )
				  echo ERROR: invalid argument for pgxc_clean_node, $1
				  exit 1;;
		  esac
	  fi
	done
	for ((i= 0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodeNames[$i]} ]
	  then
		  shift;
		  case $1 in
			  master )
				  pgxc_clean_dir ${datanodeMasterServers[$i]} ${datanodeMasterDirs[$i]}
				  return;;
			  slave )
				  pgxc_clean_dir ${datanodeSlaveServers[$i]} ${datanodeSlaveDirs[$i]}
				  return;;
			  all )
				  pgxc_clean_dir ${datanodeMasterServers[$i]} ${datanodeMasterDirs[$i]}
				  pgxc_clean_dir ${datanodeSlaveServers[$i]} ${datanodeSlaveDirs[$i]}
				  return;;
			  * )
				  echo ERROR: invalid argument for pgxc_clean_node, $1
				  exit 1;;
		  esac
	  fi
	done
	echo ERROR: no target nodename found, $1
}

function pgxc_clean_node_all
{
	local i

	pgxc_clean_node $gtmName all
	for ((i=0; i< ${#gtmProxyNames[@]}; i++))
	do
	  pgxc_clean_node ${gtmProxyNames[$i]}
	done
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  pgxc_clean_node ${coordNames[$i]} all
	done
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  pgxc_clean_ndoe ${coordNames[$i]} all
	done
}

#===========================================================                                                 
#                                                                                                            
# GTM and GTM slave staff
#                                                                                                            
#===========================================================                                                 
function pgxc_init_gtm_master
{
    vecho ================================================================
    vecho GTM Master initialize

    doit ssh $pgxcOwner@$gtmMasterServer killall -u $pgxcOwner -9 gtm
    doit ssh $pgxcOwner@$gtmMasterServer rm -rf $gtmMasterDir
    doit ssh $pgxcOwner@$gtmMasterServer mkdir -p $gtmMasterDir
    doit ssh $pgxcOwner@$gtmMasterServer initgtm -Z gtm -D $gtmMasterDir
    vecho Configuring $gtmMasterServer:$gtmMasterDir/gtm.conf
	if [ $gtmExtraConfig != "" ]
	then
		ssh $pgxcOwner@$gtmMasterServer "cat >> $gtmMasterDir/gtm.conf" < $gtmExtraConfig
	fi
    ssh $pgxcOwner@$gtmMasterServer "cat >>  $gtmMasterDir/gtm.conf" <<EOF
listen_addresses = '*'
port = $gtmPort
nodename = '$gtmName'
startup = ACT
EOF
}

function pgxc_init_gtm_slave
{
    vecho ================================================================
    vecho GTM Slave initialize

	if [ $gtmSlaveServer == N/A ]
	then
		echo ERROR: GTM Slave is not configured.
		return 1
	fi
    doit ssh $pgxcOwner@$gtmSlaveServer killall -u $pgxcOwner -9 gtm
    doit ssh $pgxcOwner@$gtmSlaveServer rm -rf $gtmSlaveDir
    doit ssh $pgxcOwner@$gtmSlaveServer mkdir -p $gtmSlaveDir
    doit ssh $pgxcOwner@$gtmSlaveServer initgtm -Z gtm -D $gtmSlaveDir
    vecho Configuring $gtmSlaveServer:$gtmSlaveDir/gtm.conf
	if [ $gtmExtraConfig != '' ]
	then
		ssh $pgxcOwner@$gtmSlaveServer "cat >> $gtmSlaveDir/gtm.conf" < $gtmExtraConfig
	fi
    ssh $pgxcOwner@$gtmSlaveServer "cat >>  $gtmSlaveDir/gtm.conf" <<EOF
listen_addresses = '*'
port = $gtmPort
nodename = '$gtmName'
startup = STANDBY
active_host = '$gtmMasterServer'
active_port = $gtmMasterPort
EOF
}

function pgxc_start_gtm_master
{
	vecho ================================================================
	vecho Starting GTM Master

	doit ssh $pgxcOwner@$gtmMasterServer killall -u $pgxcOwner -9 gtm
	doit ssh $pgxcOwner@$gtmMasterServer "gtm_ctl start -Z gtm -D $gtmMasterDir > $tmpDir/gtm.out"
	ssh $pgxcOwner@$gtmMasterServer cat $tmpDir/gtm.out
	ssh $pgxcOwner@$gtmMasterServer rm $tmpDir/gtm.out
}

function pgxc_start_gtm_slave
{
	vecho ================================================================
	vecho Starting GTM Slave

	if [ gtmSlaveServer == N/A ]
	then
		echo ERROR: GTM slave is not configured.
		return 1
	fi
	doit ssh $pgxcOwner@$gtmMasterServer "gtm_ctl status -Z gtm -D $gtmMasterDir > /dev/null"
	if [ $? -ne 0 ]
	then
		echo ERROR: GTM Master is not running. Cannot start the slave.
		exit 1
	fi
	doit ssh $pgxcOwner@$gtmSlaveServer killall -u $pgxcOwner -9 gtm
	doit ssh $pgxcOwner@$gtmSlaveServer "gtm_ctl start -Z gtm -D $gtmSlaveDir > $tmpDir/gtm.out"
	ssh $pgxcOwner@$gtmSlaveServer cat $tmpDir/gtm.out
	ssh $pgxcOwner@$gtmSlaveServer rm $tmpDir/gtm.out
}

function pgxc_stop_gtm_master
{
	vecho ================================================================
	vecho Stopping GTM Master
	doit ssh $pgxcOwner@$gtmMasterServer gtm_ctl stop -Z gtm -D $gtmMasterDir
}

function pgxc_stop_gtm_slave
{
	if [ gtmSlaveServer == N/A ]
	then
		echo ERROR: GTM slave is not configured.
		return 1
	fi
	vecho ================================================================
	vecho Stopping GTM Slave
	doit ssh $pgxcOwner@$gtmSlaveServer gtm_ctl stop -Z gtm -D $gtmSlaveDir
}

function pgxc_failover_gtm
{
	if [ gtmSlaveServer == N/A ]
	then
		echo ERROR: GTM slave is not configured.
		return 1
	fi
	# Reconnect should be done in separate action.
	vecho ================================================================
	vecho GTM Failover

	if [ $gtmSlaveServer == N/A ]
	then
		echo ERROR: pgxc_failover_gtm: GTM slave is not available.
		exit 1
	fi
	doit ssh "$pgxcOwner@$gtmSlaveServer gtm_ctl status -Z gtm -D gtmSlaveDir > /dev/null"
	if [ $? -ne 0 ]
	then
		echo ERROR: GTM slave is not running.
		exit 1
	fi
	# STONITH GTM Master
	doit ssh $pgxcOwner@$gtmMasterServer killall -u $pgxcOwner -9 gtm
	doit ssh $pgxcOwner@$gtmSlaveServer gtm_ctl promote -Z gtm -D $gtmSlaveDir
	# Update GTM configuration file as the master
	vecho Reconfigure GTM as Master
	ssh $pgxcOwner@$gtmSlaveServer "cat >> $gtmSlaveDir/gtm.conf" <<EOF
#===================================================
# Updated due to GTM failover
#        $datetime
startup = ACT
#----End of reconfiguration -------------------------
EOF
	# Update configuration
	vecho Reconfiguring whole Postgres-XC cluster
	cat >> $cofigFile <<EOF
#===================================================
# pgxc configuration file updated due to GTM failover
#        $datetime
gtmMasterServer= $gtmSlaveServer
gtmMasterPort= $gtmSlavePort
gtmMasterDir= $gtmSlaveDir
gtmSlaveServer=N/A
gtmSlavePort=N/A
gtmSlaveDir=N/A
#----End of reconfiguration -------------------------
EOF
	# Reconfigure myself
	gtmMasterServer= $gtmSlaveServer
	gtmMasterPort= $gtmSlavePort
	gtmMasterDir= $gtmSlaveDir
	gtmSlaveServer= N/A
	gtmSlavePort= N/A
	gtmSlaveDir= N/A
}

#===============================================================================
#
# GTM Proxy staff
#
#===============================================================================

function pgxc_init_gtm_proxy
{
	# First argument is the nodename
    vecho ================================================================
    vecho Initialize GTM Proxy $1

	local i

	for ((i=0; i< ${#gtmProxyNames[@]}; i++))
	do
	  if [ $1 == ${gtmProxyNames[$i]} ]
	  then
		  doit ssh $pgxcOwner@${gtmProxyServers[$i]} killall -u $pgxcOwner -9 gtm_proxy
		  doit ssh $pgxcOwner@${gtmProxyServers[$i]} rm -rf ${gtmProxyDirs[$i]}
		  doit ssh $pgxcOwner@${gtmProxyServers[$i]} mkdir -p ${gtmProxyDirs[$i]}
		  doit ssh $pgxcOwner@${gtmProxyServers[$i]} initgtm -Z gtm_proxy -D ${gtmProxyDirs[$i]}
		  vecho Configuring ${gtmProxyServers[$i]}:${gtmProxyDirs[$i]}/gtm_proxy.conf
		  if [ $gtmPxyExtraConfig != "" ]
		  then
			  ssh $pgxcOwner@${gtmProxyServers[$i]} "cat >> ${gtmProxyDirs[$i]}/gtm_proxy.conf" < $gtmPxyExtraConfig
		  fi
		  ssh $pgxcOwner@${gtmProxyServers[$i]} "cat >> ${gtmProxyDirs[$i]}/gtm_proxy.conf" <<EOF
nodename = '${gtmProxyNames[$i]}'
listen_addresses = '*'
port = '${gtmProxyPorts[$i]}'
gtm_host = $gtmMasterServer
gtm_port = $gtmMasterPort
worker_threads = 1
gtm_connect_retry_interval = 1
EOF
		  return
	  fi
	done
	echo ERROR: specified GTM proxy does not exist, $1
	return 1
}

function pgxc_start_gtm_proxy
{
	# First argument is the nodename
    vecho ================================================================
    vecho Start GTM Proxy $1

	local i
	for ((i=0; i< ${#gtmProxyNames[@]}; i++))
	do
	  if [ $1 == ${gtmProxyNames[$i]} ]
	  then
		  doit ssh $pgxcOwner@${gtmProxyServers[$i]} killall -u $pgxcOwner -9 gtm_proxy
		  doit ssh $pgxcOwner@${gtmProxyServers[$i]} "gtm_ctl start -Z gtm_proxy -D ${gtmProxyDirs[$i]} > $tmpDir/gtm_proxy.out"
		  ssh $pgxcOwner@${gtmProxyServers[$i]} cat $tmpDir/gtm_proxy.out
		  ssh $pgxcOwner@${gtmProxyServers[$i]} rm $tmpDir/gtm_proxy.out
		  return
	  fi
	done
	echo ERROR: specified GTM proxy does not exist, $1
	return 1
}

function pgxc_stop_gtm_proxy
{
	# First argument is the nodename
    vecho ================================================================
    vecho Stop GTM Proxy $1

	local i
	for ((i=0; i< ${#gtmProxyNames[@]}; i++))
	do
	  if [ $1 == ${gtmProxyNames[$i]} ]
	  then
		  doit ssh $pgxcOwner@${gtmProxyServers[$i]} gtm_ctl stop -Z gtm_proxy -D ${gtmProxyDirs[$i]}
		  return
	  fi
	done
	echo ERROR: specified GTM proxy does not exist, $1
	return 1
}

function pgxc_kill_gtm_proxy
{
	# First argument is the nodename
    vecho ================================================================
    vecho Kill GTM Proxy $1

	local i
	for ((i=0; i< ${#gtmProxyNames[@]}; i++))
	do
	  if [ $1 == ${gtmProxyNames[$i]} ]
	  then
		  doit ssh $pgxcOwner@${gtmProxyServers[$i]} killall -u $pgxcOwner -9 gtm_proxy
		  return
	  fi
	done
	echo ERROR: specified GTM proxy does not exist, $1
	return 1
}

function pgxc_reconnect_gtm_proxy
{
	# Reconnect to the current GTM master.   When failed over, the current Master must have been updated.
	# Remember to update gtm_proxy configuration file so that it connects to the new master at the next
	# start.
	# Please note that we assume GTM has already been failed over.
	# First argument is gtm_proxy nodename
    vecho ================================================================
    vecho Kill GTM Proxy $1

	local i
	for ((i=0; i< ${#gtmProxyNames[@]}; i++))
	do
	  if [ $1 == ${gtmProxyNames[$i]} ]
	  then
		  vecho doit ssh $pgxcOwner@${gtmProxyServers[$i]} gtm_ctl reconnect -Z gtm_proxy -D ${gtmProxyDirs[$i]} -o \
			\"-s $gtmMasterServer -t $gtmMasterPort\"

		  doit ssh $pgxcOwner@${gtmProxyServers[$i]} gtm_ctl reconnect -Z gtm_proxy -D ${gtmProxyDirs[$i]} -o \
			"-s $gtmMasterServer -t $gtmMasterPort"
		  vecho Reconfiguring GTM Proxy reflect reconnect.
		  ssh $pgxcOwner@${gtmProxyServers[$i]} "cat >> ${gtmProxyDirs[$i]}/gtm_proxy.conf" <<EOF
#===================================================
# Updated due to GTM Proxy reconnect
#        $datetime
gtm_host = $gtmMasterServer
gtm_port = $gtmMasterServer
#----End of reconfiguration -------------------------
EOF
		  return
	  fi
	done
	echo ERROR: specified GTM proxy does not exist, $1
	return 1

}

function pgxc_reconnect_gtm_proxy_all
{
	local i

	for ((i=0; i<${#gtmProxyNames[@]}; i++))
	do
	  pgxc_reconnect_gtm_proxy ${gtmProxyNames[$i]}
	done
}

#===============================================================================
#
# Coordinator Staff
#
#===============================================================================

function pgxc_init_coordinator_master
{
	# First argument is the nodename
    vecho ================================================================
    vecho Initialize coordinator master $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} ]
	  then
		  psql -p ${coordPorts[$i]} -h ${coordMasterServers[$i]}  -c 'select 1' postgres $pgxcOwner > /dev/null 2> /dev/null
		  if [ $? -eq 0 ]
		  then
			  echo ERROR: target coordinator master is running now.
			  return 1
		  fi
		  doit ssh $pgxcOwner@${coordMasterServers[$i]} rm -rf ${coordMasterDirs[$i]}
		  doit ssh $pgxcOwner@${coordMasterServers[$i]} mkdir -p ${coordMasterDirs[$i]}
		  doit ssh $pgxcOwner@${coordMasterServers[$i]} initdb --nodename ${coordNames[$i]}
		  vecho Configuring ${coordMasterServers[$i]}:${coordMasterDirs[$i]}/postgresql.conf
		  # Get effective GTM port and host.   If gtm_proxy is not found, then connect to GTM
		  local j
		  local targetGTMhost
		  local targetGTMport
		  targetGTMhost=$gtmMasterServer
		  gatgetGTMPort=$gtmMasterPort
		  for ((j=0; j< ${#gtmProxyServers[@]}; j++))
		  do
			if [ ${coordMasterServers[$i]} == ${gtmProxyServers[$j]} ]
			then
				targetGTMhost= ${gtmProxyServers[$j]}
				targetGTMport= ${gtmProxyPorts[$j]}
				break
			fi
		  done
		  if [ $coordExtraConfig != "" ]
		  then
			  vecho Configuring $pgxcOwner@${coordMasterServer[$i]}/postgresql.conf using $coordExtraConfig
			  ssh $pgxcOwner@${coordMasterServer[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" < $coordExtraConfig
		  fi
		  vecho Configuring  $pgxcOwner@${coordMasterServer[$i]}/postgresql.conf
		  ssh $pgxcOwner@${coordMasterServer[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" <<EOF
log_destination = 'stderr'
logging_collector = on
log_directory = 'pg_log'
listen_addresses = '*'
port = ${coordPorts[$i]}
max_connections = 100
pooler_port = ${poolerPorts[$i]}
gtm_host = '$targetGTMhost'
gtm_port = $targetGTMport
EOF
		  local j
		  if [ $coordExtraPgHba != "" ]
		  then
			  vecho Configuring ${coordMasterServers[$i]}:${coordMasterDirs[$i]}/pg_hba.conf using $coordExtraPgHba
			  ssh $pgxcOwner@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hab.conf" < $coordExtraPgHba
		  fi
		  vecho Configuring ${coordMasterServers[$i]}:${coordMasterDirs[$i]}/pg_hba.conf
		  for ((j=0; j< ${#coordListenAddresses[@]}; j++))
		  do
			ssh $pgxcOwner@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hba.conf" <<EOF
host all all ${coordListenAddresses[$j]} trust
EOF
		  done
		  return
	  fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

function pgxc_start_coordinator_master
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Start coordinator master $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} ]
	  then
		  psql -p ${coordPorts[$i]} -h ${coordMasterServers} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
		  if [ $? -eq 0 ]
		  then
			  echo ERROR: target coordinator master is running now.
			  return 1
		  fi
		  doit ssh $pgxcOwner@${coordMasterServers[$i]} "pg_ctl start -Z coordinator -D ${coordMasterDirs[$i]} -o -i > $tmpDir/coord.out"
		  ssh $pgxcOwner@${coordMasterServers[$i]} cat $tmpDir/coord.out
		  ssh $PgxcOwner@${coordMasterServers[$i]} rm -f $tmpDir/coord.out
		  return
	  fi
	done
	echo ERROR: specified coordinator is not configured, $1
	return 1
}

function pgxc_stop_coordinator_master
{
	# First arugument is the coordinator name
    vecho ================================================================
    vecho Stop coordinator master $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} ]
	  then
		  doit ssh $pgxcOwner@${coordMasterServers[$i]} pg_ctl stop -Z coordinator -D ${coordMasterDirs[$i]} $immediate
		  return
	  fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

function pgxc_kill_coordinator_master
{
	# First arugument is the coordinator name
    vecho ================================================================
    vecho Kill coordinator master $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} ]
	  then
		  doit ssh $pgxcOwner@${coordMasterServers[$i]} pg_ctl stop -Z coordinator -D ${coordMasterDirs[$i]} -m immediate
		  return
	  fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

# If a coordinator is not configured with the slave, we should remove it from the cluster
# when it fails.
function pgxc_remove_coordinator_master
{
	local i
	for ((i=0; i<${#coordNames[@]}; i++))
	do
	  if [[ ${coordNames[$i] == $1 ]]
	  then
		  local j
		  for ((j=0; j< ${#coordNames[@]}; j++))
		  do
			if [ $i -ne -$j ]
			then
				if [ ${coordMasterServers[$j]} != N/A ]
				then
					psql -p ${coordPorts[$j]} -h ${coordMasterServers[$j]} postgres $pgxcOwner -c "DROP NODE ${coordNames[$j]"
				fi
			else
				doit ssh $pgxcOwner@${coordMasterServers[$j]} pg_ctl stop -Z coordinator -D ${coordMaseterDirs[$j]} -m immediate
			fi
		  done
		  ${coordMasterServers[$i]} = N/A
		  ${coordMasterDirs[$i]} = N/A
		  cat >> $configFile <<EOF
#=========================================================
# Update due to coordinator master removal, $1, $datetime
coordMasterServers=( ${coordMasterServers[@]} )
coordMasterDirs=( ${coordMasterDirs[@] )
# End of update
EOF
	  fi
	done
}

function pgxc_init_coordinator_slave
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} }
	  then
		  if [ ${coordSlaveServers[$i]} == N/A ]
		  then
			  echo ERROR: slave for the coordinator $1 is not configured.
			  return 1
		  fi
		  # Coordinator master should be running
		  		  psql -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
		  if [ $? -ne 0 ]
		  then
			  echo ERROR: corresponding coordinator master is not running now, $1
			  return 1
		  fi
		  # Clean slave's directory
		  doit ssh $pgxcOwner@${coordSlaverServers[$i]} rm -rf ${coordSlaveDirs[$i]}
		  doit ssh $pgxcOwner@${coordSlaverServers[$i]} mkdir -p ${coordSlaveDirs[$i]}
		  # Reconfigure master's postgresql.conf
		  vecho -- Reconfigure master\'s postgresql.conf, ${coordMasterServers[$i]}:${coordMasterDirs[$i]}/postgresql.conf
		  ssh $pgxcOwner@${coordMasterServer[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" <<EOF
#=======================================
# Added to setup the slave, $datetime
wal_level = hot_standby
archive_mode = on
archive_command = 'rsync %p $pgxcOwner@${coordSlaveServers[$i]}:${coordArchLogDirs[$i]}/%f'
max_wal_senders = 10
EOF
		  # Reconfigure master's pg_hba.conf
		  vecho == Reconfigure master\'s pg_hba.conf, ${coordMasterServers[$i]}:${coordMasterDirs[$i]}/pg_hba.conf
		  local j
		  for ((j=0; j< ${#coordListenAddresses[@]}; j++))
		  do
			ssh $pgxcOwner${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hba.conf" <<EOF
#=========================================
# Added to setup the slave, $datetime
host replication all ${coordListenAddresses[$j]} trust
# End of Addition
EOF
		  done
		  # Restart the master --- some configuration parameter needs restart.  Reload is ot sufficient
		  doit ssh $pgxcOwner@${coordMasterServers[$i]} pg_ctl restart -Z coordinator -D ${coordMasterDirs[$i]}
		  # Obtain base backup of the master
		  doit ssh $pgxcOwner@${coordSlaveServers[$i]} pg_basebackup -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} -D ${coordSlaveDirs[$i]}
		  # Configure recovery.conf of the slave
		  vecho -- Configure slave\'s recovery.conf, ${coordSlaveServers[$i]}:${coordSlaveDirs[$i]}/recovery.conf
		  ssh $pgxcOwner@${coordSlaveServers[$i]} "cat >> ${coordSlaveDirs[$i]}/recovery.conf" <<EOF
#==========================================
# Added to startup the slave, $datetime
standby_mode = on
primary_conninfo = 'host = ${coordMasterServers[$i]} port = ${coordPorts[$i]} user = $pgxcOwner application_name = ${coordNames[$i]}'
restore_command = 'cp ${coordArchLogDirs[$i]}/%f %p'
archive_cleanup_command = 'pg_archivecleanup ${coordArchLogDirs[$i]} %r'
EOF
		  # Configure slave's postgresql.conf
		  vecho -- Configure slave\'s postgresql.conf, ${coordSlaveServers[$i]}:${coordSlaveDirs[$i]}/postgresql.conf
		  ssh $pgxcOwner@${coordSlaveServers[$i]} "cat >> ${coordSlaveDirs[$i]}/postgresql.conf" <<EOF
#==========================================
# Added to startup the slave, $dtetime
hot_standby = on
port = ${coordPorts[$i]}
EOF
	  fi
	  return
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

function pgxc_start_coordinator_slave
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} ]
	  then
		  if [ ${coordSlaveServers[$i]} == N/A ]
		  then
			  echo ERROR: slave for coordinator $1 is not configured.
			  return 1
		  fi
		  # Coordinator master should be running
  		  psql -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
		  if [ $? -ne 0 ]
		  then
			  echo ERROR: corresponding coordinator master is not running now, $1
			  return 1
		  fi
		  # Start the slave
		  doit ssh $pgxcOwner@${coordSlaveServers[$i]} "pg_ctl start -Z coordinator -D ${coordSlaveDirs[$i]} -o -i > $tmpDir/coord.out"
		  ssh $pgxcOwner@${coordSlaveServers[$i]} cat $tmpDir/coord.out
		  ssh $pgxcOwner@${coordSlaveServers[$i]} rm $tmpDir/coord.out
		  # Change the master to synchronous mode
		  vecho Change the master to synchrnous mode, $1
		  ssh $pgxcOwner@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}" <<EOF
#==========================================================
# Added to start the slave in sync. mode, $datetime
synchronous_commit = on
synchronous_standby_names = '${coordNames[$i]}'
# End of the addition
EOF
		  ssh $pgxcOwner@${coordMasterServers[$i]} pg_ctl reload -Z coordinator -D ${coordMasterDirs[$i]}
		  return
	  fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

# stop slave
function pgxc_stop_coordinator_slave
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} }
	  then
		  if [ ${coordSlaveServers[$i]} == N/A ]
		  then
			  echo ERROR: slave for the coordinator $1 is not configured.
		  fi
		  # If the master is running, master's switch replication to asynchronous mode.
  		  psql -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
		  if [ $? -eq 0 ]
		  then
			  # Switch Master to asynchronous mode.
			  vecho Switching master of $1 at ${coordMasterServer[$i]} to asynchronous replication mode.
			  ssh $pgxcOwner@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}" <<EOF
#=======================================
# Updated to trun off the slave $datetime
synchronous_standby_name = ''
# End of the update
EOF
			  doit ssh $pgxcOwner@${coordMasterServers[$i]} pg_ctl reload -Z coordinator -D ${coordMasterDirs[$i]}
		  fi
		  doit ssh $pgxcOwner@${coordSlaveServers[$i]} pg_ctl stop -Z coordinator -D ${coordSlaveDirs[$i]} $immediate
		  return;
	  fi
	done
	echo ERROR: Specified coordinator was not configured, $1
	return 1
}

function pgxc_stop_coordinator_master
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} }
	  then
		  if [ ${coordSlaveSrevers[$i]} == N/A ]
		  then
			  echo ERROR: slave of the coordinator $1 is not configured.
		  fi
		  doit ssh $pgxcOwner@${coordSlaveServers[$i]} pg_ctl stop -Z coordinator -D ${coordSlaveDirs[$i]} $immediat
		  return;
	  fi
	done
	echo $ERRO: specified coordinator does not exist, $1
	return 1
}

# Kill is an emergency operation.   No Master handling is included in this function.
function pgxc_kill_coordinator_slave
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} }
	  then
		  if [ ${coordServerNames[$i]} == N/A ]
		  then
			  echo ERROR: slave of the coordinator $1 is not configured.
			  return 1
		  fi
		  doit ssh $pgxcOwner@${coordSlaveServers[$i]} pg_ctl stop -Z coordinator -D ${coordSlaveDirs[$i]} -m immediate
		  return;
	  fi
	done
	echo ERROR: specified coordinator master is not configured, $1
	return 1
}

function pgxc_kill_coordinator_master
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} }
	  then
		  doit ssh $pgxcOwner@${coordMasterServers[$i]} pg_ctl stop -Z coordinator -D ${coordMasterDirs[$i]} -m immediate
		  return;
	  fi
	done
	echo ERROR: specified coordinator master is not configured, $1
	return 1
}

function pgxc_faiover_coordinator
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	local i
	for ((i=0; i< ${#coordNames[@]}; i++))
	do
	  if [ $1 == ${coordNames[$i]} }
	  then
		  if [ ${coordSlaveServers[$i]} == N/A ]
		  then
			  echo ERROR: No slave for the coordinator $1 found.
			  return 1
		  fi
		  # Find the new local gtm_proxy
		  local j
		  local targetGTMhost
		  local targetGTMport
		  targetGTMhost=""
		  targetGTMport=0
		  for ((j=0; j<${#gtmProxyServers[@]}; j++))
		  do
			if [ ${coordSlaveServers[$i]} == ${gtmProxyServers[$j]} ]
			then
				targetGTMhost= ${gtmProxyServers[$j]}
				targetGTMport= ${gtmProxyPorts[$j]}
				break
			fi
		  done
		  # gtm_proxy have to be configured properly
		  if [ $targetGTMhost == "" ]
		  then
			  echo ERROR: gtm_proxy is not configured at the server ${coordSlaveServers[$i]}
			  return 1
		  fi
		  # Now promote the slave
		  doit ssh $pgxcOwner@${coordSlaveServers[$i]} pg_ctl promote -Z coordinator -D ${coordSlaveDirs[$i]}
		  # Reconfigure new master's gtm_proxy
		  vecho Reconfiguring new gtm_proxy for ${coordSlaveSerers[$i]}:${coordSlaveDirs[$i]}/postgresql.conf
		  ssh $pgxcOwner@${coordSlaveServers[$i]} "cat >> ${coordSlaveDirs[$i]}/postgresql.conf" <<EOF
#=================================================
# Added to promote, $datetime
gtm_host = '$targetGTMhost'
gtm_port = $targetGTMport
# End of addition
EOF
		  # Restart the new master
		  doit ssh $pgxcOwner@${coordSlaveServers[$i]} "pg_ctl restart -Z coordinator -D ${coordSlaveDirs[$i]} -o -i > $tmpDir/coord.out"
		  ssh $pgxcOwner@${coordSlaveServers[$i]} cat $tmpDir/coord.out
		  ssh $pgxcOwner@${coordSlaveServers[$i]} rm $tmpDir/coord.out
		  # Update other coordinator with this new one ---> first, clean connection for all the users for all the databases
		  # Get all the available users
		  # First, find available coordinator other than this one.
		  local coord_wk=""
		  local coord_port=0
		  for ((j=0; j< ${#coordMasterServers[@]}; j++))
		  do
			if [ $j -ne -$i ]
			then
				coord_wk= ${coordMasterSerers[$j]}
				coord_port= ${coordPorts[$j]}
				break
			fi
		  done
		  if [ $coord_wk != "" ]
		  then
			  # There's at least one coordinator working.  Need to do "CLEAN CONNECTION"
              # Then get all the users
			  cat > $localTmpDir/command.sql <<EOF
\pset tuples_only on
select usename from pg_user;
\q
EOF
			  psql -p $coord_port -h $coord_wk postgres $pgxcOwner -f $localTmpDir/command.sql -o $localTmpDir/command.out >/dev/null
			  local users
			  users= `cat $localTmpDir/command.out`
			  rm $localTmpDir/command.sql $localTmpDir/command.out
		  	  # Clean all the pooler connections and update the node registration
			  for ((j=0; j< ${#coordMasterServers[@]}; j++))
			  do
				for user in $users
			    do
				  if [ $j -ne $i ]
				  then
					  psql -p ${coordPorts[$j]} -h ${coordMasterServers[$j]} postgres $pgxcOwner -c "CLEAN CONNECTION TO ALL FOR USER $user"
				  fi
				done
				psql -p ${coordPorts[$j]} -h ${coordMasterServers[$j]} postgres $pgxcOwner -c "ALTER NODE ${coordNames[$i]} WITH (HOST='${coordSlaveHosts[$i]}', PORT=${coordPorts[$i]})"
			  done
		  fi
          # Then update the configuration file with this new configuration
		  coordMasterServers[$i]=${coordSlaverServers[$i]}
		  coordSlaveServers[$i]=N/A
		  coordSlaveDirs[$i]=N/A
		  # Update the configuration file with this new configuration
		  cat >> $configFile <<EOF
#=====================================================
# Updated due to the coordinator failover, $1, $datetime
coordMasterServers=( ${coordMasterServers[@]} )
coordSlaveServers=( ${coordSlaveServers[@]} )
coordSlaveDirs=( ${coordSlaveDirs[@]} )
# End of the update
EOF
		  return;
	  fi
	done
	echo ERROR: specified coordinator $1 not configured.
}

#===============================================================================
#
# Datanode staff
#
#===============================================================================

function pgxc_init_datanode_master
{
	# First argument is the nodename
    vecho ================================================================
    vecho Initialize datanode master $1

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodeNames[$i]} ]
	  then
		  psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
		  if [ $? -eq 0 ]
		  then
			  echo ERROR: target datanode master is running now.
			  return 1
		  fi
		  doit ssh $pgxcOwner@${datanodeMasterServers[$i]} rm -rf ${datanodeMasterDirs[$i]}
		  doit ssh $pgxcOwner@${datanodeMasterServers[$i]} mkdir -p ${datanodeMasterDirs[$i]}
		  doit ssh $pgxcOwner@${datanodeMasterServers[$i]} initdb --nodename ${datanodeNames[$i]}
		  vecho Configuring ${datanodeMasterServers[$i]}:${datanodeMasterDirs[$i]}/postgresql.conf
		  if [ $datanodeExtraConfig != "" ]
		  then
			  ssh $pgxcOwner@${datanodeMasterServer[$j]} "cat >> ${datanodeMasterDir[$i]}/postgresql.conf" < $datnodeExtraConfig
		  fi
		  # Get effective GTM port and host.   If gtm_proxy is not found, then connect to GTM
		  local j
		  local targetGTMhost
		  local targetGTMport
		  for ((j=0; j< ${#gtmProxySerevrs[@]}; j++))
		  do
			if [ ${datanodeMasterServers[$i]} == ${gtmProxyServers[$j]) ]
			then
				targetGTMhost= ${gtmProxyServers[$j]}
				targetGTMport= ${gtmProxyPorts[$j]}
				break
			fi
		  done
		  ssh $pgxcOwner@${datanodeMasterServer[$j]} "cat >> ${datanodeMasterDirs[$i]}/postgresql.conf" <<EOF
log_destination = 'stderr'
logging_collector = on
log_directory = 'pg_log'
listen_addresses = '*'
port = ${datanodePorts[$i]}
max_connections = 1000
gtm_host = '$targetGTMhost'
gtm_port = $targetGTMport
EOF
		  vecho Configuring ${datanodeMasterServers[$i]}:${coordMasterDirs[$i]}/pg_hba.conf
		  local j
		  if [ $datanodeExtraPgHba != "" ]
		  then
			  ssh $pgxcOwner@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/pg_hba.conf" < $datanodeExtraPgHba
		  fi
		  for ((j=0; j<${#datanodeListenAddresses[@]}; j++))
		  do
			ssh pgxcOwner@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/pg_hba.conf" <<EOF
host all all ${datanodeListenAddresses[$j]}
EOF
		  done
		  return
	  fi
	done
	echo ERROR: specified datanode des not exist, $1
	return 1
}

function pgxc_start_datanode_master
{
	# First argument is the nodename
    vecho ================================================================
    vecho Start datanode master $1

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodeNames[$i]} ]
	  then
		  psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
		  if [ $? -eq 0 ]
		  then
			  echo ERROR: target datanode master is running now.
			  return 1
		  fi
		  doit ssh $pgxcOwner@${datanodeMasterServers[$i]} "pg_ctl start -Z datanode -D ${datanodeMasterDirs[$i]} -o -i > $tmpDir/datanode.out"
		  ssh $pgxcOwner@${datanodeMasterServers[$i]} cat $tmpDir/datanode.out
		  ssh $pgxcOwner@${datanodeMasterServers[$i]} rm -f $tmpDir/datanode.out
		  return
	  fi
	done
	echo ERROR: specified datanode is not configured, $1
	return 1
}

function pgxc_stop_datanode_master
{
	# First argument is the nodename
    vecho ================================================================
    vecho Start datanode master $1

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodeNames[$i]} ]
	  then
		  doit ssh $pgxcOwner@${datanodeMasterServers[$i]} pg_ctl stop -Z datanode -D ${datanodeMasterDirs[$i]} $immediate
		  return
	  fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}


function pgxc_kill_datanode_master
{
	# First arugument is the nodename
    vecho ================================================================
    vecho Kill coordinator master $1

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodedNames[$i]} ]
	  then
		  doit ssh $pgxcOwner@${datanodeMasterServers[$i]} pg_ctl stop -Z datanode -D ${datanodeMasterDirs[$i]} -m immediate
		  return
	  fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

function pgxc_init_datanode_slave
{
	# First argument is the datanode name
    vecho ================================================================
    vecho Initialize datanode slave $1

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodeNames[$i]} }
	  then
		  if [ ${datanodeSlaveServers[$i]} == N/A ]
		  then
			  echo ERROR: slave for the datanode $1 is not configured.
			  return 1
		  fi
		  # Coordinator master should be running
		  		  psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
		  if [ $? -ne 0 ]
		  then
			  echo ERROR: corresponding datanode master is not running now, $1
			  return 1
		  fi
		  # Clean slave's directory
		  doit ssh $pgxcOwner@${datanodeSlaverServers[$i]} rm -rf ${datanodeSlaveDirs[$i]}
		  doit ssh $pgxcOwner@${datanodeSlaverServers[$i]} mkdir -p ${datanodeSlaveDirs[$i]}
		  # Reconfigure master's postgresql.conf
		  vecho -- Reconfigure master\'s postgresql.conf, ${datanodeMasterServers[$i]}:${datanodeMasterDirs[$i]}/postgresql.conf
		  ssh $pgxcOwner@${coordMasterServer[$i]} "cat >> ${datanodeMasterDirs[$i]}/postgresql.conf" <<EOF
#=======================================
# Added to setup the slave, $datetime
wal_level = hot_standby
archive_mode = on
archive_command = 'rsync %p $pgxcOwner@${datanodeSlaveServers[$i]}:${datanodeArchLogDirs[$i]}/%f'
max_wal_senders = 10
EOF
		  # Configure slave's postgresql.conf
		  vecho -- Configure slave\'s postgresql.conf, ${doatanodeSlaveServers[$i]}:${datanodeSlaveDirs[$i]}/postgresql.conf
		  ssh $pgxcOwner@${datanodeSlaveServers[$i]} "cat >> ${datanodeSlaveDirs[$i]}/postgresql.conf" <<EOF
#==========================================
# Added to startup the slave, $dtetime
hot_standby = on
port = ${datanodePorts[$i]}
EOF
	  fi
	  return
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

function pgxc_start_datanode_slave
{
	# First argument is the datanode name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodeNames[$i]} }
	  then
		  if [ ${datanodeSlaveServers[$i]} == N/A ]
		  then
			  echo ERROR: slave for datanode $1 is not configured.
			  return 1
		  fi
		  # Datanode master should be running
  		  psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
		  if [ $? -ne 0 ]
		  then
			  echo ERROR: corresponding datanode master is not running now, $1
			  return 1
		  fi
		  # Start the slave
		  doit ssh $pgxcOwner@${datanodeSlaveServers[$i]} "pg_ctl start -Z datanode -D ${datanodeSlaveDirs[$i]} -o -i > $tmpDir/coord.out"
		  ssh $pgxcOwner@${datanodeSlaveServers[$i]} cat $tmpDir/coord.out
		  ssh $pgxcOwner@${datanodeSlaveServers[$i]} rm $tmpDir/coord.out
		  # Change the master to synchronous mode
		  vecho Change the master to synchrnous mode, $1
		  ssh $pgxcOwner@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}" <<EOF
#==========================================================
# Added to start the slave in sync. mode, $datetime
synchronous_commit = on
synchronous_standby_names = '${datanodeNames[$i]}'
# End of the addition
EOF
		  ssh $pgxcOwner@${datanodeMasterServers[$i]} pg_ctl reload -Z datanode -D ${datanodeMasterDirs[$i]}
		  return
	  fi
	done
	echo ERROR: specified datanode does not exist, $1
	return 1
}

function pgxc_stop_datanode_slave
{
	# First argument is the datanode name
    vecho ================================================================
    vecho Initialize datanode slave $1

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodeNames[$i]} }
	  then
		  if [ ${datanodeSlaveServers[$i]} == N/A ]
		  then
			  echo ERROR: slave for the datanode $1 is not configured.
		  fi
		  # If the master is running, master's switch replication to asynchronous mode.
  		  psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
		  if [ $? -eq 0 ]
		  then
			  # Switch Master to asynchronous mode.
			  vecho Switching master of $1 at ${datanodeMasterServer[$i]} to asynchronous replication mode.
			  ssh $pgxcOwner@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}" <<EOF
#=======================================
# Updated to trun off the slave $datetime
synchronous_standby_name = ''
# End of the update
EOF
			  doit ssh $pgxcOwner@${datanodeMasterServers[$i]} pg_ctl reload -Z datanode -D ${datanodeMasterDirs[$i]}
		  fi
		  doit ssh $pgxcOwner@${datanodeSlaveServers[$i]} pg_ctl stop -Z datanode -D ${datanodeSlaveDirs[$i]} $immediate
		  return;
	  fi
	done
	echo ERROR: Specified datanode was not configureed, $1
	return 1
}

function pgxc_kill_datanode_master
{
	# First argument is the datanodeinator name
    vecho ================================================================
    vecho Initialize datanode slave $1

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodeNames[$i]} }
	  then
		  doit ssh $pgxcOwner@${datanodeMasterServers[$i]} pg_ctl stop -Z datanode -D ${datanodeMasterDirs[$i]} -m immediate
		  return;
	  fi
	done
	echo ERROR: specified datanode master is not configured, $1
	return 1
}

function pgxc_faiover_datanode
{
	# First argument is the datanode name
    vecho ================================================================
    vecho Initialize datanode slave $1

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++))
	do
	  if [ $1 == ${datanodeNames[$i]} }
	  then
		  if [ ${datanodeSlaveServers[$i]} == N/A ]
		  then
			  echo ERROR: No slave for the datanode $1 found.
			  return 1
		  fi
		  # Find the new local gtm_proxy
		  local j
		  local targetGTMhost
		  local targetGTMport
		  targetGTMhost=""
		  targetGTMport=0
		  for ((j=0; j<${#gtmProxyServers[@]}; j++))
		  do
			if [ ${datanodeSlaveServers[$i]} == ${gtmProxyServers[$j]} ]
			then
				targetGTMhost= ${gtmProxyServers[$j]}
				targetGTMport= ${gtmProxyPorts[$j]}
				break
			fi
		  done
		  # gtm_proxy have to be configured properly
		  if [ $targetGTMhost == "" ]
		  then
			  echo ERROR: gtm_proxy is not configured at the server ${datanodeSlaveServers[$i]}
			  return 1
		  fi
		  # Now promote the slave
		  doit ssh $pgxcOwner@${datanodeSlaveServers[$i]} pg_ctl promote -Z datanode -D ${datanodeSlaveDirs[$i]}
		  # Reconfigure new master's gtm_proxy
		  vecho Reconfiguring new gtm_proxy for ${datanodeSlaveSerers[$i]}:${datanodeSlaveDirs[$i]}/postgresql.conf
		  ssh $pgxcOwner@${datanodeSlaveServers[$i]} "cat >> ${datanodeSlaveDirs[$i]}/postgresql.conf" <<EOF
#=================================================
# Added to promote, $datetime
gtm_host = '$targetGTMhost'
gtm_port = $targetGTMport
# End of addition
EOF
		  # Restart the new master
		  doit ssh $pgxcOwner@${datanodeSlaveServers[$i]} "pg_ctl restart -Z datanode -D ${datanodeSlaveDirs[$i]} -o -i > $tmpDir/datanode.out"
		  ssh $pgxcOwner@${datanodeSlaveServers[$i]} cat $tmpDir/datanode.out
		  ssh $pgxcOwner@${datanodeSlaveServers[$i]} rm $tmpDir/datanode.out
		  # Update other datanode with this new one ---> first, clean connection for all the users for all the databases
		  # Get all the available users
		  # First, find available coordinator.  There must be at least one.
		  local coord_wk=""
		  local coord_port=0
		  for ((j=0; j< ${#coordMasterServers[@]}; j++))
		  do
			if [ ${coordMasterSrever[$j]} != N/A ]
			then
				coord_wk= ${coordMasterSerers[$j]}
				coord_port= ${coordPorts[$j]}
				break
			fi
		  done
		  if [ $coord_wk == "" ]
		  then
			  echo ERROR: no working coordinator found.   The Postgres-XC cannot continue to run.
			  exit 1
		  fi
		  # Then get all the users
		  cat > $localTmpDir/command.sql <<EOF
\pset tuples_only on
select usename from pg_user;
\q
EOF
		  psql -p $coord_port -h $coord_wk postgres $pgxcOwner -f $localTmpDir/command.sql -o $localTmpDir/command.out >/dev/null
		  local users
		  users= `cat $localTmpDir/command.out`
		  rm $localTmpDir/command.sql $localTmpDir/command.out
		  # Clean all the pooler connections and update the node registration
		  for ((j=0; j< ${#coordMasterServers[@]}; j++))
		  do
			for user in $users
			do
			  cat > $localTmpDir/command.sql <<EOF
CLEAN CONNECTION TO ALL FOR USER $user;
ALTER NODE ${datanodeNames[$i]} WITH (HOST = '${datanodeSlaveHosts[$i]}', PORT=${datanodePorts[$i]});
\q
EOF
			  psql -p ${coordPorts[$j]} -h ${coordMasterServers[$j]} postgres $pgxcOwner -f $localTmpDir/command.sql
			  rm $locaTmpDir/command.sql
			done
		  done
		  # Then update the configuration file with this new configuration
		  datanodeMasterServers[$i]=${datanodeSlaverServers[$i]}
		  datanodeSlaveServers[$i]=N/A
		  datanodeSlaveDirs[$i]=N/A
		  # Update the configuration file with this new configuration
		  cat >> $configFile <<EOF
#=====================================================
# Updated due to the datanode failover, $1, $datetime
datanodeMasterServers=( ${datanodeMasterServers[@]} )
datanodeSlaveServers=( ${datanodeSlaveServers[@]} )
datanodeSlaveDirs=( ${datanodeSlaveDirs[@]} )
# End of the update
EOF
		  return;
	  fi
	done
	echo ERROR: specified datanode $1 not configured.
}

#=====================================================================================#
#                                                                                     #
# EXECUTION SECTION                                                                   #
#                                                                                     #
#=====================================================================================#

echo '******' 'pgxc test tool start. ' `date '+%Y.%m.%d_%k.%M.%S'`  '*********'

#
# Check the configuration file
#
if [ $PGXCCONFIG != N/A ]
then
	configFile=$PGXCCONFIG
fi

#=======================================================
# Things to be done at first
#=======================================================

# Handle options
stillmore=y
while [ $stillmore == y ]
do
  if [ $# -gt 0 ]
  then
	  case $1 in
		  -v )
			  shift;
			  verbose=y;;
		  --verbose )
			  shift;
			  verbose=y;;
		  -c ) # Configuraton file
			  shift;
			  if [ $# -le 0 ]
			  then
				  echo ERROR: no -c option value found
				  exit 1
			  else
				  $configFile=$1
				  shift
			  fi;;
		  --configuration ) # Configuraion file
			  shift;
			  if [ $# -le 0 ]
			  then
				  echo ERROR: no --configuration option value found
				  exit 1
			  else
				  $configFile=$1
				  shift
			  fi;;
		  * )
			  stillmore=n;;
	  esac
  fi
done


# Read configuration file
# source $configFile

# Check if slaves are configured and makeup each server to N/A if needed
handle_no_slaves

# Construct the server list
makeServerList

#----- Aug.27, 2012, Checked up to here!!

# == Check if the configuration is valid =======================
# Check if each component is assigned separate resources (port and directory)
# for each server.
for ((i=0; i< ${#allServers[@]}; i++))
do
  verifyResource ${allServers[$i]}
done