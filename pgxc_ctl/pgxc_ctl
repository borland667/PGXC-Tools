#!/bin/bash
#
# TODO
# * Backup configuration file (at update, too) to some remote site for pgxc_ctl HA feature too.
# * Write output of *_ctl, intdb and intgtm result to log files
# * Write every operation to the log.  Log file can be specified with --l file or --log file
# * Configure log level
# * Switch log		log file
# * Log option to the configuration file so that this can be failed over.
# * Log to a remote server?
# * Multiple log?
#
# Configuration file.   Configuration file can be specified as -c option of
# the command like, or PGXCCONFIG environment variable.  If both are
# not specified, the following configuration file will be used.
#
# Change in the cluster status due to failover will be added to the configuration file so that
# new master can be invoked as the master when restarted.
#
# All such addition will be tagged with proper comment and date/time info.   If you'd like to
# cancel such changes, you can remove or comment-out such additional lines.
#
configFile=$HOME/pgxc/pgxcConfig
#
#==========================================================================================================================
#
# Configuration Section
#
#		This section should be in the file $configFile for
#		user's configuration.
#
# Several assumptons:
# 1) configuration file will be set to data directory.
#    configuration file name is fixed to postgresql.conf
# 2) pg_hba.conf will be set to data directory.  File name is
#    fixed to pg_hba.conf
#
#================================================================
# MEMO
#
# max_connections, min_pool_size, max_pool_size --> should be configurable!
# They're not cluster specific.  So we may give a chance to include
# these specific options to be included from external files.
# They should not change by failover so they just have to be
# configured at first time only.
#===============================================================
#
#---- OVERALL -----------------------------------------------------------------------------------------------------------
#
pgxcOwner=koichi		# owner of the Postgres-XC database cluster.  Here, we use this
						# both as linus user and database user.  This must be
						# the super user of each coordinator and datanode.
pgxcUser=$pgxcOwner		# OS user of Postgres-XC owner

pgxcInstallDir=$HOME/pgxc

tmpDir=/tmp					# temporary dir used in XC servers
localTmpDir=$tmpDir			# temporary dir used here locally

logDir=$HOME/pgxc/pgxc_ctl_log


#---- GTM --------------------------------------------------------------------------------------------------------------

# GTM is mandatory.  You must have at least (and only) one GTM master in your Postgres-XC cluster.
# If GTM crashes and you need to reconfigure it, you can do it by pgxc_update_gtm command to update
# GTM master with others.   Of course, we provide pgxc_remove_gtm command to remove it.  This command
# will not stop the current GTM.  It is up to the operator.

#---- Overall -------
gtmName=gtm

#---- GTM Master -----------------------------------------------

#---- Overall ----
gtmMasterServer=node13
gtmMasterPort=20001
gtmMasterDir=$HOME/pgxc/nodes/gtm

#---- Configuration ---
gtmExtraConfig=none			# Will be added gtm.conf for both Master and Slave (done at initilization only)
gtmMasterSpecificExtraConfig=none	# Will be added to Master's gtm.conf (done at initialization only)

#---- GTM Slave -----------------------------------------------

# Because GTM is a key component to maintain database consistency, you may want to configure GTM slave
# for backup.

#---- Overall ------
gtmSlave=y					# Specify y if you configure GTM Slave.   Otherwise, GTM slave will not be configured and
							# all the following variables will be reset.
gtmSlaveServer=node12		# value none means GTM slave is not available.  Give none if you don't configure GTM Slave.
gtmSlavePort=20001			# Not used if you don't configure GTM slave.
gtmSlaveDir=$HOME/pgxc/nodes/gtm	# Not used if you don't configure GTM slave.
# Please note that when you have GTM failover, then there will be no slave available until you configure the slave
# again. (pgxc_add_gtm_slave function will handle it)

#---- Configuration ----
gtmSlaveSpecificExtraConfig=none # Will be added to Slave's gtm.conf (done at initialization only)

#---- GTM Proxy -------------------------------------------------------------------------------------------------------
# GTM proxy will be selected based upon which server each component runs on.
# When fails over to the slave, the slave inherits its master's gtm proxy.  It should be
# reconfigured based upon the new location.
#
# To do so, slave should be restarted.   So pg_ctl promote -> (edit postgresql.conf and recovery.conf) -> pg_ctl restart
#
# You don't have to configure GTM Proxy if you dont' configure GTM slave or you are happy if every component connects
# to GTM Master directly.  If you configure GTL slave, you must configure GTM proxy too.

#---- Shortcuts ------
gtmProxyDir=$HOME/pgxc/nodes/gtm_pxy

#---- Overall -------
gtmProxy=y				# Specify y if you conifugre at least one GTM proxy.   You may not configure gtm proxies
						# only when you dont' configure GTM slaves.
						# If you specify this value not to y, the following parameters will be set to default empty values.
						# If we find there're no valid Proxy server names (means, every servers are specified
						# as none), then gtmProxy value will be set to "n" and all the entries will be set to
						# empty values.
gtmProxyNames=(gtm_pxy1 gtm_pxy2 gtm_pxy3 gtm_pxy4)	# No used if it is not configured
gtmProxyServers=(node01 node02 node03 node04)			# Specify none if you dont' configure it.
gtmProxyPorts=(20001 20001 20001 20001)				# Not used if it is not configured.
gtmProxyDirs=($gtmProxyDir $gtmProxyDir $gtmProxyDir $gtmProxyDir)	# Not used if it is not configured.

#---- Configuration ----
gtmPxyExtraConfig=none		# Extra configuration parameter for gtm_proxy
gtmPxySpecificExtraConfig=(none none none none)

#---- Coordinators ----------------------------------------------------------------------------------------------------

#---- shortcuts ----------
coordMasterDir=$HOME/pgxc/nodes/coord
coordSlaveDir=$HOME/pgxc/nodes/coord_slave
coordArchLogDir=$HOME/pgxc/nodes/coord_archlog

#---- Overall ------------
coordNames=(coord1 coord2 coord3 coord4)		# Master and slave use the same name
coordPorts=(20004 20005 20004 20005)			# Master and slave use the same port
poolerPorts=(20010 20011 20010 20011)			# Master and slave use the same pooler port
coordPgHbaEntries=(192.168.1.0/24)				# Assumes that all the coordinator (master/slave) accepts
												# the same connection
												# This entry allows only $pgxcOwner to connect.
												# If you'd like to setup another connection, you should
												# supply these entries through files specified below.
# Note: The above parameter is extracted as "host all all 0.0.0.0/0 trust".   If you don't want
# such setups, specify the value () to this variable and suplly what you want using coordExtraPgHba
# and/or coordSpecificExtraPgHba variables.

#---- Master -------------
coordMasterServers=(node01 node02 node03 node04)		# none means this master is not available
coordMasterDirs=($coordMasterDir $coordMasterDir $coordMasterDir $coordMasterDir)
coordMaxWALsernder=5	# max_wal_senders: needed to configure slave. If zero value is specified,
						# it is expected to supply this parameter explicitly by external files
						# specified in the following.	If you don't configure slaves, leave this value to zero.
coordMaxWALSenders=($coordMaxWALsernder $coordMaxWALsernder $coordMaxWALsernder $coordMaxWALsernder)
						# max_wal_senders configuration for each coordinator.

#---- Slave -------------
coordSlave=y			# Specify y if you configure at least one coordiantor slave.  Otherwise, the following
						# configuration parameters will be set to empty values.
						# If no effective server names are found (that is, every servers are specified as none),
						# then coordSlave value will be set to n and all the following values will be set to
						# empty values.
coordSlaveServers=(node02 node03 node04 node01)			# none means this slave is not available
coordSlaveDirs=($coordSlaveDir $coordSlaveDir $coordSlaveDir $coordSlaveDir)
coordArchLogDirs=($coordArchLogDir $coordArchLogDir $coordArchLogDir $coordArchLogDir)

#---- Configuration files---
# Need these when you'd like setup specific non-default configuration 
# These files will go to corresponding files for the master.
# You may supply your bash script to setup extra config lines and extra pg_hba.conf entries 
# Or you may supply these files manually.
coordExtraConfig=none	# Extra configuration file for coordinators.  This file will be added to all the coordinators'
						# postgresql.conf
coordSpecificExraConfig=(none none none none)
coordExtraPgHba=none	# Extra entry for pg_hba.conf.  This file will be added to all the coordinators' pg_hba.conf
coordSpecificExtraPgHba=(none none none none)

#---- Datanodes -------------------------------------------------------------------------------------------------------

#---- Shortcuts --------------
datanodeMasterDir=$HOME/pgxc/nodes/dn_master
datanodeSlaveDir=$HOME/pgxc/nodes/dn_slave
datanodeArchLogDir=$HOME/pgxc/nodes/datanode_archlog

#---- Overall ---------------
primaryDatanode=datanode1				# Primary Node.
datanodeNames=(datanode1 datanode2 datanode3 datanode4)
datanodePorts=(20008 20009 20008 20009)	# Master and slave use the same port!
datanodePgHbaEntries=(192.168.1.0/24)	# Assumes that all the coordinator (master/slave) accepts
										# the same connection
										# This list sets up pg_hba.conf for $pgxcOwner user.
										# If you'd like to setup other entries, supply them
										# through extra configuration files specified below.
# Note: The above parameter is extracted as "host all all 0.0.0.0/0 trust".   If you don't want
# such setups, specify the value () to this variable and suplly what you want using datanodeExtraPgHba
# and/or datanodeSpecificExtraPgHba variables.

#---- Master ----------------
datanodeMasterServers=(node01 node02 node03 node04)	# none means this master is not available.
													# This means that there should be the master but is down.
													# The cluster is not operational until the master is
													# recovered and ready to run.	
datanodeMasterDirs=($datanodeMasterDir $datanodeMasterDir $datanodeMasterDir $datanodeMasterDir)
datanodeMaxWalSender=5								# max_wal_senders: needed to configure slave. If zero value is 
													# specified, it is expected this parameter is explicitly supplied
													# by external configuration files.
													# If you don't configure slaves, leave this value zero.
datanodeMaxWalSenders=($datanodeMaxWalSender $datanodeMaxWalSender $datanodeMaxWalSender $datanodeMaxWalSender)
						# max_wal_senders configuration for each datanode

#---- Slave -----------------
datanodeSlave=y			# Specify y if you configure at least one coordiantor slave.  Otherwise, the following
						# configuration parameters will be set to empty values.
						# If no effective server names are found (that is, every servers are specified as none),
						# then datanodeSlave value will be set to n and all the following values will be set to
						# empty values.
datanodeSlaveServers=(node02 node03 node04 node01)	# value none means this slave is not available
datanodeSlaveDirs=($datanodeSlaveDir $datanodeSlaveDir $datanodeSlaveDir $datanodeSlaveDir)
datanodeArchLogDirs=( $datanodeArchLogDir $datanodeArchLogDir $datanodeArchLogDir $datanodeArchLogDir )

# ---- Configuration files ---
# You may supply your bash script to setup extra config lines and extra pg_hba.conf entries here.
# These files will go to corresponding files for the master.
# Or you may supply these files manually.
datanodeExtraConfig=none	# Extra configuration file for datanodes.  This file will be added to all the 
							# datanodes' postgresql.conf
datanodeSpecificExtraConfig=(none none none none)
datanodeExtraPgHba=none		# Extra entry for pg_hba.conf.  This file will be added to all the datanodes' postgresql.conf
datanodeSpecificExtraPgHba=(none none none none)

#
#		End of Configuration Section
#
#==========================================================================================================================

# Common variables ######################################################################
xc_prompt='PGXC$ '
interactive=n
verbose=n
#============================================================
#
# Common functions
#
#============================================================

function vecho
{
	if [ "$verbose" == y ]; then
		echo $*
	fi
}

function iecho
{
	if [ "$interactive" == y ]; then
		echo $*
	fi
}

function doit
{
	vecho $*
	$*
}

function doall
{
	local i
    vecho doall target: "(" ${allServers[@]} ")"
    for (( i=0; i< ${#allServers[@]}; i++ )); do
		if [ ${allServers[$i]} != none ] && [ ${allServers[$i]} != N/A ]; then
			vecho "... ${allServers[$i]}: $* ..."
			ssh $pgxcUser@${allServers[$i]} $*
		fi
    done
}

function Doall
{
	local i
    vecho Doall target: "(" ${DoallTarget[@]} ")"
    for (( i=0; i< ${#DoallTarget[@]}; i++ )); do
		if [ ${DoallTarget[$i]} != none ] && [ ${DoallTarget[$i]} != N/A ]; then
			vecho "${DoallTarget[$i]}: $* ..."
			ssh $pgxcUser@${DoallTarget[$i]} $*
		fi
    done
}

function cpall
{
	local i
    vecho cpall target: "(" ${allServers[@]} ")"
    for (( i=0; i < ${#allServers[@]}; i++ )); do
		if [ ${allServers[$i]} != none ] && [ ${allServers[$i]} != N/A ]; then
			vecho scp -r $1 $pgxcUser@${allServers[$i]}:$2
			scp -r $1 $pgxcUser@${allServers[$i]}:$2
		fi
    done
}

function Cpall
{
	local i
    vecho Cpall target: "(" ${CpallTarget[@]} ")"
    for (( i=0; i< ${#CpallTarget[@]}; i++ )); do
		if [ ${CpallTarget[$i]} != none ] && [ ${CpallTarget[$i]} != N/A ]; then
			vecho scp -r $1 $pgxcUser@${CpallTarget[$i]}:$2
			scp -r $1 $pgxcUser@${CpallTarget[$i]}:$2
		fi
    done
}


function set_verbose
{
	if [ $# -le 0 ]; then
		iecho Specify y/n/on/off
		return 1
	fi
	case $1 in
		y )
			verbose=y;;
		n )
			verbose=n;;
		on )
			verbose=y;;
		off )
			verbose=n;;
		* )
			iecho Specify y/n/on/off
			return 1;;
	esac
	iecho Verbose set to $verbose
	return 0
}

function set_interactive
{
	if [ $# -le 0 ]; then
		iecho Specify y/n/on/off
		return 1
	fi
	case $1 in
		y )
			interactive=y;;
		n )
			interactive=n;;
		on )
			interactive=y;;
		off )
			interactive=n;;
		* )
			iecho Specify y/n/on/off
			return 1;;
	esac
	iecho Interactive set to $interactive
	return 0
}
function set_prompt
{
	xc_prompt=$1
	iecho Prompt set to $xc_prompt
}

function readyesno
{
	if [ $# -ne 1 ];then
		echo n
		return 1
	fi
	local yesno
	read yesno
	case $yesno in
		y )
			echo y;;
		yes )
			echo y;;
		n )
			echo n;;
		no )
			echo n;;
		* )
			echo $1;;
	esac
}
datetime=`date +%y%m%d_%H%M`
immediate="-m fast"			# option for pg_ctl stop.

allServers=()				# List of all the servers which appear in this configuation.

##############################################################################################################
#
#   FUNCTIONS 
#
##############################################################################################################

function create_config_file
{
	# The configuration file is just a copy of the above configuraiton section.   If you modify the above,
	# you should reflect it to the below too.
	cat > $configFile <<EOF
#!/bin/bash
# The context will be supplied finally...
EOF
}

#
# A couple of following functions helps to kill all the processes of specified
# coordinator or datanode.  Target coordinator or datanode are identified by
# the server and the working directory.
#
# They depend upon ps output format.   It depends upon specific operating system
# and may need rewrite.
#
function extract_pid
{
	# Caution: ps format deeply depends upon operating system.   
	# Checked for CentOS (RHEL), Ubuntu 10.4, ubuntu 12.4.
	local uid
	local pid
	local extra
	read uid pid extra
	if [ $? == 0 ]; then
		echo $pid
	else
		echo none
	fi
}

function get_postmaster_pid
{
	# arguments are server and directory
	# Caution: ps format deeply depends upon operating system.   
	# Checked for CentOS (RHEL), Ubuntu 10.4, ubuntu 12.4.
	# This assumes that grep extracts at most one line.
	ssh $pgxcUser@$1 ps -f -C postgres | grep $2 | extract_pid
}

function kill_all_child_parent
{
	# argument is the target node name and the PID of the parent.
	if [ $# -ne 2 ]; then
		iecho Specify nodename and parent PID
		return 1
	fi
	if [ "$2" == "" ] || [ $2 == none ] || [ $2 == N/A ]; then
		return 1
	fi
	for i in `ssh $pgxcUser@$1 pgrep -P $2`; do
		ssh $pgxcUser@$1 kill -9 $i
	done
	ssh $pgxcUser@$1 kill -9 $2
}

#----------------------------------------------
# DEBUG Aid
#----------------------------------------------

# Debug --> Should be cleaned

DEBUG=n

function set_debug
{
	if [ $# -le 0 ]; then
		iecho Specify y/n/on/off
		return 1
	fi
	case $1 in
		y )
			DEBUG=y
			;;
		n )
			DEBUG=n;;
		on )
			DEBUG=y;;
		off )
			DEBUG=n;;
		* )
			iecho Specify y/n/on/off
			return 1;;
	esac
	iecho Debug mode set to $DEBUG
	return 0
}

function funcname
{
	if [ "$DEBUG" == y ];	then
		echo '******** ' "$1() called" ' **********'
	fi
}

function decho
{
	if [ "$DEBUG" == y ]; then
		echo $*
	fi
}

function ddo
{
	if [ "$DEBUG" == y ]; then
		$*
	fi
}

# Extract the server list into ${allServers[@]}
# Check if there's no duplicate elements in ${allServers[@]}.   If not, then add the argument
# to ${allServers[@]}
function addServer
{
	local append
	local i

	append=y
	if [ "$1" == 'none' ] || [ "$i" == N/A ]; then
		return
	fi
	for((i=0; i<${#allServers[@]}; i++)); do
		if [ ${allServers[$i]} == "$1" ]; then
			append=n
			break
		fi
	done
	if [ $append == y ]; then
		allServers[$i]=$1
	fi
}

# Build unique server list
#
function makeServerList
{
	local i

	# GTM Master
	if [ $gtmMasterServer != none ]; then
		addServer $gtmMasterServer
	fi
	# GTM Slave
	if [ $gtmSlaveServer != none ]; then
		addServer $gtmSlaveServer
	fi
	# GTM Proxy
	for ((i=0; i<${#gtmProxyServers[@]};i++)); do
		if [ ${gtmProxyServers[$i]} != 'none' -a ${gtmProxyServers[$i]} != "" ]; then
			addServer ${gtmProxyServers[$i]}
		fi
	done
	# Coordinator Master
	for ((i=0; i<${#coordMasterServers[@]}; i++)); do
		if [ ${coordMasterServers[$i]} != none ]; then
			addServer ${coordMasterServers[$i]}
		fi
	done
	# Coordinator Slave
	for ((i=0; i<${#coordSlaveServers[@]}; i++)); do
		if [ ${coordSlaveServers[$i]} != none ]; then
			addServer ${coordSlaveServers[$i]}
		fi
	done
	# Datanode Master
	for ((i=0; i<${#datanodeMasterServers[@]}; i++)); do
		if [ ${datanodeMasterServers[$i]} != none ]; then
			addServer ${datanodeMasterServers[$i]}
		fi
	done
	# Datanode Slave
	for ((i=0; i<${#datanodeSlaveServers[@]}; i++)); do
		if [ ${datanodeSlaveServers[$i]} != none ] ; then
			addServer ${datanodeSlaveServers[$i]}
		fi
	done
	decho '(' ${allServers[@]} ')'
}

#### Handle Slave Configurations ###################################

# Set GTM Proxy info unconfigured.
function gtm_proxy_set_to_no
{
	local i

	gtmProxy=n
	gtmProxyNames=()
	gtmProxyServers=()
	gtmProxyPorts=()
	gtmProxyDirs=()
	gtmPxySpecificExtraConfig=()
	gtmPxyExtraConfig=""
	for ((i=0; i< ${#allServers[@]}; i++)); do
		gtmProxyNames[$i]="none"
		gtmProxyServers[$i]="none"
		gtmProxyPorts[$i]=0
		gtmProxyDirs[$i]="none"
		gtmProxySpecificExtraConfig[$i]=""
	done
}

# Set Coordinator Slave info unconfigured
function coord_slave_set_to_no
{
	local i

	coordSlave=n
	coordSlaveServers=()
	coordSlaveDirs=()
	coordArchLogDirs=()
	for ((i=0; i<${#coordMasterServers[@]}; i++)); do
		coordSlaveServers[$i]=none
		coordSlaveDirs[$i]=none
		coordArchLogDirs[$i]=none
	done
}

# Set Datanode slave info unconfigured
function datanode_slave_set_to_no
{
	local i

	datanodeSlave=n
	datanodeSlaveServers=()
	datanodeSlaveDirs=()
	datanodeSlaveArchLogDirs=()
	for ((i=0; i<${#datanodeMasterServers[@]}; i++)); do
		datanodeSlaveServers[$i]=none
		datanodeSlaveDirs[$i]=none
		datanodeSlaveArchLogDirs[$i]=none
	done
}

# Handle the case where slaves are not configured. --> Construct empty configuration for them
# We assume that all the server list has been constructed.

function handle_no_slaves
{
	local i
	local isEmpty

	# GTM slave
	if [ $gtmSlave != y ] || [ "$gtmSlaveServer" == none ] || [ "$gtmSlaveServer" == N/A ]; then
		gtmSlave=n
		gtmSlaveServer=none
		gtmSlavePort=0
		gtmSlaveDir=none
	fi

	# GTM Proxy
	if [ $gtmProxy != y ]; then
		gtm_proxy_set_to_no
	else
		isEmpty=y
		for ((i=0; i<${#gtmProxyServers[@]}; i++)); do
			if [ ${gtmProxyServers[$i]} != none ] && [ ${gtmProxyServers[$i]} != N/A ]; then
				isEmpty=n
				break
			fi
		done
		if [ "$isEmpty" == y ]; then
			gtm_proxy_set_to_no
			gtmProxy=n
		fi
	fi

	# Coordinator
	if [ $coordSlave != y ]; then
		coord_slave_set_to_no
	else
		isEmpty=y
		for ((i=0; i<${#coordSlaveServers[@]}; i++)); do
			if [ ${coordSlaveServers[$i]} != none ] && [ ${coordSlaveServers[$i]} != N/A ]; then
				isEmpty=n
				break
			fi
		done
		if [ "$isEmpty" == y ]; then
			coord_slave_set_to_no
			coordSlave=n
		fi
	fi

	# Datanode
	if [ $datanodeSlave != y ]; then
		datanode_slave_set_to_no
	else
		isEmpty=y
		for ((i=0; i<${#datanodeSlaveServers[@]}; i++)); do
			if [ ${datanodeSlaveServers[$i]} != none ] && [ ${coordSlaveServers[$I]} != N/A ]; then
				isEmpty=n
				break
			fi
		done
		if [ "$isEmpty" == y ]; then
			datanode_slave_set_to_no
			datanodeSlave=n
		fi
	fi
}



# Check if there're no duplicates in port and working directory assigment
function verifyResource
{
	local i
	local j

	# Number of array entries
	# GTM proxies
	if [ "$gtmProxy" == y ]; then
		i=${#gtmProxyNames[@]}
		if [ $i -ne ${#gtmProxyServers[@]} -o $i -ne ${#gtmProxyPorts[@]} -o $i -ne ${#gtmProxyDirs[@]} -o $i -ne ${#gtmPxySpecificExtraConfig[@]} ]; then
			echo ERROR: Invalid entry numbers in gtm proxy configuration.
			return 1
		fi
	fi
	# Coordinators
	i=${#coordNames[@]}
	if [ $i -ne ${#coordPorts[@]} -o $i -ne ${#poolerPorts[@]} -o $i -ne ${#coordSpecificExraConfig[@]} -o $i -ne ${#coordSpecificExtraPgHba[@]} ]; then
		echo ERROR: Invalid entry numbers in coordinator configuration.
		return 1
	fi
	if [ $i -ne ${#coordMasterServers[@]} -o $i -ne ${#coordMasterDirs[@]} ]; then
		echo ERROR: Invalid entry numbers in coordinator configuration.
		return 1
	fi
	if [ "$coordSlave" == y ]; then
		if [ $i -ne ${#coordSlaveServers[@]} -o $i -ne ${#coordSlaveDirs[@]} -o $i -ne ${#coordArchLogDirs[@]} -o $i -ne ${#coordMaxWALSenders[@]} ]; then
			echo ERROR: Invalid entry numbers in coordinator configuration.
			return 1
		fi
	fi
	# Datanodes
	i=${#datanodeNames[@]}
	if [ $i -ne ${#datanodePorts[@]} -o $i -ne ${#datanodeSpecificExtraConfig[@]} -o $i -ne ${#datanodeSpecificExtraPgHba[@]} ]; then
		echo ERROR: Invalid entry numbers in datanode configuration.
		return 1
	fi
	if [ $i -ne ${#datanodeMasterServers[@]} -o $i -ne ${#datanodeMasterDirs[@]} ]; then
		echo ERROR: Invalid entry numbers in datanode configuration.
		return 1
	fi
	if [ "$datanodeSlave" == y ]; then
		if [ $i -ne ${#datanodeSlaveServers[@]} -o $i -ne ${#datanodeSlaveDirs[@]} -o $i -ne ${#datanodeArchLogDirs[@]} -o $i -ne ${#datanodeMaxWalSenders[@]} ]; then
			echo ERROR: Invalid entry numbers in datanode configuration.
			return 1
		fi
	fi

	# Check if node names don't duplicate
	# GTM
	for ((i=0; i<${#gtmProxyNames[@]};i++)); do
		if [ $gtmName == ${gtmProxyNames[$i]} ]; then
			echo ERROR: GTM name duplicates one of the GTM Proxies, $gtmName
			return 1
		fi
	done
	for ((i=0; i<${#coordNames[@]}; i++)); do
		if [ $gtmName == ${coordNames[$i]} ]; then
			echo ERROR: GTM name duplicates one of the coordinators, $gtmName
			return 1
		fi
	done
	for ((i=0; i<${#datanodeNames[@]}; i++)); do
		if [ $gtmName == ${datanodeNames[$i]} ]; then
			echo ERROR: GTM name duplicates one of the datanodes, $gtmName
			return 1
		fi
	done
	# GTM Proxy
	for ((i=0; i<${#gtmProxyNames[@]}; i++)); do
		for ((j=$i+1;j<${#gtmProxyNames[@]}; j++)); do
			if [ ${gtmProxyNames[$i]} == ${gtmProxyNames[$j]} ]; then
				echo ERROR: GTM proxy name duplicates one of the other GTM proxies, ${gtmProxyNames[$i]}
				return 1
			fi
		done
		for ((j=0;j<${#coordNames[@]};j++));do
			if [ ${coordNames[$j]} == ${gtmProxyNames[$i]} ]; then
				echo ERROR: GTM proxy name duplicates one of the coordinator names, ${gtmProxyNames[$i]}
				return 1
			fi
		done
		for ((j=0;j<${#datanodeNames[@]};j++));do
			if [ ${datanodeNames[$j]} == ${gtmProxyNames[$i]} ]; then
				echo ERROR: GTM proxy name duplicates one of the datanode names, ${gtmProxyNames[$i]}
				return 1
			fi
		done
	done
	# Cordinator
	for ((i=0; i<${#coordNames[@]}; i++)); do
		for ((j=$i+1; j<${#coordNames[@]}; j++)); do
			if [ ${coordNames[$i]} == ${coordNames[$j]} ]; then
				echo ERROR: Coordinator name duplicates on of the other coordinators, ${coordNames[$i]}
				return 1
			fi
		done
		for ((j=0; j<${#datanodeNames[@]}; j++)); do
			if [ ${coordNames[$i]} == ${datanodeNames[$j]} ]
				then
				echo ERROR: Coordinator name duplicates one of the datanodes, ${coordNames[$i]}
				return 1
			fi
		done
	done
	# Datanode
	for ((i=0; i<${#datanodeNames[@]}; i++)); do
		for ((j=$i+1; j<${#datanodeNames[@]}; j++)); do
			if [ ${datanodeNames[$i]} == ${datanodeNames[$j]} ]; then
				echo ERROR: Datanode name duplicates one of the other datanodes, ${datanodeNames[$i]}
				return 1
			fi
		done
	done
	# Check if primary datanode is in datanode name list, or none
	if [ "$primaryDatanode" == "none" ] || [ "$primaryDatanode" == "N/A" ]; then
		return 0
	fi
	# Primary Datanode: is it specified?
	local xx
	xx=n
	for ((i=0;i<${#datanodeNames[@]};i++));do
		if [ "$primaryDatanode" == ${datanodeNames[$i]} ]; then
			xx=y;
			break;
		fi
	done
	if [ $xx == n ]; then
		echo ERROR: Primary datanode is not in the datanode list, $primaryDatanode
		return 1
	fi
}

function verifyNode
{
	local i
	local j
	local Ports
	local Dirs

	Ports=()
	Dirs=()

	decho $1
	# Check if there's no duplicate in port/directory assignment
	if [ $1 == $gtmMasterServer ]; then
		Ports=( ${Ports[@]} $gtmMasterPort )
		Dirs=( ${Dirs[@]} $gtmMasterDir )
	fi
	if [ $1 == $gtmSlaveServer ]; then
		Ports=( ${Ports[@]} $gtmSlavePort )
		Dirs=( ${Dirs[@]} $gtmSlaveDir )
	fi
	for ((i=0; i<${#gtmProxyServers[@]}; i++)); do
		if [ $1 == ${gtmProxyServers[$i]} ]; then
			Ports=( ${Ports[@]} ${gtmProxyPorts[$i]} )
			Dirs=( ${Dirs[@]} ${gtmProxyDirs[$i]} )
		fi
	done
	for ((i=0; i<${#coordMasterServers[@]}; i++)); do
		if [ $1 == ${coordMasterServers[$i]} ]; then
			Ports=( ${Ports[@]} ${coordPorts[$i]} )
			Ports=( ${Ports[@]} ${poolerPorts[$i]} )
			Dirs=( ${Dirs[@]} ${coordMasterDirs[$i]} )
		fi
		if [ $1 == ${coordSlaveServers[$i]} ]; then
			Ports=( ${Ports[@]} ${coordPorts[$i]} )
			Ports=( ${Ports[@]} ${poolerPorts[$i]} )
			Dirs=( ${Dirs[@]} ${coordSlaveDirs[$i]} )
		fi
	done
	for ((i=0; i<${#datanodeMasterServers[$i]}; i++)); do
		if [ $1 == ${datanodeMasterServers[$i]} ]; then
			Ports=( ${Ports[@]} ${datanodePorts[$i]} )
			Dirs=( ${Dirs[@]} ${datanodeMasterDirs[$i]} )
		fi
		if [ $1 == ${datanodeSlaveServers[$i]} ]; then
			Ports=( ${Ports[@]} ${datanodePorts[$i]} )
			Ports=( ${Ports[@]} ${datanodeSlaveDirs[$i]} )
		fi
	done
	for ((i=0; i<${#Ports[@]}; i++)); do
		for ((j=$i+1; j<${#Ports[@]}; j++)); do
			if [ ${Ports[$i]} -eq ${Ports[$j]} -a ${Ports[$i]} != none ]; then
				echo ERROR: duplicate port assignment for the server $1
				return 1
			fi
		done
	done
	for ((i=0; i<${#Dirs[@]}; i++)); do
		for ((j=$i+1; j<${#Dirs[@]}; j++)); do
			if [ ${Dirs[$i]} == ${Dirs[$j]} -a ${Dirs[$i]} != none ]; then
				echo ERROR: duplicate work directory assignment for the server $1
				return 1
			fi
		done
	done
	# We should check if GTM proxy is configured when GTM slave is configured.
	# We could do this here but it's better to do it when we configure
	# postgresql.conf of coordinator/datanode master.
}

function print_config
{
	local ii
	local jj

	echo "========= Postgres-XC configuration ========================"
	echo "=== Overall ==="
	echo Postgres-XC owner: $pgxcOwner
	echo Postgres-XC user: $pgxcUser
	echo Postgres-XC install directory: $pgxcInstallDir
	echo tmpDir: $tmpDir
	echo localTmpDir: $localTmpDir
	echo "=== Each Server ==="
	for ((ii=0;ii<${#allServers[@]};ii++)); do
		echo "=== ${allServers[$ii]} ==="
	    # GTM Master
		if [ ${allServers[$ii]} == $gtmMasterServer ]; then
			echo "GTM Master: "
			echo "    " Nodename: "'"$gtmName"'", port: $gtmMasterPort, dir: "'"$gtmMasterDir"'"
			echo "    " ExtraConfig: "'"$gtmExtraCofig"'", Specific Extra Config: "'"$gtmMasterSpecificExtraConfig"'"
		fi
	    # GTM Slave
		if [ $gtmSlave == y ]; then
			if [ ${allServers[$ii]} == $gtmSlaveServer ]; then
				echo "GTM Slave: "
				echo "    " Nodename: "'"$gtmName"'", port: $gtmSlavePort, dir: "'"$gtmSlaveDir"'"
				echo "    " ExtraConfig: "'"$gtmExtraConfig"'", Specific Extra Config: "'"$gtmSlaveSpecificExtraConfig"'"
			fi
		fi
	    # GTM Proxy
		if [ $gtmProxy == y ]; then
			for ((jj=0;jj<${#gtmProxyServers[@]};jj++)); do
				if [ ${allServers[$ii]} == ${gtmProxyServers[$jj]} ]; then
					echo "GTM Proxy:"
					echo "    " Nodename: "'"${gtmProxyNames[$jj]}"'", port: ${gtmProxyPorts[$jj]}, dir: "'"${gtmProxyDirs[$jj]}"'"
					echo "    " ExtraConfig: "'"$gtmPxyExtraConfig"'", Specific Extra Config: "'"${gtmPxySpecificExtraConfig[$jj]}"'"
				fi
			done
		fi
	    # Coordinator Master
		for ((jj=0;jj<${#coordMasterServers[@]};jj++)); do
			if [ ${allServers[$ii]} == ${coordMasterServers[$jj]} ]; then
				echo "Coordinator Master:"
				echo "    " Nodename: "'"${coordNames[$jj]}"'", port: ${coordPorts[$jj]}, pooler port: "'"${poolerPorts[$jj]}"'"
				echo "    " MaxWalSenders: ${coordMaxWalsenders[$jj]}, Dir: "'"${coordMasterDirs[$jj]}"'"
				echo "    " ExtraConfig: "'"$coordExtraConfig"'", Specific Extra Config: "'"${coordSpecificExtraConfig[$jj]}"'"
				echo "    " pg_hba entries: '(' ${coordPgHbaEntries[@]} ')'
				echo "    " Extra pg_hba: "'"$coordExraPgHba"'", Specific Extra pg_hba: "'"${coordSpecificExtraPgHba[$jj]}"'"
			fi
		done
	    # Coordinator Slave
		if [ $coordSlave == y ]; then
			for ((jj=0;jj<${#coordSlaveServers[@]};jj++)); do
				if [ ${allServers[$ii]} == ${coordSlaveServers[$jj]} ]; then
					echo "Coordinator Slave:"
					echo "    " Nodename: "'"${coordNames[$jj]}"'", port: ${coordPorts[$jj]}, pooler port: ${poolerPorts[$jj]}
					echo "    " Dir: "'"${coordSlaveDirs[$jj]}"'", Archive Log Dir: "'"${coordArchLogDirs[$jj]}"'"
				fi
			done
		fi
	    # Datanode Master
		for ((jj=0;jj<${#datanodeMasterServers[@]};jj++)); do
			if [ ${allServers[$ii]} == ${datanodeMasterServers[$jj]} ]; then
				echo "Datanode Master"
				echo "    " Nodename: "'"${datanodeNames[$jj]}"'", port: ${datanodePorts[$jj]}
				echo "    " MaxWalSenders: ${datanodeMaxWalSenders[$jj]}, Dir: "'"${datanodeMasterDirs[$jj]}
				echo "    " ExtraConfig: "'"datanodeExtraConfig"'", Specific Extra Config: \
					"'"${datanodeSpecificExtraConfig[$jj]}"'"
				echo "    " pg_hba entries: '(' ${datanodePgHbaEntries[@]} ')'
				echo "    " Extra pg_hba: "'"$datanodeExtraPgHba"'", Specific Extra pg_hba: \
					"'"${datanodeSpecificExtraPgHba[$jj]}"'"
			fi
		done
	    # Datanode Slave
		if [ $datanodeSlave == y ]; then
			for ((jj=0;jj<${#datanodeMasterServers[@]};jj++)); do
				if [ ${allServers[$ii]} == ${datanodeSlaveServers[$jj]} ]; then
					echo "Datanode Slave"
					echo "    " Nodename: "'"${datanodeNames[$jj]}"'", port: ${datanodePorts[$jj]}
					echo "    " MaxWalSenders: ${datanodeMaxWalSenders[$jj]}, Dir: "'"${datanodeSlaveDirs[$jj]}
					echo "    " ExtraConfig: "'"datanodeExtraConfig"'", Specific Extra Config: \
						"'"${datanodeSpecificExtraConfig[$jj]}"'"
					echo "    " pg_hba entries: '(' ${datanodePgHbaEntries[@]} ')'
					echo "    " Extra pg_hba: "'"$datanodeExtraPgHba"'", Specific Extra pg_hba: \
						"'"${datanodeSpecificExtraPgHba[$jj]}"'"
				fi
			done
		fi
	done
	echo "=== End of configuration ==="
}

#==============================================================
# Check each component status (running or not)
# You must install/deploy pgxc_monitor
#==============================================================
function monitor_components
{
	local i
	local j

	# GTM master
	pgxc_monitor -Z gtm -p $gtmMasterPort -h $gtmMasterServer
	if [ $? -eq 0 ]; then
		echo -n GTM master "("$gtmName")": running. "   "
	else
		echo -n GTM master "("$gtmName")": not running. "   "
	fi
	echo host: $gtmMasterServer, port: $gtmMasterPort, dir: $gtmMasterDir

	# GTM slave
	if [ $gtmSlave == y ]; then
		if [ $gtmSlaveServer == none ] || [ $gtmSlaveServer == N/A ]; then
			echo -n GTM slave "("$gtmName")": not configured.
		else
			pgxc_monitor -Z gtm -p $gtmSlavePort -h $gtmSlaveServer
			if [ $? -eq 0 ]; then
				echo -n GTM slave "("$gtmName")": running. "   "
			else
				echo -n GTM slave "("$gtmName")": not running. "   "
			fi
			echo host: $gtmSlaveServer, port: $gtmSlavePort, dir: $gtmSlaveDir
		fi
	fi

	# GTM proxies
	if [ $gtmProxy == y ]; then
		for ((i=0; i<${#gtmProxyNames[@]}; i++)); do
			if [ ${gtmProxyServers[$i]} != none ] && [ ${gtmProxyServers[$i]} != N/A ]; then
				pgxc_monitor -Z gtm -p ${gtmProxyPorts[$i]} -h ${gtmProxyServers[$i]}
				if [ $? -eq 0 ]; then
					echo -n GTM proxy "("${gtmProxyNames[$i]}")": running. "   "
				else
					echo -n GTM proxy "("${gtmProxyNames[$i]}")": not running. "   "
				fi
				echo host: ${gtmProxyServers[$i]}, port: ${gtmProxyPorts[$i]}, dir: ${gtmProxyDirs[$i]}
			fi
		done
	fi

	# Coordinator masters
	local postmaster_pid
	for ((i=0; i<${#coordNames[@]};i++));do
		if [ ${coordMasterServers[$i]} != none ] && [ ${coordMasterServers[$i]} != N/A ]; then
			pgxc_monitor -Z node -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} -U $pgxcOwner -d postgres
			if [ $? == 0 ]; then
				echo -n Coordinator master "("${coordNames[$i]}")": running. "   "
			else
				echo -n Coordinator master "("${coordNames[$i]}")": not running. "   "
			fi
			echo host: ${coordMasterServers[$i]}, port: ${coordPorts[$i]}, dir: ${coordMasterDirs[$i]}
		fi
	done

	# Coordinator slaves
	if [ $coordSlave == y ]; then
		for ((i=0; i<${#coordNames[@]};i++)); do
			if [ ${coordSlaveServers[$i]} == none ] || [ ${coordSlaveServers[$i]} == N/A ]; then
				if [ ${coordNames[$i]} != none ] && [ $coordNames[$i] != N/A ]; then
					echo Coordinator slave "("${coordNames[$i]}")": not configured
				fi
			else
				pgxc_monitor -Z node -p ${coordPorts[$i]} -h ${coordSlaveServers[$i]} -U $pgxcOwner -d postgres
				if [ $? == 0 ]; then
					echo -n Coordinator slave "("${coordNames[$i]}")": running. "   "
				else
					echo -n Coordinator slave "("${coordNames[$i]}")": not running. "   "
				fi
				echo host: ${coordSlaveServers[$i]}, port: ${coordPorts[$i]}, dir: ${coordSlaveDirs[$i]}
			fi
		done
	fi

	# Datanode masters
	for ((i=0; i<${#datanodeNames[@]}; i++));do
		if [ ${datanodeMasterServers[$i]} != none ] && [ ${datanodeMasterServers[$i]} != N/A ]; then
			pgxc_monitor -Z node -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} -U $pgxcOwner -d postgres
			if [ $? == 0 ]; then
				echo -n Datanode master "("${datanodeNames[$i]}")": running. "   "
			else
				echo -n Datanode master "("${datanodeNames[$i]}")": not running. "   "
			fi
			echo host: ${datanodeMasterServers[$i]}, port: ${datanodePorts[$i]}, dir: ${datanodeMasterDirs[$i]}
		fi
	done

	# Datanode slaves
	if [ $datanodeSlave == y ]; then
		for ((i=0; i<${#datanodeNames[@]}; i++)); do
			if [ ${datanodeSlaveServers[$i]} == none ] || [ ${datanodeSlaveServers[$i]} == N/A ]; then
				if [ ${datanodeNames[$i]} != none ] && [ ${datanodeNames[$i]} != N/A ]; then
					echo Datanode slave "("${datanodeNames[$i]}")": not configured
				fi
			else
				pgxc_monitor -Z node -p ${datanodePorts[$i]} -h ${datanodeSlaveServers[$i]} -U $pgxcOwner -d postgres
				if [ $? == 0 ]; then
					echo -n Datanode slave "("${datanodeNames[$i]}")": running. "   "
				else
					echo -n Datanode slave "("${datanodeNames[$i]}")": not running. "   "
				fi
				echo host: ${datanodeSlaveServers[$i]}, port: ${datanodePorts[$i]}, dir: ${datanodeSlaveDirs[$i]}
			fi
		done
	fi
}


#===============================================================                                             
# Tool function to check -m option to stop coordinator and                                                   
# datanode                                                                                                   
#===============================================================                                             
function check_immediate
{
    case $1 in
		immediate )
            immediate="-m immediate" ;;
        fast )
            immediate="-m fast" ;;
        normal )
            immediate="" ;;
        * )
            echo "ERROR: Please specify immediate, fast or normal"
            exit 1;;
    esac;
}

#==================================================                                                          
#                                                                                                            
# Setup .bashrc file for PATH and LD_LIBRARY_PATH                                                            
#                                                                                                            
#==================================================                                                          

function setup_bashrc
{
    vecho ================================================================
    vecho Setting .bashrc files
    for ((i=0; i< ${#allServers[@]}; i++)); do
		vecho ---- ${allServers[$i]} -------------------------
		doit ssh $pgxcUser@${allServers[$i]} cp .bashrc .bashrc.org
		ssh $pgxcUser@${allServers[$i]} "cat >> .bashrc" <<EOF
# .bachrc addition for Postgres-XC PATH and LD_LIBRARY_PATH
# $datetime
export PATH_ORG=\$PATH
export PATH=$pgxcInstallDir/bin:\$PATH
export LD_LIBRARY_PATH_ORG=\$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$pgxcInstallDir/lib:\$LD_LIBRARY_PATH
export MANPATH_ORG=\$MANPATH
export MANPATH=$pgxcInstallDir/share/man:\$MANPATH
# End of addition
EOF
	  done
}

function setup_bashrc_individual
{
    vecho ================================================================
    vecho Setting .bashrc files for $pgxcOwner at $1
	doit ssh $pgxcUser@$1 cp .bashrc .bashrc.org
    ssh $pgxcUser@$1 "cat >> .bashrc" <<EOF
# .bachrc addition for Postgres-XC PATH and LD_LIBRARY_PATH
# $datetime
export PATH_ORG=\$PATH
export PATH=$pgxcInstallDir/bin:\$PATH
export LD_LIBRARY_PATH_ORG=\$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$pgxcInstallDir/lib:\$LD_LIBRARY_PATH
export MANPATH_ORG=\$MANPATH
export MANPATH=$pgxcInstallDir/share/man:\$MANPATH
# End of addition
EOF
}

#==================================================                                                          
#                                                                                                            
# Deploy binaries and other common things to each server                                                     
#                                                                                                            
# All the build materials will be deployed to each                                                           
# servers.                                                                                                   
#                                                                                                            
#=================================================                                                           

function pgxc_deploy_all
{
    vecho ================================================================
    vecho pgxc_deploy_all: copy built materials to all the target servers.

    doall rm -rf $pgxcInstallDir/bin $pgxcInstallDir/include $pgxcInstallDir/lib $pgxcInstallDir/share
    doall mkdir -p $pgxcInstallDir
    vecho tar czCf $pgxcInstallDir $tmpDir/wk.tgz bin include lib share
    tar czCf $pgxcInstallDir $tmpDir/wk.tgz bin include lib share
    cpall $tmpDir/wk.tgz $pgxcInstallDir/wk.tgz
    doall tar xzCf  $pgxcInstallDir $pgxcInstallDir/wk.tgz
    doall rm $pgxcInstallDir/wk.tgz
    doit rm $tmpDir/wk.tgz
}

# First argument is the target node.                                                                         

function pgxc_deploy_individual
{
    vecho ================================================================
    vecho pgxc_deploy_individual: copy built materials to the server $1

    doit ssh $pgxcUser@$1 rm -rf $pgxcInstallDir/bin $pgxcInstallDir/include $pgxcInstallDir/lib $pgxcInstallDir/share
    doit ssh $pgxcUser@$1 mkdir -p $pgxcInstallDir
    doit tar czCf $pgxcInstallDir $tmpDir/wk.tgz bin include lib share
    doit scp $tmpDir/wk.tgz $pgxcUser@$1:$pgxcInstallDir/wk.tgz
    doit ssh $pgxcUser@$1 tar xzCf $pgxcInstallDir $pgxcInstallDir/wk.tgz
    doit ssh $pgxcUser@$1 rm $pgxcInstallDir/wk.tgz
    doit rm $tmpDir/wk.tgz
}

#==================================================
#
# Cleanup work directories
#
#==================================================

# First argument is the server name. second argument is the directory name
# Server name could be none, where the target does not exist.
function pgxc_clean_dir
{
	if [ $# -ne 2 ]; then
		return 2
	fi
	if [ $1 == none ]; then
		return 0
	fi
	doit ssh $pgxcUser@$1 rm -rf $2
	doit ssh $pgxcUser@$1 mkdir -p $2
	doit ssh $pgxcUser@$1 chmod 0700 $2
}

# First argument is the nodename.  The second is "master", "slave" or "all".
function pgxc_clean_node
{
	local i

	if [ $1 == $gtmName ]; then
		shift;
		case $1 in
			master )
				pgxc_clean_dir $gtmMasterServer $gtmMasterDir
				return;;
			slave )
				pgxc_clean_dir $gtmSlaveServer $gtmSlaveDir
				return;;
			all )
				pgxc_clean_dir $gtmMasterServer $gtmMasterDir
				pgxc_clean_dir $gtmSlaveServer $gtmSlaveDir
				return;;
			* )
				echo ERROR: invalid argument for pgxc_clean_node, $1
				return 1;;
		esac
	fi
	for ((i= 0; i< ${#gtmProxyNames[@]}; i++)); do
		if [ $1 == ${gtmProxyNames[$i]} ]; then
			pgxc_clean_dir ${gtmProxyServers[$i]} ${gtmProxyDirs[$i]}
			return;
		fi
	done
	for ((i= 0; i< ${#coordNames[@]}; i++)); do
		if [ $1 == ${coordNames[$i]} ]; then
			shift;
			case $1 in
				master )
					pgxc_clean_dir ${coordMasterServers[$i]} ${coordMasterDirs[$i]}
					return;;
				slave )
					pgxc_clean_dir ${coordSlaveServers[$i]} ${coordSlaveDirs[$i]}
					return;;
				all )
					pgxc_clean_dir ${coordMasterServers[$i]} ${coordMasterDirs[$i]}
					pgxc_clean_dir ${coordSlaveServers[$i]} ${coordSlaveDirs[$i]}
					return;;
				* )
					echo ERROR: invalid argument for pgxc_clean_node, $1
					return 1;;
			esac
		fi
	done
	for ((i= 0; i< ${#datanodeNames[@]}; i++)); do
		if [ $1 == ${datanodeNames[$i]} ]; then
			shift;
			case $1 in
				master )
					pgxc_clean_dir ${datanodeMasterServers[$i]} ${datanodeMasterDirs[$i]}
					return;;
				slave )
					pgxc_clean_dir ${datanodeSlaveServers[$i]} ${datanodeSlaveDirs[$i]}
					return;;
				all )
					pgxc_clean_dir ${datanodeMasterServers[$i]} ${datanodeMasterDirs[$i]}
					pgxc_clean_dir ${datanodeSlaveServers[$i]} ${datanodeSlaveDirs[$i]}
					return;;
				* )
					echo ERROR: invalid argument for pgxc_clean_node, $1
					return 1;;
			esac
		fi
	done
	echo ERROR: no target nodename found, $1
}

# First argument is "master", "slave" or "all"
function pgxc_clean_gtm
{
	if [ $# -ne 1 ];then
		echo Specify master, slave or all
		return 1
	fi
	case $1 in
		master );;
		slave );;
		all );;
		* )
			vecho Specify master, slave or all
			return 1;;
	esac
	if [ $1 == master ] || [ $1 == all ]; then
		pxc_clean_dir $gtmMasterServer $gtmMasterDir
	fi
	if [ $gtm_slave != y ]; then
		if [ $1 == slave ] || [ $1 == all ]; then
			pgxc_clean_dir $gtmSlaveServer $gtmSlaveDir
		fi
	fi
}

# First argument is gtm_proxy name
function pgxc_clean_gtm_proxy
{
	if [ $gtmProxy != y ]; then
		echo gtm_proxy is not configured
		return 1
	fi
	if [ $# -ne 1 ]; then
		echo Specify gtm_proxy name
		return 2
	fi
	local i
	for ((i=0; i<${#gtmProxyNames[@]}; i++));do
		if [ $1 == ${gtmProxyNames[$i]} ]; then
			pgxc_clean_dir ${gtmProxyServers[$i]} ${gtmProxyDirs[$i]}
			return
		fi
	done
	echo specified gtm_proxy is not configured, $1
	return 2
}

# No argument
function pgxc_clean_gtm_proxy_all
{
	if [ $gtmProxy != y ]; then
		echo gtm_proxy is not configured
		return 1
	fi
	local i
	for ((i=0; i<${#gtmProxyNames[@]}; i++));do
		pgxc_clean_gtm_proxy ${gtmProxyNames[$i]}
	done
}

# First argument is coordinator name
function pgxc_clean_coordinator_master
{
	if [ $# -ne 1 ]; then
		echo specify coordinator name
		return 2
	fi
	local i
	for ((i=0; i<${#coordNames[@]}; i++));do
		if [ $1 == ${coordNames[$i]} ]; then
			pgxc_clean_dir ${coordMasterServers[$i]} ${coordMasterDirs[$i]}
			return
		fi
	done
	echo specified coordinator is not configured, $1
	return 2
}

function pgxc_clean_coordinator_master_all
{
	local i
	for ((i=0;i<${#coordNames[@]};i++));do
		pgxc_clean_coordinator_master ${coordNames[$i]}
	done
}

# First argument is a coordinator name
function pgxc_clean_coordinator_slave
{
	if [ $coordSlave != y ]; then
		echo Coordinator slave is not configured.
		return 1
	fi
	if [ $# -ne 1 ]; then
		echo Specify coordinator name.
		return 2
	fi
	local i
	for ((i=0;i<${#coordNames[@]};i++));do
		if [ $1 == ${coordNames[$i]} ]; then
			if [ ${coordSlaveServers[$i]} == none ] || [ ${coordSlaveServers[$i]} == N/A ]; then
				echo Specified coordinator slave is not configured, $1
				return 2
			fi
			pgxc_clean_dir ${coordSlaveServers[$i]} ${coordSlaveDirs[$i]}
			return
		fi
	done
	echo Specified coordinator is not configured, $1
	return 2
}

function pgxc_clean_coordinator_slave_all
{
	if [ $coordSlave != y ]; then
		echo Coordinator slave is not configured.
		return 1
	fi
	local i
	for ((i=0; i<${#coordNames[@]}; i++));do
		pgxc_clean_coordinator_slave ${coordNames[$i]}
	done
}

function pgxc_clean_coordinator_all
{
	pgxc_clean_coordinator_master_all
	pgxc_clean_coordinator_slave_all
}

function pgxc_clean_datanode_master
{
	if [ $# -ne 1 ]; then
		echo Specify datanode name
		return 2
	fi
	local i
	for ((i=0; i<${datanodeNames[@]}; i++));do
		if [ $1 == ${datanodeNames[$i]} ]; then
			pgxc_clean_dir ${datanodeMasterServers[$i]} ${datanodeMasterDirs[$i]}
			return
		fi
	done
	echo Specified datanode is not configured, $1
	return 2
}

function pgxc_clena_datanode_master_all
{
	local i
	for ((i=0; i<${#datanodeNames[@]}; i++));do
		pgxc_clean_datanode_master ${datanodeNames[$i]}
	done
}

function pgxc_clean_datanode_slave
{
	if [ $datanodeSlave != y ]; then
		echo Datanode slave is not configured.
		return 1
	fi
	if [ $# -ne 1 ]; then
		echo Specify datanode name.
		return 2
	fi
	local i
	for ((i=0;i<${#datanodeNames[@]}; i++));do
		if [ $1 == ${datanodeNames[$i]} ]; then
			if [ ${datanodeSlaveServers[$i]} == none ] || [ ${datanodeSlaveServers[$i]} == N/A ]; then
				echo Specified datanode slave is not configured, $1
				return 2
			fi
			pgxc_clean_dir ${datanodeSlaveServers[$i]} ${datanodeSlaveDirs[$i]}
			return
		fi
	done
	echo Specified datanode is not configured, $1
	return 2
}

function pgxc_clean_datanode_slave_all
{
	if [ $datanodeSlave != y ]; then
		echo Datanode slave is not configured.
		return 1
	fi
	local i
	for ((i=0; i<${#datanodeNames[@]}; i++)); do
		pgxc_clean_datanode_slave ${datanodeNames[$i]}
	done
}

function pgxc_clean_datanode_all
{
	pgxc_clean_datanode_master_all
	pgxc_clean_datanode_slave_all
}

function pgxc_clean_node_all
{
	local i

	pgxc_clean_node $gtmName all
	for ((i=0; i< ${#gtmProxyNames[@]}; i++)); do
		pgxc_clean_node ${gtmProxyNames[$i]}
	done
	for ((i=0; i< ${#coordNames[@]}; i++)); do
		pgxc_clean_node ${coordNames[$i]} all
	done
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		pgxc_clean_node ${coordNames[$i]} all
	done
}

# print_stdout host file prompt
function print_stdout
{
	local host
	local file
	if [ $# -ne 3 ]; then
		return 1
	fi
	host=$1
	shift
	file=$1
	shift
	if [ $verbose == y ]; then
		if [ $interactive != y ]; then
			ssh $pgxcUser@$host cat $file
		else
			echo -n $*
			if [ `readyesno n` == y ]; then
				ssh $pgxcUser@$host cat $file
			fi
		fi
	fi
	ssh $pgxcUser@$host rm -f $file
}

# Print without asking, if $verbose == y
function do_stdout
{
	local host
	local file
	if [ $# -ne 2 ]; then
		return 1
	fi
	host=$1
	shift
	file=$1
	shift
	if [ $verbose == y ]; then
		ssh $pgxcUser@$host cat $file
	fi
	ssh $pgxcUser@$host rm -f $file
}

function print_initgtm_out
{
	print_stdout $1 $2 "Print initgtm output\?"
}

function print_initdb_out
{
	print_stdout $1 $2 "Print initgtm output\?"
}

function print_pg_ctl_out
{
	print_stdout $1 $2 "Print pg_ctl output\?"
}

function print_gtm_ctl_out
{
	print_stdout $1 $2 "Print gtm_ctl output\?"
}

#===========================================================                                                 
#                                                                                                            
# GTM and GTM slave staff
#                                                                                                            
#===========================================================                                                 

# Reconfigure GTM Master.   Result will be reflected to the configuration file too.
# Parameters are server, port and directory.   Please note that the node name
# is fixed.  You should edit configuration file to change GTM node name.
# You may specify two more arguments, gtm additional configuration file common to
# master/slave and gtm additional configuration file specific to gtm master.
# If you don't specify "no" additional ones, you can specify "none".
function pgxc_configure_gtm_master
{
    vecho ================================================================
    vecho Configure GTM Master

	if [ $# -lt 3 -o $# -gt 5 ]; then
		echo ERROR: invalid arguments, $*
		return 1
	fi
	gtmMasterServer=$1
	gtmMasterPort=$2
	gtmMasterDir=$3
	vecho New GTM Master config: Server: "'"$gtmMasterServer"'", Port: $gtmMasterPort, Dir: "'"$gtmMasterDir"'"
	# Update configuration file
	cat >> $configFile <<EOF
#====================================
# Updated due to GTM Master reconfiguration
#           $datetime
gtmMasterServer=$gtmMasterServer
gtmMasterPort=$gtmMasterPort
gtmMasterDir=$gtmMasterDir
EOF
	if [ $# -ge 4 ]; then
		gtmExtraConfig=$4
		vecho -n "                       "ExtraConfig: "'"$gtmExtraConfig"'"
	cat >> $configFile <<EOF
gtmExtraConfig=$gtmExtraConfig
EOF
		if [ $# -eq 5 ]; then
			gtmMasterSpecificExtraConfig=$5
			vecho "," ExtraSpecificConfig: "'"$gtmMasterSpecificExtraConfig"'"
			cat >> $configFile <<EOF
gtmMasterSpecificExtraConfig=$gtmMasterSpecificExtraConfig
EOF
		else
			vecho ""
			cat >> $configFile <<EOF
# --- End of reconfiguration --------
EOF
		fi
	fi
}

function pgxc_init_gtm_master
{
	local yesno

    vecho ================================================================
    vecho GTM Master initialize

    doit ssh $pgxcUser@$gtmMasterServer "killall -u $pgxcOwner -9 gtm > /dev/null 2>&1"
    doit ssh $pgxcUser@$gtmMasterServer rm -rf $gtmMasterDir
    doit ssh $pgxcUser@$gtmMasterServer mkdir -p $gtmMasterDir
    doit ssh $pgxcUser@$gtmMasterServer "initgtm -Z gtm -D $gtmMasterDir > $tmpDir/initgtm.out 2>&1"
	print_initgtm_out $gtmMasterServer $tmpDir/initgtm.out
    vecho Configuring $gtmMasterServer:$gtmMasterDir/gtm.conf
    ssh $pgxcUser@$gtmMasterServer "cat >>  $gtmMasterDir/gtm.conf" <<EOF
#===========================================
# Added at initialization. $datetime
listen_addresses = '*'
EOF
	if [ $gtmExtraConfig != none ]; then
		ssh $pgxcUser@$gtmMasterServer "cat >> $gtmMasterDir/gtm.conf" < $gtmExtraConfig
	fi
	if [ $gtmMasterSpecificExtraConfig != none ]; then
		ssh $pgxcUser@$gtmMasterServer "cat >> $gtmMasterDir/gtm.conf" < $gtmSpecificExtraConfig
	fi
    ssh $pgxcUser@$gtmMasterServer "cat >>  $gtmMasterDir/gtm.conf" <<EOF
port = $gtmMasterPort
nodename = '$gtmName'
startup = ACT
# End of addition
EOF
	# Next two lines a needed to start GTM with minimum GXID as possible. (default is 10000).
	# Current default GXID initial value will cause datanode slave error because there's too
	# many XID's involved.  This is a dirty hack and should be corrected by running initdb with
	# gtm, or gtm can correct what GXID to start with from all the nodes.
	vecho Initializing starting GXID value...
	(ssh $pgxcUser@$gtmMasterServer gtm -x 1000 -D $gtmMasterDir &)
	sleep 1
	ssh $pgxcUser@$gtmMasterServer gtm_ctl stop -Z gtm -D $gtmMasterDir > /dev/null 2>&1
}

# Configure gtm_slave.   The arguments are host name, port and directory.
# If you remove (or don't configure) the slave, you specify host name as
# none.   You don't have to worry about the rest of the parameters.
# You can specify additional parameters, extra file to go to gtm.conf 
# file, only to the slave.   The common file should be configured
# using pgxc_configure_gtm_master function.
#function pgxc_configure_gtm_slave
#{
#}


function pgxc_init_gtm_slave
{
    vecho ================================================================
    vecho GTM Slave initialize

	if [ $gtmSlaveServer == N/A ]; then
		echo ERROR: GTM Slave is not configured.
		return 1
	fi
    doit ssh $pgxcUser@$gtmSlaveServer "killall -u $pgxcOwner -9 gtm > /dev/null 2>&1"
    doit ssh $pgxcUser@$gtmSlaveServer rm -rf $gtmSlaveDir
    doit ssh $pgxcUser@$gtmSlaveServer mkdir -p $gtmSlaveDir
    doit ssh $pgxcUser@$gtmSlaveServer "initgtm -Z gtm -D $gtmSlaveDir > $tmpDir/initgtm.out 2>&1"
	print_initgtm_out $gtmSlaveServer $tmpDir/initgtm.out
    vecho Configuring $gtmSlaveServer:$gtmSlaveDir/gtm.conf
    ssh $pgxcUser@$gtmSlaveServer "cat >>  $gtmSlaveDir/gtm.conf"  <<EOF
listen_addresses = '*'
EOF
	if [ $gtmExtraConfig != none ]; then
		ssh $pgxcUser@$gtmSlaveServer "cat >> $gtmSlaveDir/gtm.conf" < $gtmExtraConfig
	fi
	if [ $gtmSlaveSpecificExtraConfig != none ]; then
		ssh $pgxcUser@$gtmSlaveServer "cat >> $gtmSlaveDir/gtm.conf" < $gtmSlaveSpecificExtraConfig
	fi
    ssh $pgxcUser@$gtmSlaveServer "cat >>  $gtmSlaveDir/gtm.conf" <<EOF
port = $gtmSlavePort
nodename = '$gtmName'
startup = STANDBY
active_host = '$gtmMasterServer'
active_port = $gtmMasterPort
EOF
}

function pgxc_start_gtm_master
{
	vecho ================================================================
	vecho Starting GTM Master

	doit ssh $pgxcUser@$gtmMasterServer "killall -u $pgxcOwner -9 gtm > /dev/null 2>&1"
	doit ssh $pgxcUser@$gtmMasterServer "gtm_ctl start -Z gtm -D $gtmMasterDir > $tmpDir/gtm.out"
	do_stdout $gtmMasterServer $tmpDir/gtm.out
}

function pgxc_start_gtm_slave
{
	vecho ================================================================
	vecho Starting GTM Slave

	if [ $gtmSlaveServer == none ]; then
		echo ERROR: GTM slave is not configured.
		return 1
	fi
	doit ssh $pgxcUser@$gtmMasterServer "gtm_ctl status -Z gtm -D $gtmMasterDir > /dev/null 2> /dev/null"
	if [ $? -ne 0 ]; then
		echo ERROR: GTM Master is not running. Cannot start the slave.
		return 1
	fi
	doit ssh $pgxcUser@$gtmSlaveServer "killall -u $pgxcOwner -9 gtm >/dev/null 2>&1"
	doit ssh $pgxcUser@$gtmSlaveServer "gtm_ctl start -Z gtm -D $gtmSlaveDir > $tmpDir/gtm.out"
	do_stdout $gtmSlaveServer $tmpDir/gtm.out
}


function pgxc_stop_gtm_master
{
	vecho ================================================================
	vecho Stopping GTM Master
	doit ssh $pgxcUser@$gtmMasterServer gtm_ctl stop -Z gtm -D $gtmMasterDir
}

function pgxc_stop_gtm_slave
{
	if [ $gtmSlaveServer == none ] || [ $gtmSlaveServer == N/A ]; then
		echo ERROR: GTM slave is not configured.
		return 1
	fi
	vecho ================================================================
	vecho Stopping GTM Slave
	doit ssh $pgxcUser@$gtmSlaveServer gtm_ctl stop -Z gtm -D $gtmSlaveDir
}

function pgxc_kill_gtm_master
{
	vecho ================================================================
	vecho Stopping GTM Master
	doit ssh $pgxcUser@$gtmMasterServer "killall -u $pgxcUser -9 gtm >/dev/null 2>&1"
}

function pgxc_kill_gtm_slave
{
	if [ $gtmSlaveServer == none ] || [ $gtmSlaveServer == N/A ]; then
		echo ERROR: GTM slave is not configured.
		return 1
	fi
	vecho ================================================================
	vecho Stopping GTM Slave
	doit ssh $pgxcUser@$gtmSlaveServer "killall -u $pgxcUser -9 gtm >/dev/null 2&>1"
}

function pgxc_failover_gtm
{
	# Reconnect should be done in separate action.
	vecho ================================================================
	vecho GTM Failover

	if [ $gtmSlaveServer == none ]; then
		echo ERROR: pgxc_failover_gtm: GTM slave is not available.
		return 1
	fi
	doit ssh "$pgxcUser@$gtmSlaveServer gtm_ctl status -Z gtm -D $gtmSlaveDir > /dev/null 2> /dev/null"
	if [ $? -ne 0 ]; then
		echo ERROR: GTM slave is not running.
		return 1
	fi
	# STONITH GTM Master
	# Please note that master and slave should run on different server.
	doit ssh $pgxcUser@$gtmMasterServer "killall -u $pgxcOwner -9 gtm >/dev/null 2>&1"
	doit ssh $pgxcUser@$gtmSlaveServer gtm_ctl promote -Z gtm -D $gtmSlaveDir
	# Update GTM configuration file as the master
	vecho Reconfigure GTM as Master
	ssh $pgxcUser@$gtmSlaveServer "cat >> $gtmSlaveDir/gtm.conf" <<EOF
#===================================================
# Updated due to GTM failover
#        $datetime
startup = ACT
#----End of reconfiguration -------------------------
EOF
	# Update configuration
	vecho Reconfiguring whole Postgres-XC cluster
	cat >> $configFile <<EOF
#===================================================
# pgxc configuration file updated due to GTM failover
#        $datetime
gtmMasterServer=$gtmSlaveServer
gtmMasterPort=$gtmSlavePort
gtmMasterDir=$gtmSlaveDir
gtmSlaveServer=none
gtmSlavePort=0
gtmSlaveDir=none
#----End of reconfiguration -------------------------
EOF
	# Reconfigure myself
	gtmMasterServer=$gtmSlaveServer
	gtmMasterPort=$gtmSlavePort
	gtmMasterDir=$gtmSlaveDir
	gtmSlaveServer=none
	gtmSlavePort=0
	gtmSlaveDir=none
}

#===============================================================================
#
# GTM Proxy staff
#
#===============================================================================

function pgxc_init_gtm_proxy
{
	# First argument is the nodename
    vecho ================================================================
    vecho Initialize GTM Proxy $1

	local i

	if [ $# -ne 1 ]; then
		iecho Specify gtm_proxy name
		return 1
	fi
	for ((i=0; i< ${#gtmProxyNames[@]}; i++)); do
		if [ $1 == ${gtmProxyNames[$i]} ] && [ ${gtmProxyServers[$i]} != none ] && [ ${gtmProxyServers[$i]} != N/A ]; then
			doit ssh $pgxcUser@${gtmProxyServers[$i]} "killall -u $pgxcOwner -9 gtm_proxy >/dev/null 2>&1"
			doit ssh $pgxcUser@${gtmProxyServers[$i]} rm -rf ${gtmProxyDirs[$i]}
			doit ssh $pgxcUser@${gtmProxyServers[$i]} mkdir -p ${gtmProxyDirs[$i]}
			doit ssh "$pgxcUser@${gtmProxyServers[$i]} initgtm -Z gtm_proxy -D ${gtmProxyDirs[$i]} > $tmpDir/initgtm.out 2>&1"
			print_initgtm_out ${gtmProxyServers[$i]} $localTmpDir/initgtm.out
			vecho Configuring ${gtmProxyServers[$i]}:${gtmProxyDirs[$i]}/gtm_proxy.conf
			if [ $gtmPxyExtraConfig != none ] && [ $gtmPxyExtrConfig != N/A ]; then
				ssh $pgxcUser@${gtmProxyServers[$i]} "cat >> ${gtmProxyDirs[$i]}/gtm_proxy.conf" < $gtmPxyExtraConfig
			fi
			ssh $pgxcUser@${gtmProxyServers[$i]} "cat >> ${gtmProxyDirs[$i]}/gtm_proxy.conf" <<EOF
nodename = '${gtmProxyNames[$i]}'
listen_addresses = '*'
port = '${gtmProxyPorts[$i]}'
gtm_host = $gtmMasterServer
gtm_port = $gtmMasterPort
worker_threads = 1
gtm_connect_retry_interval = 1
EOF
			return
		fi
	done
	echo ERROR: specified GTM proxy is not configured, $1
	return 1
}

function pgxc_init_gtm_proxy_all
{
	local i
	if [ $gtmProxy != y ]; then
		return 1
	fi
	for((i=0;i<${#gtmProxyNames[@]};i++)); do
		if [ ${gtmProxyServers[$i]} != none ] && [ ${gtmProxyServers[$i]} != N/A ]; then
			pgxc_init_gtm_proxy ${gtmProxyNames[$i]}
		fi
	done
}

function pgxc_start_gtm_proxy
{
	# First argument is the nodename
    vecho ================================================================
    vecho Start GTM Proxy $1

	if [ $# -ne 1 ]; then
		iecho Specify GTM proxy name
		return 1
	fi
	local i
	for ((i=0; i< ${#gtmProxyNames[@]}; i++)); do
		if [ $1 == ${gtmProxyNames[$i]} ]; then
			doit ssh $pgxcUser@${gtmProxyServers[$i]} "killall -u $pgxcOwner -9 gtm_proxy >/dev/null 2>&1"
			doit ssh $pgxcUser@${gtmProxyServers[$i]} "gtm_ctl start -Z gtm_proxy -D ${gtmProxyDirs[$i]} > $tmpDir/gtm_proxy.out 2>&1"
			do_stdout ${gtmProxyServers[$i]} $tmpDir/gtm_proxy.out
			return
		fi
	done
	echo ERROR: specified GTM proxy does not exist, $1
	return 1
}

function pgxc_start_gtm_proxy_all
{
	local i
    vecho ================================================================
    vecho Starting all the GTM proxies
	if [ $gtmProxy != y ]; then
		iecho GTM proxy is not configured.
		return 1
	fi
	for((i=0;i<${#gtmProxyNames[@]};i++)); do
		if [ ${gtmProxyServers[$i]} != none ] && [ ${gtmProxyServers[$i]} != N/A ]; then
			pgxc_start_gtm_proxy ${gtmProxyNames[$i]}
		fi
	done
}

function pgxc_stop_gtm_proxy
{
	# First argument is the nodename
    vecho ================================================================
    vecho Stop GTM Proxy $1

	if [ $# -ne 1 ]; then
		iecho Specify GTM Proxy name
		return 1
	fi
	local i
	for ((i=0; i< ${#gtmProxyNames[@]}; i++)); do
		if [ $1 == ${gtmProxyNames[$i]} ]; then
			doit ssh $pgxcUser@${gtmProxyServers[$i]} gtm_ctl stop -Z gtm_proxy -D ${gtmProxyDirs[$i]}
			return
		fi
	done
	echo ERROR: specified GTM proxy does not exist, $1
	return 1
}

function pgxc_stop_gtm_proxy_all
{
    vecho ================================================================
    vecho Stop all the GTM Proxies

	local i
	if [ $gtmProxy != y ]; then
		iecho GTM Proxy is not configured
		return 1
	fi
	for((i=0;i<${#gtmProxyNames[@]};i++)); do
		if [ ${gtmProxyServers[$i]} == none ] || [ ${gtmProxyServers[$i]} == N/A ]; then
			continue
		fi
		pgxc_stop_gtm_proxy ${gtmProxyNames[$i]}
	done
}



function pgxc_kill_gtm_proxy
{
	# First argument is the nodename
    vecho ================================================================
    vecho Kill GTM Proxy $1

	if [ $# -ne 1 ]; then
		iecho Specify GTM proxy name
		return 1
	fi
	local i
	for ((i=0; i< ${#gtmProxyNames[@]}; i++)); do
		if [ $1 == ${gtmProxyNames[$i]} ]; then
			doit ssh $pgxcUser@${gtmProxyServers[$i]} "killall -u $pgxcOwner -9 gtm_proxy >/dev/null 2>&1"
			return
		fi
	done
	echo ERROR: specified GTM proxy does not exist, $1
	return 1
}

function pgxc_kill_gtm_proxy_all
{
    vecho ================================================================
    vecho Killing all the  GTM Proxies

	local i
	if [ $gtmProxy != y ]; then
		iecho GTM Proxy is not configured
		return 1
	fi
	for((i=0;i<${#gtmProxyNames[@]};i++)); do
		if [ ${gtmProxyServers[$i]} == none ] || [ ${gtmProxySevrers[$i]} == N/A ]; then
			continue
		fi
		pgxc_kill_gtm_proxy ${gtmProxyNames[$i]}
	done
}

function pgxc_reconnect_gtm_proxy
{
	# Reconnect to the current GTM master.   When failed over, the current Master must have been updated.
	# Remember to update gtm_proxy configuration file so that it connects to the new master at the next
	# start.
	# Please note that we assume GTM has already been failed over.
	# First argument is gtm_proxy nodename
    vecho ================================================================
    vecho Reconnect GTM Proxy $1

	if [ $# -ne 1 ]; then
		iecho Specify GTM proxy name
		return 1
	fi
	local i
	for ((i=0; i< ${#gtmProxyNames[@]}; i++)); do
		if [ $1 == ${gtmProxyNames[$i]} ]; then
			vecho doit ssh $pgxcUser@${gtmProxyServers[$i]} gtm_ctl reconnect -Z gtm_proxy -D ${gtmProxyDirs[$i]} -o \
				\"-s $gtmMasterServer -t $gtmMasterPort\"

			doit ssh $pgxcUser@${gtmProxyServers[$i]} gtm_ctl reconnect -Z gtm_proxy -D ${gtmProxyDirs[$i]} -o \
				\"-s $gtmMasterServer -t $gtmMasterPort\"
			vecho Reconfiguring GTM Proxy reflect reconnect.
			ssh $pgxcUser@${gtmProxyServers[$i]} "cat >> ${gtmProxyDirs[$i]}/gtm_proxy.conf" <<EOF
#===================================================
# Updated due to GTM Proxy reconnect
#        $datetime
gtm_host = $gtmMasterServer
gtm_port = $gtmMasterServer
#----End of reconfiguration -------------------------
EOF
			return
		fi
	done
	echo ERROR: specified GTM proxy does not exist, $1
	return 1

}

function pgxc_reconnect_gtm_proxy_all
{
    vecho ================================================================
    vecho Reconnect all the GTM proxies

	local i
	if [ $gtmProxy != y ]; then
		iecho GTM Poxy is not configured
		return 1
	fi
	for((i=0;i<${#gtmProxyNames[@]};i++)); do
		if [ ${gtmProxyServers[$i]} == none ] || [ ${gtmProxyServers[$i]} == N/A ]; then
			continue
		fi
		pgxc_reconnect_gtm_proxy ${gtmProxyNames[$i]}
	done
}
#===============================================================================
#
# Coordinator Staff
#
#===============================================================================

function pgxc_init_coordinator_master
{
	# First argument is the nodename
    vecho ================================================================
    vecho Initialize coordinator master $1

	if [ $# -ne 1 ]; then
		iecho Specify coordinator name
		return 1
	fi
	local i
	for ((i=0; i< ${#coordNames[@]}; i++)); do
		if [ $1 == ${coordNames[$i]} ] && [ ${coordMasterServers[$i]} != none ] && [ ${coordMasterServers[$i]} != N/A ]; then
			psql -p ${coordPorts[$i]} -h ${coordMasterServers[$i]}  -c 'select 1' postgres $pgxcOwner > /dev/null 2> /dev/null
			if [ $? -eq 0 ]; then
				echo ERROR: target coordinator master is running now.  Stop it to configure.
				return 1
			fi
			doit ssh $pgxcUser@${coordMasterServers[$i]} rm -rf ${coordMasterDirs[$i]}
			doit ssh $pgxcUser@${coordMasterServers[$i]} mkdir -p ${coordMasterDirs[$i]}
			doit ssh $pgxcUser@${coordMasterServers[$i]} "initdb --nodename ${coordNames[$i]} -D ${coordMasterDirs[$i]} > $tmpDir/initdb.out 2>&1"
			print_initdb_out ${coordMasterServers[$i]} $tmpDir/initdb.out
			vecho Configuring ${coordMasterServers[$i]}:${coordMasterDirs[$i]}/postgresql.conf
		    # Get effective GTM port and host.   If gtm_proxy is not found, then connect to GTM
			local j
			local targetGTMhost
			local targetGTMport
			targetGTMhost=$gtmMasterServer
			gatgetGTMPort=$gtmMasterPort
			for ((j=0; j< ${#gtmProxyServers[@]}; j++)); do
				if [ ${coordMasterServers[$i]} == ${gtmProxyServers[$j]} ]; then
					targetGTMhost=${gtmProxyServers[$j]}
					targetGTMport=${gtmProxyPorts[$j]}
					break
				fi
			done
			if [ $coordExtraConfig != none ] && [ $coordExtraConfig != N/A ]; then
				vecho Configuring $pgxcUser@${coordMasterServer[$i]}:${coaordMasterDirs[$i]}/postgresql.conf using $coordExtraConfig
				ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" < $coordExtraConfig
			fi
			if [ ${coordSpecificExraConfig[$i]} != none ] && [ ${coordSpecificExraConfig[$i]} != none ]; then
				vecho Configuring $pgxcUser@${coordMasterServers[$i]}:${coordMasterDirs[$i]}/postgresql.conf using ${coordSpecificExtraConfig[$i]}
				ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" < ${coordSpecificExtraConfig[$i]}
			fi
			vecho Configuring  $pgxcUser@${coordMasterServers[$i]}:${coordMasterDirs[$i]}/postgresql.conf
			ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" <<EOF
#===========================================
# Added at initialization. $datetime
log_destination = 'stderr'
logging_collector = on
log_directory = 'pg_log'
listen_addresses = '*'
port = ${coordPorts[$i]}
max_connections = 100
pooler_port = ${poolerPorts[$i]}
gtm_host = '$targetGTMhost'
gtm_port = $targetGTMport
EOF
			# Additional initialization for log_shipping.
			if [ $coordSlave == y ] && [ ${coordSlaveServers[$i]} != none ] && [ ${coordSlaveServers[$i]} != N/A ]; then
				# At least cleanup remote archive directory.
				pgxc_clean_dir ${coordSlaveServers[$i]} ${coordArchLogDirs[$i]}
				# Then setup postgresql.conf
				ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" <<EOF
wal_level = hot_standby
archive_mode = on
archive_command = 'rsync %p $pgxcUser@${coordSlaveServers[$i]}:${coordArchLogDirs[$i]}/%f'
max_wal_senders = ${coordMaxWALSenders[$i]}
# End of Addition
EOF
			else
				ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" <<EOF
# End of Addition
EOF
			fi
			vecho Configuring ${coordMasterServers[$i]}:${coordMasterDirs[$i]}/pg_hba.conf
			ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hba.conf"  <<EOF
#=================================================
# Addition at initialization, $datetime 
EOF
			if [ $coordExtraPgHba != none ] && [ $coordExtraPgHba != N/A ]; then
				vecho Configuring ${coordMasterServers[$i]}:${coordMasterDirs[$i]}/pg_hba.conf using $coordExtraPgHba
				ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hab.conf" < $coordExtraPgHba
			fi
			if [ ${coordSpecificExtraPgHba[$i]} != none ] && [ ${coordSpecificExtraPgHba[$i]} != N/A ]; then
				ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hab.conf" < ${coordSpecificExtraPgHba[$i]}
			fi
			local j
			for ((j=0; j< ${#coordPgHbaEntries[@]}; j++)); do
				ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hba.conf" <<EOF
host all $pgxcOwner ${coordPgHbaEntries[$j]} trust
EOF
				if [ ${coordSlaveServers[$i]} != none ] && [ ${coordSlaveServers[$i]} != N/A ]; then
					ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hba.conf"  <<EOF
host replication $pgxcOwner ${coordPgHbaEntries[$j]} trust
EOF
				fi
			done
			ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hba.conf"  <<EOF
# End of addition
EOF
			return
		fi
	done
	echo ERROR: specified coordinator is not configured, $1
	return 1
}

function pgxc_init_coordinator_master_all
{
    vecho ================================================================
    vecho Initialize all the coordinator masters

	local i
	for ((i=0; i< ${#coordNames[@]}; i++)); do
		if [ ${coordMasterServers[$i]} != none ] && [ ${coordMasterServers[$i]} != N/A ]; then
			pgxc_init_coordinator_master ${coordNames[$i]}
		fi
	done
}

function pgxc_start_coordinator_master
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Start coordinator master $1

	if [ $# -ne 1 ]; then
		iecho Specify coordinator name
		return 1
	fi
	local i
	for ((i=0; i< ${#coordNames[@]}; i++));	do
		if [ $1 == ${coordNames[$i]} ]; then
			psql -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
			if [ $? -eq 0 ]; then
				echo ERROR: target coordinator master is running now.
				return 1
			fi
			doit ssh $pgxcUser@${coordMasterServers[$i]} "pg_ctl start -Z coordinator -D ${coordMasterDirs[$i]} -o -i > $tmpDir/coord.out 2>&1"
			do_stdout ${coordMasterServers[$i]} $tmpDir/coord.out
			return
		fi
	done
	echo ERROR: specified coordinator is not configured, $1
	return 1
}

function pgxc_start_coordinator_master_all
{
    vecho ================================================================
    vecho Start all the coordinator masters

	local i
	for ((i=0; i< ${#coordNames[@]}; i++)); do
		if [ ${coordMasterServers[$i]} != none ] && [ ${coordMasterServers[$i]} != N/A ]; then
			pgxc_start_coordinator_master ${coordNames[$i]}
		fi
	done
}

function pgxc_stop_coordinator_master
{
	# First arugument is the coordinator name
    vecho ================================================================
    vecho Stop coordinator master $1

	if [ $# -ne 1 ]; then
		iecho Specify coordinator name
		return 1
	fi
	local i
	for ((i=0; i< ${#coordNames[@]}; i++));	do
		if [ $1 == ${coordNames[$i]} ]; then
			doit ssh $pgxcUser@${coordMasterServers[$i]} pg_ctl stop -Z coordinator -D ${coordMasterDirs[$i]} $immediate
			return
		fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

function pgxc_stop_coordinator_master_all
{
    vecho ================================================================
    vecho Start all the coordinator masters

	local i
	for ((i=0; i< ${#coordNames[@]}; i++)); do
		if [ ${coordMasterServers[$i]} != none ] && [ ${coordMasterServers[$i]} != N/A ]; then
			pgxc_stop_coordinator_master ${coordNames[$i]}
		fi
	done
}


function pgxc_kill_coordinator_master		# NOT TESTED YET
{
	# First arugument is the coordinator name

	# It's safer to kill the target coordinator with killall command.  In this case, we need to
	# capture postmaster's pid for the target
    vecho ================================================================
    vecho Kill coordinator master $1

	if [ $# -ne 1 ]; then
		iecho Specify nodename
		return 1
	fi
	local i
	local postmaster_pid
	for ((i=0; i< ${#coordNames[@]}; i++));	do
		if [ $1 == ${coordNames[$i]} ]; then
			if [ ${coordMasterServers[$i]} != none ] && [ ${coordMasterServers[$i]} != N/A ]; then
				postmaster_pid=`get_postmaster_pid ${coordMasterServers[$i]} ${coordMasterDirs[$i]}`
				if [ $postmaster_pid != none ]; then
					kill_all_child_parent ${coordMasterServers[$i]} $postmaster_pid
				fi
			else
				echo specified coordinator master does not exist, $1
			fi
			return
		fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

function pgxc_kill_coordinator_master_all	# NOT TESTED YET
{
    vecho ================================================================
    vecho Start all the coordinator masters

	local i
	for ((i=0; i< ${#coordNames[@]}; i++)); do
		if [ ${coordMasterServers[$i]} != none ] && [ ${coordMasterServers[$i]} != N/A ]; then
			pgxc_kill_coordinator_master ${coordNames[$i]}
		fi
	done
}


# Caution: This function has not been tested yet!  Should test when datanode is ready.
# If a coordinator is not configured with the slave, we should remove it from the cluster
# when it fails.
function pgxc_remove_coordinator_master		# NOT TESTED YET
{
	local i
	for ((i=0; i<${#coordNames[@]}; i++)); do
		if [[ ${coordNames[$i]} == $1 ]]; then
			local j
			for ((j=0; j< ${#coordNames[@]}; j++)); do
				if [ $i -ne -$j ]; then
					if [ ${coordMasterServers[$j]} != none ] && [ ${coordMasterServers[$i]} != N/A ]; then
						psql -p ${coordPorts[$j]} -h ${coordMasterServers[$j]} postgres $pgxcOwner -c "DROP NODE ${coordNames[$j]}"
					fi
				else
					doit ssh $pgxcUser@${coordMasterServers[$j]} pg_ctl stop -Z coordinator -D ${coordMaseterDirs[$j]} -m immediate
				fi
			done
			${coordMasterServers[$i]}=none
			${coordMasterDirs[$i]}=none
			cat >> $configFile <<EOF
#=========================================================
# Update due to coordinator master removal, $1, $datetime
coordMasterServers=(${coordMasterServers[@]})
coordMasterDirs=(${coordMasterDirs[@])
# End of update
EOF
		fi
	done
}

# To construct coordinator slave, pg_basebackup utility is used, which needs master coordinator running.
# If the master coordinator is not running, then we temporary run it.   After copying the base backup,
# the the master will be stopped.  Please be sure that coordinator master is initialized properly.
# If it is running, then it will be restarted to reflect the change to postgresql.conf.
function pgxc_init_coordinator_slave
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	if [ $# -ne 1 ]; then
		iecho Specify coordinator ndoe name
		return 1
	fi
	local i
	local start_master=n
	restart=n
	for ((i=0; i< ${#coordNames[@]}; i++));	do
		if [ $1 == ${coordNames[$i]} ]; then
			decho $i
			if [ ${coordSlaveServers[$i]} == N/A ] || [ ${coordSlaveServers[$i]} == none ]; then
				echo ERROR: slave for the coordinator $1 is not configured.
				return 1
			fi
		    # Coordinator master should be running
			psql -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
			if [ $? -ne 0 ]; then
				start_master=y
			fi
  		    # Clean slave's directory
			doit ssh $pgxcUser@${coordSlaveServers[$i]} rm -rf ${coordSlaveDirs[$i]}
			doit ssh $pgxcUser@${coordSlaveServers[$i]} mkdir -p ${coordSlaveDirs[$i]}
			doit ssh $pgxcUser@${coordSlaveServers[$i]} chmod 0700 ${coordSlaveDirs[$i]}
		    # if the master is not running, we just start it and then stop it.
			if [ $start_master == y ]; then
				iecho Starting the coordinator master to obtain base backup
				doit ssh $pgxcUser@${coordMasterServers[$i]} "pg_ctl start -Z coordinator -D ${coordMasterDirs[$i]} -o -i > $tmpDir/cmd.out"
				do_stdout ${coordMasterServers[$i]} $tmpDir/cmd.out
				sleep 2
			fi
		    # Obtain base backup of the master
			doit ssh $pgxcUser@${coordSlaveServers[$i]} pg_basebackup -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} -D ${coordSlaveDirs[$i]} -x

		    # Configure recovery.conf of the slave
			vecho -- Configure slave\'s recovery.conf, ${coordSlaveServers[$i]}:${coordSlaveDirs[$i]}/recovery.conf
			ssh $pgxcUser@${coordSlaveServers[$i]} "cat >> ${coordSlaveDirs[$i]}/recovery.conf" <<EOF
#==========================================
# Added to initialize the slave, $datetime
standby_mode = on
primary_conninfo = 'host = ${coordMasterServers[$i]} port = ${coordPorts[$i]} user = $pgxcOwner application_name = ${coordNames[$i]}'
restore_command = 'cp ${coordArchLogDirs[$i]}/%f %p'
archive_cleanup_command = 'pg_archivecleanup ${coordArchLogDirs[$i]} %r'
EOF
		    # Configure slave's postgresql.conf
			vecho -- Configure slave\'s postgresql.conf, ${coordSlaveServers[$i]}:${coordSlaveDirs[$i]}/postgresql.conf
			ssh $pgxcUser@${coordSlaveServers[$i]} "cat >> ${coordSlaveDirs[$i]}/postgresql.conf" <<EOF
#==========================================
# Added to initialize the slave, $dtetime
hot_standby = on
port = ${coordPorts[$i]}
EOF
		    # Stop the Master if it was not runnig
			if [ $start_master = y ]; then
				doit ssh $pgxcUser@${coordMasterServers[$i]} pg_ctl stop -Z coordinator -D ${coordMasterDirs[$i]} -m fast
			fi
			return
		fi
	done
	echo ERROR: specified coordinator is not configured, $1
	return 1
}

function pgxc_init_coordinator_slave_all
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize all the coordinator slaves

	local i
	if [ $coordSlave != y ]; then
		echo Coordinator slaves are not configured.
		return 1
	fi
	for ((i=0;i<${#coordNames[@]};i++)); do
		if [ ${coordNames[$i]} != none ] && [ ${coordNames[$i]} != N/A ]; then
			pgxc_init_coordinator_slave ${coordNames[$i]}
		fi
	done
	return
}

function pgxc_start_coordinator_slave
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	if [ $coordSlave != y ]; then
		echo Coordinator slaves are not configured.
		return 1
	fi
	local i
	for ((i=0; i< ${#coordNames[@]}; i++));	do
		if [ $1 == ${coordNames[$i]} ]; then
			if [ ${coordSlaveServers[$i]} == none ] || [ ${coordSlaveServers[$i]} == N/A ]; then
				echo ERROR: slave for coordinator $1 is not configured.
				return 1
			fi
		  	# Coordinator master should be running
			psql -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
			if [ $? -ne 0 ]; then
				echo ERROR: corresponding coordinator master is not running now, $1
				return 1
			fi
		  	# Start the slave
			doit ssh $pgxcUser@${coordSlaveServers[$i]} "pg_ctl start -Z coordinator -D ${coordSlaveDirs[$i]} -o -i > $tmpDir/coord.out"
			do_stdout ${coordSlaveServers[$i]} $tmpDir/coord.out
		  	# Change the master to synchronous mode
			vecho Change the master to synchrnous mode, $1
			ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" <<EOF
#==========================================================
# Added to start the slave in sync. mode, $datetime
synchronous_commit = on
synchronous_standby_names = '${coordNames[$i]}'
# End of the addition
EOF
			ssh $pgxcUser@${coordMasterServers[$i]} pg_ctl reload -Z coordinator -D ${coordMasterDirs[$i]}
			return
		fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}


function pgxc_start_coordinator_slave_all
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Start all the coordinator slaves

	if [ $coordSlave != y ]; then
		iecho Coordinator slaves are not configured.
		return 1
	fi
	local i
	for ((i=0; i<${#coordNames[@]}; i++)); do
		if [ ${coordNames[$i]} != none ] && [ ${coordNames[$i]} != N/A ]; then
			pgxc_start_coordinator_slave ${coordNames[$i]}
		fi
	done
}

function pgxc_stop_coordinator_slave
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Stop coordinator slave $1

	if [ $coordSlave != y ]; then
		iecho Coordinator slaves are not configured.
		return 1
	fi
	if [ $# -ne 1 ]; then
		iecho Specify coordinator node name
		return 1
	fi

	local i
	for ((i=0; i< ${#coordNames[@]}; i++));	do
		if [ $1 == ${coordNames[$i]} ]; then
			if [ ${coordSlaveServers[$i]} == none ] || [ ${coordSlaveServers[$i]} == N/A ]; then
				echo ERROR: slave for the coordinator $1 is not configured.
				return 1
			fi
			# If the master is running, master's switch replication to asynchronous mode.
			psql -p ${coordPorts[$i]} -h ${coordMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
			if [ $? -eq 0 ]; then
			    # Switch Master to asynchronous mode.
				vecho Switching master of $1 at ${coordMasterServer[$i]} to asynchronous replication mode.
				ssh $pgxcUser@${coordMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/postgresql.conf" <<EOF
#=======================================
# Updated to trun off the slave $datetime
synchronous_standby_names = ''
# End of the update
EOF
				doit ssh $pgxcUser@${coordMasterServers[$i]} pg_ctl reload -Z coordinator -D ${coordMasterDirs[$i]}
			fi
			doit ssh $pgxcUser@${coordSlaveServers[$i]} pg_ctl stop -Z coordinator -D ${coordSlaveDirs[$i]} $immediate
			return;
		fi
	done
	echo ERROR: Specified coordinator was not configured, $1
	return 1
}

function pgxc_stop_coordinator_slave_all
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Stop all the coordinator slaves

	if [ $coordSlave != y ]; then
		iecho Coordinator slaves are not configured.
		return 1
	fi
	local i
	for ((i=0; i<${#coordNames[@]}; i++)); do
		if [ ${coordNames[$i]} != none ] && [ ${coordNames[$i]} != N/A ]; then
			pgxc_stop_coordinator_slave ${coordNames[$i]}
		fi
	done
}

function pgxc_kill_coordinator_slave
{
	# First arugument is the coordinator name

	# It's safer to kill the target coordinator with killall command.  In this case, we need to
	# capture postmaster's pid for the target
    vecho ================================================================
    vecho Kill coordinator master $1

	if [ $coordSlave != y ]; then
		echo Coordinator slaves are not configured.
		return 1
	fi
	if [ $# -ne 1 ]; then
		iecho Specify nodename
		return 1
	fi
	local i
	local postmaster_pid
	for ((i=0; i< ${#coordNames[@]}; i++));	do
		if [ $1 == ${coordNames[$i]} ]; then
			if [ ${coordSlaveServers[$i]} != none ] && [ ${coordSlaveServers[$i]} != N/A ]; then
				postmaster_pid=`get_postmaster_pid ${coordSlaveServers[$i]} ${coordSlaveDirs[$i]}`
				if [ $postmaster_pid != none ]; then
					kill_all_child_parent ${coordSlaveServers[$i]} $postmaster_pid
				fi
			else
				echo specified coordinator slave does not exist, $1
			fi
			return
		fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}	

# This failover function assumes that no IP address is carried over from corresponding master server.
# If IP address can be carried over, then you don't need a section which issues ALTER NODE statement.
function pgxc_failover_coordinator			# Not tested yet.   Shoudl be tested after datanode master is okay.
{
	# First argument is the coordinator name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	if [ $# -ne 1 ]; then
		echo Specify coordinator name to failover
		return 1
	fi
	if [ $coordSlave != y ]; then
		echo Coordinator slaves are not configured. Cannot failover
		return 1
	fi

	local i
	for ((i=0; i< ${#coordNames[@]}; i++));	do
		if [ $1 == ${coordNames[$i]} ]; then
			if [ ${coordSlaveServers[$i]} == none ] || [ ${coordSlaveServers[$i]} == N/A ]; then
			  echo ERROR: Coordinator $1 slave is not configured.  Cannot failover.
			  return 1
		  fi
		  # Find the new local gtm_proxy
		  local j
		  local targetGTMhost
		  local targetGTMport
		  targetGTMhost=none
		  targetGTMport=0
		  for ((j=0; j<${#gtmProxyServers[@]}; j++)); do
			  if [ ${coordSlaveServers[$i]} == ${gtmProxyServers[$j]} ]; then
				  targetGTMhost=${gtmProxyServers[$j]}
				  targetGTMport=${gtmProxyPorts[$j]}
				  break
			  fi
		  done
		  # gtm_proxy has to be configured properly
		  # This can be a bit more flexible so that each component can connect to GTM directly if
		  # gtm_proxy is not configured locally.
		  if [ "$targetGTMhost" == none ]; then
			  echo ERROR: gtm_proxy is not configured at the server ${coordSlaveServers[$i]}
			  return 1
		  fi
		  # Now promote the slave
		  doit ssh $pgxcUser@${coordSlaveServers[$i]} pg_ctl promote -Z coordinator -D ${coordSlaveDirs[$i]}
		  # Reconfigure new master's gtm_proxy
		  vecho Reconfiguring new gtm_proxy for ${coordSlaveSerers[$i]}:${coordSlaveDirs[$i]}/postgresql.conf
		  ssh $pgxcUser@${coordSlaveServers[$i]} "cat >> ${coordSlaveDirs[$i]}/postgresql.conf" <<EOF
#=================================================
# Added to promote, $datetime
gtm_host = '$targetGTMhost'
gtm_port = $targetGTMport
# End of addition
EOF
		  # Restart the new master
		  doit ssh $pgxcUser@${coordSlaveServers[$i]} "pg_ctl restart -Z coordinator -D ${coordSlaveDirs[$i]} -o -i > $tmpDir/coord.out"
		  do_stdout ${coordSlaveServers[$i]} $tmpDir/coord.out
		  # Update other coordinator with this new one ---> first, clean connection for all the users for all the databases
		  # Get all the available users
		  # First, find available coordinator other than this one.
		  local coord_wk=none
		  local coord_port=0
		  for ((j=0; j< ${#coordMasterServers[@]}; j++)); do
			  if [ $j != $i ]; then
				  coord_wk=${coordMasterServers[$j]}
				  coord_port=${coordPorts[$j]}
				  break
			  fi
		  done
		  if [ $coord_wk != none ] && [ $coord_wk != N/A ]; then
			  # There's at least one coordinator working.  Need to do "CLEAN CONNECTION"
              # Then get all the users
			  cat > $localTmpDir/command.sql <<EOF
\pset tuples_only on
select usename from pg_user;
\q
EOF
			  psql -p $coord_port -h $coord_wk postgres $pgxcOwner -f $localTmpDir/command.sql -o $localTmpDir/command.out >/dev/null
			  local users
			  users=`cat $localTmpDir/command.out`
			  rm $localTmpDir/command.sql $localTmpDir/command.out
		  	  # Clean all the pooler connections and update the node registration
			  for ((j=0; j< ${#coordMasterServers[@]}; j++)); do
				  for user in $users; do
					  if [ $j != $i ]; then
						  psql -p ${coordPorts[$j]} -h ${coordMasterServers[$j]} postgres $pgxcOwner -c "CLEAN CONNECTION TO ALL TO USER $user"
					  fi
				  done
				  psql -p ${coordPorts[$j]} -h ${coordMasterServers[$j]} postgres $pgxcOwner -c "ALTER NODE ${coordNames[$i]} WITH (HOST='${coordSlaveHosts[$i]}', PORT=${coordPorts[$i]})"
			  done
		  fi
          # Then update the configuration file with this new configuration
		  coordMasterServers[$i]=${coordSlaverServers[$i]}
		  coordSlaveServers[$i]=N/A
		  coordSlaveDirs[$i]=N/A
		  # Update the configuration file with this new configuration
		  cat >> $configFile <<EOF
#=====================================================
# Updated due to the coordinator failover, $1, $datetime
coordMasterServers=( ${coordMasterServers[@]} )
coordSlaveServers=( ${coordSlaveServers[@]} )
coordSlaveDirs=( ${coordSlaveDirs[@]} )
# End of the update
EOF
		  return;
	  fi
	done
	echo ERROR: specified coordinator $1 not configured.
}

#===============================================================================
#
# Datanode staff
#
#===============================================================================

function pgxc_init_datanode_master
{
	# First argument is the nodename
    vecho ================================================================
    vecho Initialize datanode master $1

	if [ $# -ne 1 ]; then
		iecho Specify datanode name
		return 1
	fi
	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ $1 == ${datanodeNames[$i]} ] && [ ${datanodeMasterServers[$i]} != none ] && [ ${datanodeMasterServers[$i]} != N/A ]; then
			psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]}  -c 'select 1' postgres $pgxcOwner > /dev/null 2> /dev/null
			if [ $? -eq 0 ]; then
				echo ERROR: target coordinator master is running now.  Stop it to configure.
				return 1
			fi
			doit ssh $pgxcUser@${datanodeMasterServers[$i]} rm -rf ${datanodeMasterDirs[$i]}
			doit ssh $pgxcUser@${datanodeMasterServers[$i]} mkdir -p ${datanodeMasterDirs[$i]}
			doit ssh $pgxcUser@${datanodeMasterServers[$i]} "initdb --nodename ${datanodeNames[$i]} -D ${datanodeMasterDirs[$i]} > $tmpDir/initdb.out 2>&1"
			print_initdb_out ${datanodeMasterServers[$i]} $tmpDir/initdb.out
			vecho Configuring ${datanodeMasterServers[$i]}:${datanodeMasterDirs[$i]}/postgresql.conf
		    # Get effective GTM port and host.   If gtm_proxy is not found, then connect to GTM
			local j
			local targetGTMhost
			local targetGTMport
			targetGTMhost=$gtmMasterServer
			gatgetGTMPort=$gtmMasterPort
			for ((j=0; j< ${#gtmProxyServers[@]}; j++)); do
				if [ ${datanodeMasterServers[$i]} == ${gtmProxyServers[$j]} ]; then
					targetGTMhost=${gtmProxyServers[$j]}
					targetGTMport=${gtmProxyPorts[$j]}
					break
				fi
			done
			if [ $datanodeExtraConfig != none ] && [ $datanodeExtraConfig != N/A ]; then
				vecho Configuring $pgxcUser@${datanodeMasterServer[$i]}:${datanodeMasterDirs[$i]}/postgresql.conf using $datanodeExtraConfig
				ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/postgresql.conf" < $datanodeExtraConfig
			fi
			if [ ${datanodeSpecificExtraConfig[$i]} != none ] && [ ${datanodeSpecificExtraConfig[$i]} != none ]; then
				vecho Configuring $pgxcUser@${datanodeMasterServers[$i]}:${datanodeMasterDirs[$i]}/postgresql.conf using ${datanodeSpecificExtraConfig[$i]}
				ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/postgresql.conf" < ${datanodeSpecificExtraConfig[$i]}
			fi
			vecho Configuring  $pgxcUser@${datanodeMasterServers[$i]}:${datanodeMasterDirs[$i]}/postgresql.conf
			ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/postgresql.conf" <<EOF
#===========================================
# Added at initialization. $datetime
log_destination = 'stderr'
logging_collector = on
log_directory = 'pg_log'
listen_addresses = '*'
port = ${datanodePorts[$i]}
max_connections = 100
gtm_host = '$targetGTMhost'
gtm_port = $targetGTMport
EOF
			# Additional initialization for log_shipping.
			if [ $datanodeSlave == y ] && [ ${datanodeSlaveServers[$i]} != none ] && [ ${datanodeSlaveServers[$i]} != N/A ]; then
				# At least cleanup remote archive directory.
				pgxc_clean_dir ${datanodeSlaveServers[$i]} ${datanodeArchLogDirs[$i]}
				# Then setup postgresql.conf
				ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/postgresql.conf" <<EOF
wal_level = hot_standby
archive_mode = on
archive_command = 'rsync %p $pgxcUser@${datanodeSlaveServers[$i]}:${datanodeArchLogDirs[$i]}/%f'
max_wal_senders = ${datanodeMaxWalSenders[$i]}
# End of Addition
EOF
			else
				ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/postgresql.conf" <<EOF
# End of Addition
EOF
			fi
			vecho Configuring ${cdatanodeMasterServers[$i]}:${datanodeMasterDirs[$i]}/pg_hba.conf
			ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${coordMasterDirs[$i]}/pg_hba.conf"  <<EOF
#=================================================
# Addition at initialization, $datetime 
EOF
			if [ $datanodeExtraPgHba != none ] && [ $datanodeExtraPgHba != N/A ]; then
				vecho Configuring ${datanodeMasterServers[$i]}:${datanodeMasterDirs[$i]}/pg_hba.conf using $datanodeExtraPgHba
				ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/pg_hab.conf" < $datanodeExtraPgHba
			fi
			if [ ${datanodeSpecificExtraPgHba[$i]} != none ] && [ ${datanodeSpecificExtraPgHba[$i]} != N/A ]; then
				ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/pg_hab.conf" < ${datanodeSpecificExtraPgHba[$i]}
			fi
			local j
			for ((j=0; j< ${#datanodePgHbaEntries[@]}; j++)); do
				ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/pg_hba.conf" <<EOF
host all $pgxcOwner ${datanodePgHbaEntries[$j]} trust
EOF
				if [ ${datanodeSlaveServers[$i]} != none ] && [ ${datanodeSlaveServers[$i]} != N/A ]; then
					vecho ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/pg_hba.conf"
					ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/pg_hba.conf"  <<EOF
host replication $pgxcOwner ${datanodePgHbaEntries[$j]} trust
EOF
				fi
			done
			ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/pg_hba.conf"  <<EOF
# End of addition
EOF
			return
		fi
	done
	echo ERROR: specified coordinator is not configured, $1
	return 1
}

function pgxc_init_datanode_master_all
{
    vecho ================================================================
    vecho Initialize all the datanode masters

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ ${datanodeMasterServers[$i]} != none ] && [ ${datanodeMasterServers[$i]} != N/A ]; then
			pgxc_init_datanode_master ${datanodeNames[$i]}
		fi
	done
}

function pgxc_start_datanode_master
{
	# First argument is the nodename
    vecho ================================================================
    vecho Start datanode master $1

	if [ $# -ne 1 ];then
		iecho Specify datanode name
		return 1
	fi
	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ $1 == ${datanodeNames[$i]} ]; then
			psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
			if [ $? -eq 0 ]; then
				echo ERROR: target datanode master is running now.
				return 1
			fi
			doit ssh $pgxcUser@${datanodeMasterServers[$i]} "pg_ctl start -Z datanode -D ${datanodeMasterDirs[$i]} -o -i > $tmpDir/datanode.out"
			do_stdout ${datanodeMasterServers[$i]} $tmpDir/datanode.out
			return
		fi
	done
	echo ERROR: specified datanode is not configured, $1
	return 1
}

function pgxc_start_datanode_master_all
{
    vecho ================================================================
    vecho Start all the datanode masters
	
	local i
	for ((i=0;i<${#datanodeNames[@]};i++));do
		pgxc_start_datanode_master ${datanodeNames[$i]}
	done
}

function pgxc_stop_datanode_master
{
	# First argument is the nodename
    vecho ================================================================
    vecho Stop datanode master $1

	if [ $# -ne 1 ]; then
		iecho Specify datanode name
		return 1
	fi
	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ $1 == ${datanodeNames[$i]} ];  then
			doit ssh $pgxcUser@${datanodeMasterServers[$i]} pg_ctl stop -Z datanode -D ${datanodeMasterDirs[$i]} $immediate
			return
		fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

function pgxc_stop_datanode_master_all
{
    vecho ================================================================
    vecho Stop all the datanode master

	local i
	for ((i=0;i<${#datanodeNames[@]};i++));do
		pgxc_stop_datanode_master ${datanodeNames[$i]}
	done
}

function pgxc_kill_datanode_master
{
	# First arugument is the nodename
    vecho ================================================================
    vecho Kill coordinator master $1

	if [ $# -ne 1 ]; then
		iecho Specify datanode name
		return 1
	fi
	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ $1 == ${datanodedNames[$i]} ]; then
			if [ ${datanodeMasterServers[$i]} != none ] && [ ${datanodeMasterServers[$i]} != N/A ]; then
				postmaster_pid=`get_postmaster_pid ${datanodeMasterServers[$i]} ${datandoeMasterDirs[$i]}`
				if [ $postmaster_pid != none ]; then
					kill_all_child_parent ${datanodeMasterServers[$i]} $postmaster_pid
				fi
			else
				echo Dould not find specified coordinator master, $1
			fi
			return
		fi
	done
	echo ERROR: specified coordinator does not exist, $1
	return 1
}

function pgxc_kill_datanode_master_all
{
    vecho ================================================================
    vecho Kill all the datanode master

	local i
	for ((i=0;i<${#datanodeNames[@]};i++));do
		pgxc_kill_datanode_master ${datanodeNames[$i]}
	done
}

# To construct datanode slave, pg_basebackup utility is used, which needs master coordinator running.
# If the master is not running, then we temporary run it.   After copying the base backup,
# the the master will be stopped.  Please be sure that coordinator master is initialized properly.
# If it is running, then it will be restarted to reflect the change to postgresql.conf.
function pgxc_init_datanode_slave
{
	# First argument is the datanode name
    vecho ================================================================
    vecho Initialize datanode slave $1

	local i
	local start_master=n
	for ((i=0;i<${#datanodeNames[@]};i++)); do
		if [ $1 == ${datanodeNames[$i]} ]; then
			if [ ${datanodeSlaveServers[$i]} == N/A ]; then
				echo ERROR: slave for the datanode $1 is not configured.
				return 1
			fi
		    # Datanode master should be running
			psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
			if [ $? -ne 0 ]; then
				start_master=y
			fi
		    # Clean slave's directory
			doit ssh $pgxcUser@${datanodeSlaveServers[$i]} rm -rf ${datanodeSlaveDirs[$i]}
			doit ssh $pgxcUser@${datanodeSlaveServers[$i]} mkdir -p ${datanodeSlaveDirs[$i]}

		    # if the master is not running, we just start it and then stop it.
			if [ $start_master == y ]; then
				vecho Starting the datanode master to obtain base backup
				doit ssh $pgxcUser@${datanodeMasterServers[$i]} "pg_ctl start -Z datanode -D ${datanodeMasterDirs[$i]} -o -i > $tmpDir/cmd.out"
				do_stdout ${datanodeMasterServers[$i]} $tmpDir/cmd.out
				sleep 2
			fi
		    # Obtain base backup of the master
			pgxc_clean_dir ${datanodeSlaveServers[$i]} ${datanodeSlaveDirs[$i]}
			doit ssh $pgxcUser@${datanodeSlaveServers[$i]} pg_basebackup -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} -D ${datanodeSlaveDirs[$i]} -x

		    # Configure recovery.conf of the slave
			vecho -- Configure slave\'s recovery.conf, ${datanodeSlaveServers[$i]}:${datanodeSlaveDirs[$i]}/recovery.conf
			ssh $pgxcUser@${datanodeSlaveServers[$i]} "cat >> ${datanodeSlaveDirs[$i]}/recovery.conf" <<EOF
#==========================================
# Added to initialize the slave, $datetime
standby_mode = on
primary_conninfo = 'host = ${datanodeMasterServers[$i]} port = ${datanodePorts[$i]} user = $pgxcOwner application_name = ${datanodeNames[$i]}'
restore_command = 'cp ${datanodeArchLogDirs[$i]}/%f %p'
archive_cleanup_command = 'pg_archivecleanup ${datanodeArchLogDirs[$i]} %r'
EOF

		    # Configure slave's postgresql.conf
			vecho -- Configure slave\'s postgresql.conf, ${doatanodeSlaveServers[$i]}:${datanodeSlaveDirs[$i]}/postgresql.conf
			ssh $pgxcUser@${datanodeSlaveServers[$i]} "cat >> ${datanodeSlaveDirs[$i]}/postgresql.conf" <<EOF
#==========================================
# Added to startup the slave, $dtetime
hot_standby = on
port = ${datanodePorts[$i]}
EOF
			if [ $start_master == y ]; then
				vecho Stopping the datanode master.
				doit ssh $pgxcUser@${datanodeMasterServers[$i]} "pg_ctl stop -Z datanode -D ${datanodeMasterDirs[$i]} > /dev/null 2>&1"
			fi
			return
		fi
	done
	echo ERROR: specified coordinator is not configured, $1
	return 1
}

function pgxc_init_datanode_slave_all
{
    vecho ================================================================
    vecho Initialize all the datanode slaves

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ ${datanodeSlaveServers[$i]} != none ] && [ ${datanodeSlaveServers[$i]} != N/A ]; then
			pgxc_init_datanode_slave ${datanodeNames[$i]}
		fi
	done
}

function pgxc_start_datanode_slave
{
	# First argument is the datanode name
    vecho ================================================================
    vecho Initialize coordinator slave $1

	if [ $datanodeSlave != y ]; then
		echo Datanode slaves are not configured
		return 1
	fi
	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ $1 == ${datanodeNames[$i]} ]; then
			if [ ${datanodeSlaveServers[$i]} == none ] || [ ${datanodeSlaveServers[$i]} == N/A ]; then
				echo ERROR: slave for datanode $1 is not configured.
				return 1
			fi
		    # Datanode master should be running
			psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
			if [ $? -ne 0 ] ; then
				echo ERROR: corresponding datanode master is not running now, $1
				return 1
			fi
		    # Start the slave
			doit ssh $pgxcUser@${datanodeSlaveServers[$i]} "pg_ctl start -Z datanode -D ${datanodeSlaveDirs[$i]} -o -i > $tmpDir/coord.out"
			do_stdout ${datanodeSlaveServers[$i]} $tmpDir/coord.out
		    # Change the master to synchronous mode
			vecho Change the master to synchrnous mode, $1
			ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/postgresql.conf" <<EOF
#==========================================================
# Added to start the slave in sync. mode, $datetime
synchronous_commit = on
synchronous_standby_names = '${datanodeNames[$i]}'
# End of the addition
EOF
			ssh $pgxcUser@${datanodeMasterServers[$i]} pg_ctl reload -Z datanode -D ${datanodeMasterDirs[$i]}
			return
		fi
	done
	echo ERROR: specified datanode does not exist, $1
	return 1
}

function pgxc_start_datanode_slave_all
{
    vecho ================================================================
    vecho Start all the datanode slaves

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ ${datanodeSlaveServers[$i]} != none ] && [ ${datanodeSlaveServers[$i]} != N/A ]; then
			pgxc_start_datanode_slave ${datanodeNames[$i]}
		fi
	done
}

function pgxc_stop_datanode_slave
{
	# First argument is the datanode name
    vecho ================================================================
    vecho Initialize datanode slave $1


	if [ $datanodeSlave != y ]; then
		echo Datanode slaves are not configured
		return 1
	fi
	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ $1 == ${datanodeNames[$i]} ];  then
			if [ ${datanodeSlaveServers[$i]} == none ] || [ ${datanodeSlaveServers[$i]} == N/A ]; then
				echo ERROR: slave for the datanode $1 is not configured.
			fi
		    # If the master is running, master's switch replication to asynchronous mode.
			psql -p ${datanodePorts[$i]} -h ${datanodeMasterServers[$i]} postgres $pgxcOwner -c 'select 1' > /dev/null 2> /dev/null
			if [ $? -eq 0 ]; then
			    # Switch Master to asynchronous mode.
				vecho Switching master of $1 at ${datanodeMasterServer[$i]} to asynchronous replication mode.
				ssh $pgxcUser@${datanodeMasterServers[$i]} "cat >> ${datanodeMasterDirs[$i]}/postgresql.conf" <<EOF
#=======================================
# Updated to trun off the slave $datetime
synchronous_standby_names = ''
# End of the update
EOF
				doit ssh $pgxcUser@${datanodeMasterServers[$i]} pg_ctl reload -Z datanode -D ${datanodeMasterDirs[$i]}
			fi
			doit ssh $pgxcUser@${datanodeSlaveServers[$i]} pg_ctl stop -Z datanode -D ${datanodeSlaveDirs[$i]} $immediate
			return;
		fi
	done
	echo ERROR: Specified datanode was not configureed, $1
	return 1
}
function pgxc_stop_datanode_slave_all
{
    vecho ================================================================
    vecho Stop all the datanode slaves

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ ${datanodeSlaveServers[$i]} != none ] && [ ${datanodeSlaveServers[$i]} != N/A ]; then
			pgxc_stop_datanode_slave ${datanodeNames[$i]}
		fi
	done
}

function pgxc_kill_datanode_master
{
	# First argument is the datanodeinator name
    vecho ================================================================
    vecho Initialize datanode slave $1

	local i
	local postmaster_pid
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ $1 == ${datanodeNames[$i]} ]; then
			if [ ${datanodeMasterServers[$i]} != none ] && [ ${datanodeMasterServers[$i]} != N/A ]; then
				postmaster_pid=`get_postmaster_pid ${datanodeMasterServers[$i]} ${datanodeMasterDirs[$i]}`
				if [ $postmaster_pid != none ]; then
					kill_all_child_parent ${datanodeMasterServers[$i]} $postmaster_pid
				fi
			else
				echo specified coordinator master does not exist, $1
			fi
			return
		fi
	done
	echo ERROR: specified datanode master is not configured, $1
	return 1
}

function pgxc_kill_datanode_master_all
{
    vecho ================================================================
    vecho Kill all the datanode masters

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ ${datanodeMasterServers[$i]} != none ] && [ ${datanodeMasterServers[$i]} != N/A ]; then
			pgxc_kill_datanode_master ${datanodeNames[$i]}
		fi
	done
}

# Please note that this function does not take care of anything but just kill the processes.
function pgxc_kill_datanode_slave
{
	# First argument is the datanodeinator name
    vecho ================================================================
    vecho Kill datanode slave $1

	local i
	local postmaster_pid
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ $1 == ${datanodeNames[$i]} ]; then
			if [ ${datanodeSlaveServers[$i]} != none ] && [ ${datanodeSlaveServers[$i]} != N/A ]; then
				postmaster_pid=`get_postmaster_pid ${datanodeSlaveServers[$i]} ${datanodeSlaveDirs[$i]}`
				if [ $postmaster_pid != none ]; then
					kill_all_child_parent ${datanodeSlaveServers[$i]} $postmaster_pid
				fi
			else
				echo specified coordinator master does not exist, $1
			fi
			return
		fi
	done
	echo ERROR: specified datanode master is not configured, $1
	return 1
}

function pgxc_kill_datanode_slave_all
{
    vecho ================================================================
    vecho Kill all the datanode masters

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++)); do
		if [ ${datanodeSlaveServers[$i]} != none ] && [ ${datanodeSlaveServers[$i]} != N/A ]; then
			pgxc_kill_datanode_slave ${datanodeNames[$i]}
		fi
	done
}

function pgxc_failover_datanode		# Not tested yet. Test should be done after CREATE NODE/ALTER NODE is tested.
{
	# First argument is the datanode name
    vecho ================================================================
    vecho Initialize datanode slave $1

	if [ $# -ne 1 ]; then
		iecho Specify datanode name to failover
		return 1
	fi
	if [ datanodeSlave != y ]; then
		echo Datanode slaves are not configured. Cannot failover
		return 1
	fi

	local i
	for ((i=0; i< ${#datanodeNames[@]}; i++));	do
		if [ $1 == ${datanodeNames[$i]} ]; then
			if [ ${datanodeSlaveServers[$i]} == none ] || [ ${datanodeSlaveServers[$i]} == N/A ]; then
			  echo ERROR: Datanode $1 slave is not configured
			  return 1
		  fi
		  # Find the new local gtm_proxy
		  local j
		  local targetGTMhost
		  local targetGTMport
		  targetGTMhost=none
		  targetGTMport=0
		  for ((j=0; j<${#gtmProxyServers[@]}; j++)); do
			  if [ ${datanodeSlaveServers[$i]} == ${gtmProxyServers[$j]} ]; then
				  targetGTMhost= ${gtmProxyServers[$j]}
				  targetGTMport= ${gtmProxyPorts[$j]}
				  break
			  fi
		  done
		  # gtm_proxy have to be configured properly
		  # This can be a bit more flexible so that each component can connect to GTM directly if
		  # gtm_proxy is not configured locally.
		  if [ "$targetGTMhost" == none ]; then
			  echo ERROR: gtm_proxy is not configured at the server ${datanodeSlaveServers[$i]}
			  return 1
		  fi
		  # Now promote the slave
		  doit ssh $pgxcUser@${datanodeSlaveServers[$i]} pg_ctl promote -Z datanode -D ${datanodeSlaveDirs[$i]}
		  # Reconfigure new master's gtm_proxy
		  vecho Reconfiguring new gtm_proxy for ${datanodeSlaveSerers[$i]}:${datanodeSlaveDirs[$i]}/postgresql.conf
		  ssh $pgxcUser@${datanodeSlaveServers[$i]} "cat >> ${datanodeSlaveDirs[$i]}/postgresql.conf" <<EOF
#=================================================
# Added to promote, $datetime
gtm_host = '$targetGTMhost'
gtm_port = $targetGTMport
# End of addition
EOF
		  # Restart the new master
		  doit ssh $pgxcUser@${datanodeSlaveServers[$i]} "pg_ctl restart -Z datanode -D ${datanodeSlaveDirs[$i]} -o -i > $tmpDir/datanode.out"
		  do_stdout ${datanodeSlaveServers[$i]} $tmpDir/datanode.out
		  # Update other datanode with this new one ---> first, clean connection for all the users for all the databases
		  # Get all the available users
		  # First, find available datanode other than this one.
		  local datanode_wk=none
		  local datanode_port=0
		  for ((j=0; j< ${#datanodeMasterServers[@]}; j++)); do
			  if [ $j -ne -$i ]; then
				  datanode_wk= ${datanodeMasterServers[$j]}
				  datanode_port= ${datanodePorts[$j]}
				  break
			  fi
		  done
		  if [ $datanode_wk != none ] && [ $datanode_wk != N/A ]; then
			  # There's at least one datanode working.  Need to do "CLEAN CONNECTION"
              # Then get all the users
			  cat > $localTmpDir/command.sql <<EOF
\pset tuples_only on
select usename from pg_user;
\q
EOF
			  psql -p $datanode_port -h $datanode_wk postgres $pgxcOwner -f $localTmpDir/command.sql -o $localTmpDir/command.out >/dev/null
			  local users
			  users= `cat $localTmpDir/command.out`
			  rm $localTmpDir/command.sql $localTmpDir/command.out
		  	  # Clean all the pooler connections and update the node registration
			  for ((j=0; j< ${#datanodeMasterServers[@]}; j++)); do
				  for user in $users; do
					  if [ $j -ne $i ]; then
						  psql -p ${datanodePorts[$j]} -h ${datanodeMasterServers[$j]} postgres $pgxcOwner -c "CLEAN CONNECTION TO ALL FOR USER $user"
					  fi
				  done
				  psql -p ${datanodePorts[$j]} -h ${datanodeMasterServers[$j]} postgres $pgxcOwner -c "ALTER NODE ${datanodeNames[$i]} WITH (HOST='${datanodeSlaveHosts[$i]}', PORT=${datanodePorts[$i]})"
			  done
		  fi
          # Then update the configuration file with this new configuration
		  datanodeMasterServers[$i]=${datanodeSlaverServers[$i]}
		  datanodeSlaveServers[$i]=N/A
		  datanodeSlaveDirs[$i]=N/A
		  # Update the configuration file with this new configuration
		  cat >> $configFile <<EOF
#=====================================================
# Updated due to the datanode failover, $1, $datetime
datanodeMasterServers=( ${datanodeMasterServers[@]} )
datanodeSlaveServers=( ${datanodeSlaveServers[@]} )
datanodeSlaveDirs=( ${datanodeSlaveDirs[@]} )
# End of the update
EOF
		  return;
	  fi
	done
	echo ERROR: specified datanode $1 not configured.
}

function pgxc_configure_nodes
{
	# First argument is a coordinator name
    vecho ================================================================
    vecho Configure nodes for coordinator $1

	if [ $# -ne 1 ]; then
		echo Specify coorinator name.
		return 2
	fi
	local i
	local j
	for ((i=0; i<${#coordNames[@]}; i++));do
		if [ $1 == ${coordNames[$i]} ] && [ ${coordNames[$i]} != none ] && [ ${coordNames[$i]} != N/A ]; then
			for ((j=0; j<${#coordNames[@]}; j++));do
				decho Setup pgxc_node for ${coordNames[$j]} at ${coordNames[$i]}.
				# Setup coordinators
				decho i=$i, j=$j
				if [ $i != $j ]; then
					echo CREATE NODE
					cat > $localTmpDir/cmd.sql <<EOF
CREATE NODE ${coordNames[$j]} WITH (TYPE='coordinator', HOST='${coordMasterServers[$j]}', PORT=${coordPorts[$j]});
\q
EOF
				else
					echo ALTER NODE
					cat > $localTmpDir/cmd.sql <<EOF
ALTER NODE ${coordNames[$j]} WITH (HOST='${coordMasterServers[$j]}', PORT=${coordPorts[$j]});
\q
EOF
				fi
				ddo cat $localTmpDir/cmd.sql
				psql -h ${coordMasterServers[$i]} -p ${coordPorts[$i]} -f $localTmpDir/cmd.sql postgres $pgxcOwner
				rm -rf $localTmpDir/cmd.sql
			done
			for ((j=0; j<${#datanodeNames[@]}; j++)); do
				decho Setup pgxc_node for ${datanodeNames[$j]} at ${coordNames[$i]}.
				# Setup datanodes
				cat > $localTmpDir/cmd.sql <<EOF
CREATE NODE ${datanodeNames[$j]} WITH (TYPE='datanode', HOST='${datanodeMasterServers[$j]}', PORT=${datanodePorts[$j]});
EOF
				if [ ${datanodeNames[$j]} == $primaryDatanode ]; then
					# Primary node
					cat >> $localTmpDir/cmd.sql <<EOF
ALTER NODE  ${datanodeNames[$j]} WITH (PRIMARY);
EOF
				fi
				if [ ${datanodeMasterServers[$j]} == ${coordMasterServers[$i]} ]; then
					# Preferred node
					cat >> $localTmpDir/cmd.sql <<EOF
ALTER NODE  ${datanodeNames[$j]} WITH (PREFERRED);
EOF
				fi
				cat >> $localTmpDir/cmd.sql <<EOF
\q
EOF
				ddo cat $localTmpDir/cmd.sql
				psql -h ${coordMasterServers[$i]} -p ${coordPorts[$i]} -f $localTmpDir/cmd.sql postgres $pgxcOwner
				rm -rf $localTmpDir/cmd.sql
			done
			return
		fi
	done
	echo Coordinator $1 is not configured.
	return 1
}

function pgxc_configure_nodes_all
{
    vecho ================================================================
    vecho Configure nodes for all the coordinators

	local i
	for ((i=0;i<${#coordNames[@]};i++)); do
		if [ ${coordMasterServers[$i]} != none ] && [ ${coordMasterServers[$i]} != N/A ]; then
			pgxc_configure_nodes ${coordNames[$i]}
		fi
	done
}

function init_all
{
	pgxc_init_gtm_master
	pgxc_init_gtm_slave
	pgxc_init_gtm_proxy_all
	pgxc_init_datanode_master_all
	pgxc_start_gtm_master
	pgxc_start_gtm_proxy_all
	pgxc_init_datanode_slave_all
	pgxc_init_coordinator_master_all
	pgxc_init_coordinator_slave_all
	pgxc_stop_gtm_proxy_all
	pgxc_stop_gtm_slave
	pgxc_stop_gtm_master
}

function start_all
{
	pgxc_start_gtm_master
	pgxc_start_gtm_slave
	pgxc_start_gtm_proxy_all
	pgxc_start_datanode_master_all
	pgxc_start_datanode_slave_all
	pgxc_start_coordinator_master_all
	pgxc_start_coordinator_slave_all
}

function stop_all
{
	pgxc_stop_coordinator_slave_all
	pgxc_stop_coordinator_master_all
	pgxc_stop_datanode_slave_all
	pgxc_stop_datanode_master_all
	pgxc_stop_gtm_proxy_all
	pgxc_stop_gtm_slave
	pgxc_stop_gtm_master
}

function clean_all
{
	local immediate_bk
	immediate_bk="$immediate"
	immediate="-m immediate"
	stop_all
	pgxc_clean_node_all
	immediate="$immediate_bk"
}

###############################################################################
#
#   EXECUTING SECTION
#
###############################################################################

#=======================================================
# Things to be done at first
#=======================================================

# Handle options
progname=$0
moretodo=y
while [ $moretodo == y ]; do
	if [ $# -gt 0 ]; then
		case $1 in
			-v )
				shift;
				verbose=y;
				continue;;
			--verbose )
				shift;
				verbose=y;
				continue;;
			--silent )
				verbose=n;
				continue;;
			-d ) # debug option
				shift;
				DEBUG=y;
				continue;;
			--debug )
				shift;
				DEBUG=y;
				continue;;
			-c ) # Configuraton file
				shift;
				if [ $# -le 0 ]; then
					echo ERROR: no -c option value found
					exit 1
				else
					$configFile=$1
					shift
				fi;
				continue;;
			--configuration ) # Configuraion file
				shift;
				if [ $# -le 0 ]; then
					echo ERROR: no --configuration option value found
					exit 1
				else
					$configFile=$1
					shift
				fi;
				continue;;
			-i ) # input_file
				shift;
				if [ "$1" == "" ]; then
					echo Epecify input file
				else
					$progname < $1
				fi
				continue;;
			--interactive ) # interactive
				shift;
				interactive=y
				continue;;
			--batch )
				shift
				interactive=n
				continue;;
			* )
				moretodo=n
				continue;;
		esac
	else
		moretodo=n
	fi
done


# Read configuration file --> Should be activated only when debug option is off
if [ -f $configFile ]; then
	source $configFile
fi

# Check if slaves are configured and makeup each server to N/A if needed
handle_no_slaves

# Construct the server list
makeServerList

# For interactive operation ---> Finally this should be a part of pgxc_ctl interactive command
# interactive=y --> should be set by options
while [ 1 ]; do
	echo -n "$xc_prompt"
	read cmdname cmdparam

	if [ "$cmdname" == "" ];  then
		continue
	fi
	if [ "$cmdname" == "end"  -o "$cmdname" == "q" ]; then
		break
	fi
	# Need the following trick to handle variable value assignment and echo.
	cat > $localTmpDir/wk.cmd <<EOF
$cmdname $cmdparam
EOF
	source $localTmpDir/wk.cmd
	rm $localTmpDir/wk.cmd
	continue
done
